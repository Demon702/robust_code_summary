Clean Code,Corrupted Code,Gold output,Prediction1,Prediction2,Shuffle_Index,Better Label,
"def func_from_info(self):
        
        info = self.funcinfo
        functype = info['func_type']
        if functype in ['instancemethod', 'classmethod', 'staticmethod']:
            the_modelclass = get_module_member_by_dottedpath(info['class_path'])
            if functype == 'instancemethod':
                the_modelobject = the_modelclass.objects.get(pk=info['model_pk'])
                the_callable = get_member(the_modelobject, info['func_name'])
            else:
                the_callable = get_member(the_modelclass, info['func_name'])
            return the_callable
        elif functype == 'function':
            mod = import_module(info['module_name'])
            the_callable = get_member(mod, info['func_name'])
            return the_callable
        else:
            raise ValueError(f""Unknown functype '{functype} in task {self.pk} ({self.label})"")","

def func1(arg1):
    var1 = self.funcinfo
    var2 = var1['func_type']
    if (var2 in ['instancemethod', 'classmethod', 'staticmethod']):
        var3 = get_module_member_by_dottedpath(var1['class_path'])
        if (var2 == 'instancemethod'):
            var5 = the_modelclass.objects.get(pk=var1['model_pk'])
            var6 = get_member(var5, var1['func_name'])
        else:
            var6 = get_member(var3, var1['func_name'])
        return var6
    elif (var2 == 'function'):
        var8 = import_module(var1['module_name'])
        var6 = get_member(var8, var1['func_name'])
        return var6
    else:
        raise ValueError(f""Unknown functype '{var2} in task {self.pk} ({self.label})"")
",Find and return a callable object from a task info dictionary,Return the value of the first function in this task,Get the callable for this task .,0,Prediction2,
"def _choose_front_id_from_candidates(candidate_offset_front_ids, offset_fronts, offsets_corresponding_to_onsets):
    
    noverlaps = []  # will contain tuples of the form (number_overlapping, offset_front_id)
    for offset_front_id in candidate_offset_front_ids:
        offset_front_f_idxs, offset_front_s_idxs = np.where(offset_fronts == offset_front_id)
        offset_front_idxs = [(f, i) for f, i in zip(offset_front_f_idxs, offset_front_s_idxs)]
        noverlap_this_id = len(set(offset_front_idxs).symmetric_difference(set(offsets_corresponding_to_onsets)))
        noverlaps.append((noverlap_this_id, offset_front_id))
    _overlapped, chosen_offset_front_id = max(noverlaps, key=lambda t: t[0])
    return int(chosen_offset_front_id)","

def func1(arg1, arg2, arg3):
    var1 = []
    for var2 in arg1:
        (var3, var4) = np.where((arg2 == var2))
        var5 = [(var6, var7) for (var6, var7) in zip(var3, var4)]
        var9 = len(set(offset_front_idxs).symmetric_difference(set(arg3)))
        noverlaps.append((var9, var2))
    (var12, var13) = max(var1, key=(lambda arg4: arg4[0]))
    return int(var13)
",Returns a front ID which is the id of the offset front that contains the most overlap with offsets that correspond to the given onset front ID .,func1 returns the index of the first occurrence of arg2 in arg3,Choose an offset front id from a list of candidate fronts .,0,Prediction2,
"def first(self) -> Signature:
        
        k = sorted(self._hsig.keys())
        return self._hsig[k[0]]","

def func1(arg1) -> var3:
    var1 = sorted(self._hsig.keys())
    return self._hsig[var1[0]]
",Retrieve the first Signature ordered by mangling descendant,Return the signature of arg1 .,Return the first signature .,0,Prediction2,
"def reset(self, new_session=True):
        
        self.output_hist.clear()
        # The directory history can't be completely empty
        self.dir_hist[:] = [os.getcwdu()]
        
        if new_session:
            if self.session_number:
                self.end_session()
            self.input_hist_parsed[:] = [""""]
            self.input_hist_raw[:] = [""""]
            self.new_session()","

def func1(arg1, arg2=True):
    self.output_hist.clear()
    self.dir_hist[:] = [os.getcwdu()]
    if arg2:
        if self.session_number:
            self.end_session()
        self.input_hist_parsed[:] = ['']
        self.input_hist_raw[:] = ['']
        self.new_session()
",Clear the session history releasing all object references and optionally open a new session .,Called when a command is received from the server .,Reset the history .,0,Prediction2,
"def create_tfs_tfvc_client(url, token=None):
    
    if token is None:
        token = os.environ.get('TFS_API_TOKEN', None)

    tfs_connection = create_tfs_connection(url, token)
    tfs_tfvc_client = tfs_connection.get_client('vsts.tfvc.v4_1.tfvc_client.TfvcClient')

    if tfs_tfvc_client is None:
        msg = 'Unable to create TFS Git Client, failed to connect to TFS Enterprise (%s) with provided token.'
        raise RuntimeError(msg, url)

    return tfs_tfvc_client","

def func1(arg1, arg2=None):
    if (arg2 is None):
        arg2 = os.environ.get('TFS_API_TOKEN', None)
    var1 = create_tfs_connection(arg1, arg2)
    var3 = tfs_connection.get_client('vsts.tfvc.v4_1.tfvc_client.TfvcClient')
    if (var3 is None):
        var4 = 'Unable to create TFS Git Client, failed to connect to TFS Enterprise (%s) with provided token.'
        raise RuntimeError(var4, arg1)
    return var3
",Creates a TFS TFVC Client to pull TFVC repo info,Function to connect to TFS Enterprise with provided token,Create a TFS Git Client .,0,Tie,
"def semilocal_linear_trend_transition_matrix(autoregressive_coef):
  
  # We want to write the following 2 x 2 matrix:
  #  [[1., 1., ],    # level(t+1) = level(t) + slope(t)
  #   [0., ar_coef], # slope(t+1) = ar_coef * slope(t)
  # but it's slightly tricky to properly incorporate the batch shape of
  # autoregressive_coef. E.g., if autoregressive_coef has shape [4,6], we want
  # to return shape [4, 6, 2, 2]. We do this by breaking the matrix into its
  # fixed entries, written explicitly, and then the autoregressive_coef part
  # which we add in after using a mask to broadcast to the correct matrix shape.

  fixed_entries = tf.constant(
      [[1., 1.],
       [0., 0.]],
      dtype=autoregressive_coef.dtype)

  autoregressive_coef_mask = tf.constant([[0., 0.],
                                          [0., 1.]],
                                         dtype=autoregressive_coef.dtype)
  bottom_right_entry = (autoregressive_coef[..., tf.newaxis, tf.newaxis] *
                        autoregressive_coef_mask)
  return tf.linalg.LinearOperatorFullMatrix(
      fixed_entries + bottom_right_entry)","

def func1(arg1):
    var1 = tf.constant([[1.0, 1.0], [0.0, 0.0]], dtype=autoregressive_coef.dtype)
    var2 = tf.constant([[0.0, 0.0], [0.0, 1.0]], dtype=autoregressive_coef.dtype)
    var3 = (arg1[(..., tf.newaxis, tf.newaxis)] * var2)
    return tf.linalg.LinearOperatorFullMatrix((var1 + var3))
",Build the transition matrix for a semi - local linear trend model .,Function 1 .,Returns the semilocal linear trend transition matrix .,0,Prediction2,
"def flat_plane(script, plane=0, aspect_ratio=False):
    
    filter_xml = ''.join([
        '  <filter name=""Parametrization: Flat Plane "">\n',
        '    <Param name=""projectionPlane""',
        'value=""%d""' % plane,
        'description=""Projection plane""',
        'enum_val0=""XY""',
        'enum_val1=""XZ""',
        'enum_val2=""YZ""',
        'enum_cardinality=""3""',
        'type=""RichEnum""',
        'tooltip=""Choose the projection plane""',
        '/>\n',
        '    <Param name=""aspectRatio""',
        'value=""%s""' % str(aspect_ratio).lower(),
        'description=""Preserve Ratio""',
        'type=""RichBool""',
        'tooltip=""If checked the resulting parametrization will preserve the original apsect ratio of the model otherwise it will fill up the whole 0..1 uv space""',
        '/>\n',
        '  </filter>\n'])
    util.write_filter(script, filter_xml)
    return None","

def func1(arg1, arg2=0, arg3=False):
    var1 = ''.join(['  <filter name=""Parametrization: Flat Plane "">\n', '    <Param name=""projectionPlane""', ('value=""%d""' % arg2), 'description=""Projection plane""', 'enum_val0=""XY""', 'enum_val1=""XZ""', 'enum_val2=""YZ""', 'enum_cardinality=""3""', 'type=""RichEnum""', 'tooltip=""Choose the projection plane""', '/>\n', '    <Param name=""aspectRatio""', ('value=""%s""' % str(aspect_ratio).lower()), 'description=""Preserve Ratio""', 'type=""RichBool""', 'tooltip=""If checked the resulting parametrization will preserve the original apsect ratio of the model otherwise it will fill up the whole 0..1 uv space""', '/>\n', '  </filter>\n'])
    util.write_filter(arg1, var1)
    return None
",Flat plane parameterization,Apply flat plane filter to a script .,Create a flat plane parametrization filter,1,Tie,
"def run(self):
        
        try:
            while True:
                msg, args, kwargs = self._receive_data()
                stop = self._handle_data(msg, args, kwargs)
                if stop:
                    break
        finally:
            if self._storage_service.is_open:
                self._close_file()
            self._trajectory_name = ''","

def func1(arg1):
    try:
        while True:
            (var1, var2, var3) = self._receive_data()
            var4 = self._handle_data(var1, var2, var3)
            if var4:
                break
    finally:
        if self._storage_service.is_open:
            self._close_file()
        self._trajectory_name = ''
",Starts listening to the queue .,Run the trajectories .,Receives data and handles it .,1,Prediction2,
"def set_constant(self, name, value):
        
        assert isinstance(name, str) or isinstance(name, sympy.Symbol), \
            ""constant name needs to be of type str, unicode or a sympy.Symbol""
        assert type(value) is int, ""constant value needs to be of type int""
        if isinstance(name, sympy.Symbol):
            self.constants[name] = value
        else:
            self.constants[symbol_pos_int(name)] = value","

def func1(arg1, arg2, arg3):
    assert (isinstance(arg2, var2) or isinstance(arg2, sympy.Symbol)), 'constant name needs to be of type str, unicode or a sympy.Symbol'
    assert (type(arg3) is var4), 'constant value needs to be of type int'
    if isinstance(arg2, sympy.Symbol):
        self.constants[arg2] = arg3
    else:
        self.constants[symbol_pos_int(arg2)] = arg3
",Set constant of name to value .,Set a constant .,This function is used to set the value of a constant .,1,Tie,
"def download_price(self, symbol: str, currency: str, agent: str) -> PriceModel:
        
        price = self.__download_price(symbol, currency, agent)
        self.save()
        return price","

def func1(arg1, arg2: str, arg3: str, arg4: str) -> var2:
    var1 = self.__download_price(arg2, arg3, arg4)
    self.save()
    return var1
",Download and save price online,Function to download and save the price .,Download a price for a symbol and currency .,0,Tie,
"def insertText(self, data, insertBefore=None):
        
        if insertBefore:
            self.insertBefore(tree.text(data), insertBefore)
        else:
            self.xml_append(tree.text(data))","

def func1(arg1, arg2, arg3=None):
    if arg3:
        self.insertBefore(tree.text(arg2), arg3)
    else:
        self.xml_append(tree.text(arg2))
",Insert data as text in the current node positioned before the start of node insertBefore or to the end of the node s text .,This function is called when an element is inserted or inserted .,Insert text data into the tree .,0,Prediction2,
"def post_notification(self, ntype, sender, *args, **kwargs):
        

        if(ntype==None or sender==None):
            raise NotificationError(
                ""Notification type and sender are required."")

        # If there are no registered observers for the type/sender pair
        if((ntype not in self.registered_types and
                None not in self.registered_types) or
            (sender not in self.registered_senders and
                None not in self.registered_senders)):
            return

        for o in self._observers_for_notification(ntype, sender):
            o(ntype, sender, *args, **kwargs)","

def func1(arg1, arg2, arg3, *arg4, **arg5):
    if ((arg2 == None) or (arg3 == None)):
        raise NotificationError('Notification type and sender are required.')
    if (((arg2 not in self.registered_types) and (None not in self.registered_types)) or ((arg3 not in self.registered_senders) and (None not in self.registered_senders))):
        return
    for var2 in self._observers_for_notification(arg2, arg3):
        o(arg2, arg3, *arg4, **arg5)
",Post notification to all registered observers .,Post a notification to all registered observers .,This function is called when a notification is received .,1,Prediction1,
"def init(self):
        

        # Drop cache flag set by .reset() method.
        cache.get('sitetrees_reset') and self.empty(init=False)

        self.cache = cache.get(
            'sitetrees', {'sitetrees': {}, 'parents': {}, 'items_by_ids': {}, 'tree_aliases': {}})","

def func1(arg1):
    (cache.get('sitetrees_reset') and self.empty(init=False))
    self.cache = cache.get('sitetrees', {'sitetrees': {}, 'parents': {}, 'items_by_ids': {}, 'tree_aliases': {}})
",Initializes local cache from Django cache .,This function is used to reset the site tree cache .,Initialize the tree cache .,0,Tie,
"def plot_time_series(self, f_start=None, f_stop=None, if_id=0, logged=True, orientation='h', MJD_time=False, **kwargs):
        

        ax = plt.gca()
        plot_f, plot_data = self.grab_data(f_start, f_stop, if_id)

        if logged and self.header[b'nbits'] >= 8:
            plot_data = db(plot_data)

        #Since the data has been squeezed, the axis for time goes away if only one bin, causing a bug with axis=1
        if len(plot_data.shape) > 1:
            plot_data = plot_data.mean(axis=1)
        else:
            plot_data = plot_data.mean()

        #Make proper time axis for plotting (but only for plotting!). Note that this makes the values inclusive.
        extent = self._calc_extent(plot_f=plot_f,plot_t=self.timestamps,MJD_time=MJD_time)
        plot_t = np.linspace(extent[2],extent[3],len(self.timestamps))

        if MJD_time:
            tlabel = ""Time [MJD]""
        else:
            tlabel = ""Time [s]""

        if logged:
            plabel = ""Power [dB]""
        else:
            plabel = ""Power [counts]""

        # Reverse oder if vertical orientation.
        if 'v' in orientation:
            plt.plot(plot_data, plot_t, **kwargs)
            plt.xlabel(plabel)

        else:
            plt.plot(plot_t, plot_data, **kwargs)
            plt.xlabel(tlabel)
            plt.ylabel(plabel)

        ax.autoscale(axis='both',tight=True)","

def func1(arg1, arg2=None, arg3=None, arg4=0, arg5=True, arg6='h', arg7=False, **arg8):
    var1 = plt.gca()
    (var2, var3) = self.grab_data(arg2, arg3, arg4)
    if (arg5 and (self.header[b'nbits'] >= 8)):
        var3 = db(var3)
    if (len(plot_data.shape) > 1):
        var3 = plot_data.mean(axis=1)
    else:
        var3 = plot_data.mean()
    var6 = self._calc_extent(plot_f=var2, plot_t=self.timestamps, MJD_time=arg7)
    var7 = np.linspace(var6[2], var6[3], len(self.timestamps))
    if arg7:
        var8 = 'Time [MJD]'
    else:
        var8 = 'Time [s]'
    if arg5:
        var9 = 'Power [dB]'
    else:
        var9 = 'Power [counts]'
    if ('v' in arg6):
        plt.plot(var3, var7, **arg8)
        plt.xlabel(var9)
    else:
        plt.plot(var7, var3, **arg8)
        plt.xlabel(var8)
        plt.ylabel(var9)
    ax.autoscale(axis='both', tight=True)
",Plot the time series .,Plot time series .,Function 1 for plotting data .,1,Tie,
"def parse_agi_result(line):
    
    # print(""--------------\n"", line)
    if line == 'HANGUP':
        return {'error': 'AGIResultHangup',
                'msg': 'User hungup during execution'}

    kwargs = dict(code=0, response="""", line=line)
    m = re_code.search(line)
    try:
        kwargs.update(m.groupdict())
    except AttributeError:
        # None has no attribute groupdict
        pass
    return agi_code_check(**kwargs)","

def func1(arg1):
    if (arg1 == 'HANGUP'):
        return {'error': 'AGIResultHangup', 'msg': 'User hungup during execution'}
    var1 = dict(code=0, response='', line=arg1)
    var3 = re_code.search(arg1)
    try:
        kwargs.update(m.groupdict())
    except AttributeError:
        pass
    return agi_code_check(**var1)
",Parse AGI results using Regular expression .,Parse an agi result line .,This function is called by agi_code_check and agi_code2 .,1,Prediction1,
"def plot_filters(filters):
    ''''''
    imgs = filters.get_value()

    N, channels, x, y = imgs.shape
    n = int(np.sqrt(N))
    assert n * n == N, 'filters must contain a square number of rows!'
    assert channels == 1 or channels == 3, 'can only plot grayscale or rgb filters!'

    img = np.zeros(((y+1) * n - 1, (x+1) * n - 1, channels), dtype=imgs[0].dtype)
    for i, pix in enumerate(imgs):
        r, c = divmod(i, n)
        img[r * (y+1):(r+1) * (y+1) - 1,
            c * (x+1):(c+1) * (x+1) - 1] = pix.transpose((1, 2, 0))

    img -= img.min()
    img /= img.max()

    ax = plt.gcf().add_subplot(111)
    ax.xaxis.set_visible(False)
    ax.yaxis.set_visible(False)
    ax.set_frame_on(False)
    ax.imshow(img.squeeze(), cmap=plt.cm.gray)","

def func1(arg1):
    ''
    var1 = filters.get_value()
    (var2, var3, var4, var5) = imgs.shape
    var6 = int(np.sqrt(var2))
    assert ((var6 * var6) == var2), 'filters must contain a square number of rows!'
    assert ((var3 == 1) or (var3 == 3)), 'can only plot grayscale or rgb filters!'
    var8 = np.zeros(((((var5 + 1) * var6) - 1), (((var4 + 1) * var6) - 1), var3), dtype=imgs[0].dtype)
    for (var9, var10) in enumerate(var1):
        (var12, var13) = divmod(var9, var6)
        var8[(var12 * (var5 + 1)):(((var12 + 1) * (var5 + 1)) - 1), (var13 * (var4 + 1)):(((var13 + 1) * (var4 + 1)) - 1)] = pix.transpose((1, 2, 0))
    var8 -= img.min()
    var8 /= img.max()
    var15 = plt.gcf().add_subplot(111)
    ax.xaxis.set_visible(False)
    ax.yaxis.set_visible(False)
    ax.set_frame_on(False)
    ax.imshow(img.squeeze(), cmap=plt.cm.gray)
",Create a plot of conv filters visualized as pixel arrays .,plot a grayscale or rgb filter,Function to plot grayscale or rgb filters .,1,Tie,
"def raises(*exceptions):
    
    valid = ' or '.join([e.__name__ for e in exceptions])
    def decorate(func):
        name = func.__name__
        def newfunc(*arg, **kw):
            try:
                func(*arg, **kw)
            except exceptions:
                pass
            except:
                raise
            else:
                message = ""%s() did not raise %s"" % (name, valid)
                raise AssertionError(message)
        newfunc = make_decorator(func)(newfunc)
        return newfunc
    return decorate","

def func3(*arg1):
    var1 = ' or '.join([e.__name__ for var2 in arg1])

    def func2(arg2):
        var3 = func.__name__

        def func1(*arg3, **arg4):
            try:
                func(*arg3, **arg4)
            except exceptions:
                pass
            except:
                raise
            else:
                var4 = ('%s() did not raise %s' % (var3, var1))
                raise AssertionError(var4)
        func1 = var6(arg2)(func1)
        return func1
    return func2
",Test must raise one of expected exceptions to pass .,Return a function that can be used as a decorator .,Decorator that ensures that the decorated function raises any of the given exceptions .,0,Prediction2,
"def analysis(self, morf):
        
        f, s, _, m, mf = self.analysis2(morf)
        return f, s, m, mf","

def func1(arg1, arg2):
    (var1, var2, var3, var4, var5) = self.analysis2(arg2)
    return (var1, var2, var4, var5)
",Like analysis2 but doesn t return excluded line numbers .,Returns the analysis of the given morf .,func1 returns a tuple of the first and second arguments,1,Prediction1,self.analysis in corrupted code should be self.func1
"def decompose(hangul_letter):
    

    from . import checker

    if len(hangul_letter) < 1:
        raise NotLetterException('')
    elif not checker.is_hangul(hangul_letter):
        raise NotHangulException('')

    if hangul_letter in CHO:
        return hangul_letter, '', ''

    if hangul_letter in JOONG:
        return '', hangul_letter, ''

    if hangul_letter in JONG:
        return '', '', hangul_letter

    code = hangul_index(hangul_letter)
    cho, joong, jong = decompose_index(code)

    if cho < 0:
        cho = 0

    try:
        return CHO[cho], JOONG[joong], JONG[jong]
    except:
        print(""%d / %d  / %d""%(cho, joong, jong))
        print(""%s / %s "" %( JOONG[joong].encode(""utf8""), JONG[jong].encode('utf8')))
        raise Exception()","

def func1(arg1):
    from . import checker
    if (len(arg1) < 1):
        raise NotLetterException('')
    elif (not checker.is_hangul(arg1)):
        raise NotHangulException('')
    if (arg1 in var4):
        return (arg1, '', '')
    if (arg1 in var5):
        return ('', arg1, '')
    if (arg1 in var6):
        return ('', '', arg1)
    var7 = hangul_index(arg1)
    (var9, var10, var11) = decompose_index(var7)
    if (var9 < 0):
        var9 = 0
    try:
        return (var4[var9], var5[var10], var6[var11])
    except:
        print(('%d / %d  / %d' % (var9, var10, var11)))
        print(('%s / %s ' % (JOONG[joong].encode('utf8'), JONG[jong].encode('utf8'))))
        raise Exception()
",This function returns letters by decomposing the specified Hangul letter .,Decompose a Hangul letter into its components .,A function to extract a value from a string .,1,Prediction1,discuss
"def _explored_parameters_in_group(traj, group_node):
    
    explored = False
    for param in traj.f_get_explored_parameters():
            if param in group_node:
                explored = True
                break

    return explored","

def func1(arg1, arg2):
    var1 = False
    for var2 in traj.f_get_explored_parameters():
        if (var2 in arg2):
            var1 = True
            break
    return var1
",Checks if one the parameters in group_node is explored .,Return True if all parameters in group_node are explored .,returns True if arg1 is in arg2 False otherwise,1,Prediction1,code corruption incorrect
"def localmax(x, axis=0):
    

    paddings = [(0, 0)] * x.ndim
    paddings[axis] = (1, 1)

    x_pad = np.pad(x, paddings, mode='edge')

    inds1 = [slice(None)] * x.ndim
    inds1[axis] = slice(0, -2)

    inds2 = [slice(None)] * x.ndim
    inds2[axis] = slice(2, x_pad.shape[axis])

    return (x > x_pad[tuple(inds1)]) & (x >= x_pad[tuple(inds2)])","

def func1(arg1, arg2=0):
    var1 = ([(0, 0)] * x.ndim)
    var1[arg2] = (1, 1)
    var2 = np.pad(arg1, var1, mode='edge')
    var3 = ([slice(None)] * x.ndim)
    var3[arg2] = slice(0, (- 2))
    var5 = ([slice(None)] * x.ndim)
    var5[arg2] = slice(2, x_pad.shape[arg2])
    return ((arg1 > var2[tuple(var3)]) & (arg1 >= var2[tuple(var5)]))
",Find local maxima in an array x .,Return True if arg1 > arg2 .,Returns True if x is the maximum of the given axis .,0,Prediction2,code corruption incorrect
"def standard(cls, element):
        

        # We remove all special characters and return the formatted string.
        return (
            Regex(element, cls.regex_replace, replace_with=""@funilrys"")
            .replace()
            .replace(""@funilrys"", """")
        )","

def func1(arg1, arg2):
    return Regex(element, cls.regex_replace, replace_with='@funilrys').replace().replace('@funilrys', '')
",Implement the standard and alphabetical sorting .,Returns a Regex that matches arg1 and arg2 .,Return the standard string for the given element .,0,Prediction2,
"def tiny(x):
    '''
    '''

    # Make sure we have an array view
    x = np.asarray(x)

    # Only floating types generate a tiny
    if np.issubdtype(x.dtype, np.floating) or np.issubdtype(x.dtype, np.complexfloating):
        dtype = x.dtype
    else:
        dtype = np.float32

    return np.finfo(dtype).tiny","

def func1(arg1):
    '\n    '
    arg1 = np.asarray(arg1)
    if (np.issubdtype(x.dtype, np.floating) or np.issubdtype(x.dtype, np.complexfloating)):
        var1 = x.dtype
    else:
        var1 = np.float32
    return np.finfo(dtype).tiny
",Compute the tiny - value corresponding to an input s data type .,\ n,Convert an array of floats to float32 .,0,Prediction2,
"def position(self):
    
    pos = self._position
    if pos is None and self.children:
      ch1 = self.children[0]
      if isinstance(ch1, ParseNode):
        pos = ch1.position
    return pos","

def func1(arg1):
    var1 = self._position
    if ((var1 is None) and self.children):
        var2 = self.children[0]
        if isinstance(var2, var4):
            var1 = ch1.position
    return var1
",Gets the position of the text the ParseNode processed . If the ParseNode does not have its own position it looks to its first child for its position .,Return the position of the first child of this node .,Return the current position of the parse node .,0,Tie,
"def _find_jar(self, path0=None):
        
        jar_paths = [path0] if path0 else self._jar_paths()
        searched_paths = []
        for jp in jar_paths:
            searched_paths.append(jp)
            if os.path.exists(jp):
                return jp
        raise H2OStartupError(""Cannot start local server: h2o.jar not found. Paths searched:\n"" +
                              """".join(""    %s\n"" % s for s in searched_paths))","

def func1(arg1, arg2=None):
    var1 = ([arg2] if arg2 else self._jar_paths())
    var2 = []
    for var3 in var1:
        searched_paths.append(var3)
        if os.path.exists(var3):
            return var3
    raise H2OStartupError(('Cannot start local server: h2o.jar not found. Paths searched:\n' + ''.join((('    %s\n' % var5) for var5 in var2))))
",Return the location of an h2o . jar executable .,Return the path to the h2o . jar file .,Find the path to the h2o . jar file .,0,Tie,
"def iter_osm_notes(feed_limit=25, interval=60, parse_timestamps=True):
    

    last_seen_guid = None
    while True:
        u = urllib2.urlopen('https://www.openstreetmap.org/api/0.6/notes/feed?limit=%d' % feed_limit)

        tree = etree.parse(u)

        new_notes = []
        for note_item in tree.xpath('/rss/channel/item'):
            title = note_item.xpath('title')[0].text

            if title.startswith('new note ('):
                action = 'create'
            elif title.startswith('new comment ('):
                action = 'comment'
            elif title.startswith('closed note ('):
                action = 'close'

            # Note that (at least for now) the link and guid are the same in the feed.
            guid = note_item.xpath('link')[0].text

            if last_seen_guid == guid:
                break
            elif last_seen_guid == None:
                # The first time through we want the first item to be the ""last seen""
                # because the RSS feed is newest-to-oldest
                last_seen_guid = guid
            else:
                note_id = int(guid.split('/')[-1].split('#c')[0])
                new_notes.append((action, get_note(note_id, parse_timestamps)))

        # We yield the reversed list because we want to yield in change order
        # (i.e. ""oldest to most current"")
        for note in reversed(new_notes):
            yield note

        yield model.Finished(None, None)

        time.sleep(interval)","

def func1(arg1=25, arg2=60, arg3=True):
    var1 = None
    while True:
        var2 = urllib2.urlopen(('https://www.openstreetmap.org/api/0.6/notes/feed?limit=%d' % arg1))
        var3 = etree.parse(var2)
        var4 = []
        for var5 in tree.xpath('/rss/channel/item'):
            var6 = note_item.xpath('title')[0].text
            if title.startswith('new note ('):
                var7 = 'create'
            elif title.startswith('new comment ('):
                var7 = 'comment'
            elif title.startswith('closed note ('):
                var7 = 'close'
            var8 = note_item.xpath('link')[0].text
            if (var1 == var8):
                break
            elif (var1 == None):
                var1 = var8
            else:
                var9 = int(guid.split('/')[(- 1)].split('#c')[0])
                new_notes.append((var7, get_note(var9, arg3)))
        for var12 in reversed(var4):
            (yield var12)
        (yield model.Finished(None, None))
        time.sleep(arg2)
",Parses the global OSM Notes feed and yields as much Note information as possible .,A generator function that returns a list of new notes .,Return an iterator over the OpenStreetMap notes .,0,Prediction2,
"def list_filenames_in_directory(self, dirname):
        
        prefix = self._create_prefix(dirname)
        filenames = []
        for obj in self._bucket.objects.filter(Prefix=prefix):
            if obj.key.endswith('/'):
                # a directory redirect object, not a file
                continue
            obj_dirname = os.path.dirname(obj.key)
            if obj_dirname == prefix:
                # object is at root of directory
                filenames.append(os.path.relpath(obj.key,
                                                 start=prefix))
        return filenames","

def func1(arg1, arg2):
    var1 = self._create_prefix(arg2)
    var2 = []
    for var3 in self._bucket.objects.filter(Prefix=var1):
        if obj.key.endswith('/'):
            continue
        var4 = os.path.dirname(obj.key)
        if (var4 == var1):
            filenames.append(os.path.relpath(obj.key, start=var1))
    return var2
",List all file - type object names that exist at the root of this bucket directory .,Return a list of all filenames in a directory .,Return a list of filenames that are relative to arg2 .,1,Prediction1,
"def get_reason(self):
        
        for i in range(_lib.X509_REVOKED_get_ext_count(self._revoked)):
            ext = _lib.X509_REVOKED_get_ext(self._revoked, i)
            obj = _lib.X509_EXTENSION_get_object(ext)
            if _lib.OBJ_obj2nid(obj) == _lib.NID_crl_reason:
                bio = _new_mem_buf()

                print_result = _lib.X509V3_EXT_print(bio, ext, 0, 0)
                if not print_result:
                    print_result = _lib.M_ASN1_OCTET_STRING_print(
                        bio, _lib.X509_EXTENSION_get_data(ext)
                    )
                    _openssl_assert(print_result != 0)

                return _bio_to_string(bio)","

def func1(arg1):
    for var1 in range(_lib.X509_REVOKED_get_ext_count(self._revoked)):
        var3 = _lib.X509_REVOKED_get_ext(self._revoked, var1)
        var4 = _lib.X509_EXTENSION_get_object(var3)
        if (_lib.OBJ_obj2nid(var4) == _lib.NID_crl_reason):
            var5 = _new_mem_buf()
            var7 = _lib.X509V3_EXT_print(var5, var3, 0, 0)
            if (not var7):
                var7 = _lib.M_ASN1_OCTET_STRING_print(var5, _lib.X509_EXTENSION_get_data(var3))
                _openssl_assert((var7 != 0))
            return _bio_to_string(var5)
",Get the reason of this revocation .,Get the reason for this certificate .,X509_REVOKED_get_ext function .,1,Prediction1,
"def start_event_loop_wx(app=None):
    
    if app is None:
        app = get_app_wx()
    if not is_event_loop_running_wx(app):
        app._in_event_loop = True
        app.MainLoop()
        app._in_event_loop = False
    else:
        app._in_event_loop = True","

def func1(arg1=None):
    if (arg1 is None):
        arg1 = get_app_wx()
    if (not is_event_loop_running_wx(arg1)):
        app._in_event_loop = True
        app.MainLoop()
        app._in_event_loop = False
    else:
        app._in_event_loop = True
",Start the wx event loop in a consistent manner .,This function is called when a wx event loop is started .,Starts the wx event loop .,0,Tie,
"def add_files(self, *filenames: str, owner: str=SANDBOX_USERNAME, read_only: bool=False):
        
        if owner != SANDBOX_USERNAME and owner != 'root':
            raise ValueError('Invalid value for parameter ""owner"": {}'.format(owner))

        with tempfile.TemporaryFile() as f, \
                tarfile.TarFile(fileobj=f, mode='w') as tar_file:
            for filename in filenames:
                tar_file.add(filename, arcname=os.path.basename(filename))

            f.seek(0)
            subprocess.check_call(
                ['docker', 'cp', '-',
                 self.name + ':' + SANDBOX_WORKING_DIR_NAME],
                stdin=f)

            file_basenames = [os.path.basename(filename) for filename in filenames]
            if owner == SANDBOX_USERNAME:
                self._chown_files(file_basenames)

            if read_only:
                chmod_cmd = ['chmod', '444'] + file_basenames
                self.run_command(chmod_cmd, as_root=True)","

def func1(arg1, *arg2: str, arg3: str=var1, arg4: bool=False):
    if ((arg3 != var1) and (arg3 != 'root')):
        raise ValueError('Invalid value for parameter ""owner"": {}'.format(arg3))
    with tempfile.TemporaryFile() as var3, tarfile.TarFile(fileobj=var3, mode='w') as var4:
        for var5 in arg2:
            tar_file.add(var5, arcname=os.path.basename(var5))
        f.seek(0)
        subprocess.check_call(['docker', 'cp', '-', ((self.name + ':') + var6)], stdin=var3)
        var7 = [os.path.basename(var5) for var5 in arg2]
        if (arg3 == var1):
            self._chown_files(var7)
        if arg4:
            var8 = (['chmod', '444'] + var7)
            self.run_command(var8, as_root=True)
",Copies the specified files into the working directory of this sandbox . The filenames specified can be absolute paths or relative paths to the current working directory .,Run a docker command .,Add files to the container .,0,Prediction2,
"def upload(request):
    
    # Get the optional parameters
    title = request.GET.get(""title"", ""%s's video on %s"" % (
        request.user.username, request.get_host()))
    description = request.GET.get(""description"", """")
    keywords = request.GET.get(""keywords"", """")

    # Try to create post_url and token to create an upload form
    try:
        api = Api()

        # upload method needs authentication
        api.authenticate()

        # Customize following line to your needs, you can add description, keywords or developer_keys
        # I prefer to update video information after upload finishes
        data = api.upload(title, description=description, keywords=keywords,
                          access_control=AccessControl.Unlisted)
    except ApiError as e:
        # An api error happened, redirect to homepage
        messages.add_message(request, messages.ERROR, e.message)
        return HttpResponseRedirect(""/"")
    except:
        # An error happened, redirect to homepage
        messages.add_message(request, messages.ERROR, _(
            'An error occurred during the upload, Please try again.'))
        return HttpResponseRedirect(""/"")

    # Create the form instance
    form = YoutubeUploadForm(initial={""token"": data[""youtube_token""]})

    protocol = 'https' if request.is_secure() else 'http'
    next_url = '%s://%s%s/' % (protocol, request.get_host(), reverse(""django_youtube.views.upload_return""))
    return render_to_response(
        ""django_youtube/upload.html"",
        {""form"": form, ""post_url"": data[""post_url""], ""next_url"": next_url},
        context_instance=RequestContext(request)
    )","

def func1(arg1):
    var1 = request.GET.get('title', (""%s's video on %s"" % (request.user.username, request.get_host())))
    var2 = request.GET.get('description', '')
    var3 = request.GET.get('keywords', '')
    try:
        var4 = Api()
        api.authenticate()
        var6 = api.upload(var1, description=var2, keywords=var3, access_control=AccessControl.Unlisted)
    except ApiError as e:
        messages.add_message(arg1, messages.ERROR, e.message)
        return HttpResponseRedirect('/')
    except:
        messages.add_message(arg1, messages.ERROR, _('An error occurred during the upload, Please try again.'))
        return HttpResponseRedirect('/')
    var10 = YoutubeUploadForm(initial={'token': var6['youtube_token']})
    var12 = ('https' if request.is_secure() else 'http')
    var13 = ('%s://%s%s/' % (var12, request.get_host(), reverse('django_youtube.views.upload_return')))
    return render_to_response('django_youtube/upload.html', {'form': var10, 'post_url': var6['post_url'], 'next_url': var13}, context_instance=RequestContext(arg1))
",Displays an upload form Creates upload url and token from youtube api and uses them on the form,Upload a video to Youtube .,Uploads a video to the Youtube API .,0,Tie,
"def db_list(username=None, password=None, host=None, port=None,
        maintain_db='postgres'):
    """"

    conn = _connection(username=username, password=password, host=host,
        port=port, db=maintain_db)

    cur = conn.cursor()

    cur.execute('SELECT DATNAME from pg_database')
    rows = cur.fetchall()

    conn.close()

    result = []
    for row in rows:
        result.append(row[0])

    return result","

def func1(arg1=None, arg2=None, arg3=None, arg4=None, arg5='postgres'):
    ''
    var1 = _connection(username=arg1, password=arg2, host=arg3, port=arg4, db=arg5)
    var3 = conn.cursor()
    cur.execute('SELECT DATNAME from pg_database')
    var4 = cur.fetchall()
    conn.close()
    var5 = []
    for var6 in var4:
        result.append(var6[0])
    return var5
",returns a list of all databases on this server,function 1 returns a list of database names,returns a list of all databases on this server,0,Tie,
"def add_list(self, query_params=None):
        '''
        
        '''
        list_json = self.fetch_json(
            uri_path=self.base_uri + '/lists',
            http_method='POST',
            query_params=query_params or {}
        )

        return self.create_list(list_json)","

def func1(arg1, arg2=None):
    '\n        \n        '
    var1 = self.fetch_json(uri_path=(self.base_uri + '/lists'), http_method='POST', query_params=(arg2 or {}))
    return self.create_list(var1)
",Create a list for a board . Returns a new List object .,\ n \ n,Add a list to the user s account .,0,Prediction2,
"def save_task_request(self, idents, msg):
        
        client_id = idents[0]

        try:
            msg = self.session.unserialize(msg)
        except Exception:
            self.log.error(""task::client %r sent invalid task message: %r"",
                    client_id, msg, exc_info=True)
            return
        record = init_record(msg)

        record['client_uuid'] = client_id.decode('ascii')
        record['queue'] = 'task'
        header = msg['header']
        msg_id = header['msg_id']
        self.pending.add(msg_id)
        self.unassigned.add(msg_id)
        try:
            # it's posible iopub arrived first:
            existing = self.db.get_record(msg_id)
            if existing['resubmitted']:
                for key in ('submitted', 'client_uuid', 'buffers'):
                    # don't clobber these keys on resubmit
                    # submitted and client_uuid should be different
                    # and buffers might be big, and shouldn't have changed
                    record.pop(key)
                    # still check content,header which should not change
                    # but are not expensive to compare as buffers

            for key,evalue in existing.iteritems():
                if key.endswith('buffers'):
                    # don't compare buffers
                    continue
                rvalue = record.get(key, None)
                if evalue and rvalue and evalue != rvalue:
                    self.log.warn(""conflicting initial state for record: %r:%r <%r> %r"", msg_id, rvalue, key, evalue)
                elif evalue and not rvalue:
                    record[key] = evalue
            try:
                self.db.update_record(msg_id, record)
            except Exception:
                self.log.error(""DB Error updating record %r"", msg_id, exc_info=True)
        except KeyError:
            try:
                self.db.add_record(msg_id, record)
            except Exception:
                self.log.error(""DB Error adding record %r"", msg_id, exc_info=True)
        except Exception:
            self.log.error(""DB Error saving task request %r"", msg_id, exc_info=True)","

def func1(arg1, arg2, arg3):
    var1 = arg2[0]
    try:
        arg3 = self.session.unserialize(arg3)
    except Exception:
        self.log.error('task::client %r sent invalid task message: %r', var1, arg3, exc_info=True)
        return
    var3 = init_record(arg3)
    var3['client_uuid'] = client_id.decode('ascii')
    var3['queue'] = 'task'
    var5 = arg3['header']
    var6 = var5['msg_id']
    self.pending.add(var6)
    self.unassigned.add(var6)
    try:
        var7 = self.db.get_record(var6)
        if var7['resubmitted']:
            for var8 in ('submitted', 'client_uuid', 'buffers'):
                record.pop(var8)
        for (var8, var9) in existing.iteritems():
            if key.endswith('buffers'):
                continue
            var10 = record.get(var8, None)
            if (var9 and var10 and (var9 != var10)):
                self.log.warn('conflicting initial state for record: %r:%r <%r> %r', var6, var10, var8, var9)
            elif (var9 and (not var10)):
                var3[var8] = var9
        try:
            self.db.update_record(var6, var3)
        except Exception:
            self.log.error('DB Error updating record %r', var6, exc_info=True)
    except KeyError:
        try:
            self.db.add_record(var6, var3)
        except Exception:
            self.log.error('DB Error adding record %r', var6, exc_info=True)
    except Exception:
        self.log.error('DB Error saving task request %r', var6, exc_info=True)
",Save the submission of a task .,Save a task request .,Called when a task message is received from a client .,1,Prediction1,
"def einsum_matmul_index(gate_indices, number_of_qubits):
    

    mat_l, mat_r, tens_lin, tens_lout = _einsum_matmul_index_helper(gate_indices,
                                                                    number_of_qubits)

    # Right indices for the N-qubit input and output tensor
    tens_r = ascii_uppercase[:number_of_qubits]

    # Combine indices into matrix multiplication string format
    # for numpy.einsum function
    return ""{mat_l}{mat_r}, "".format(mat_l=mat_l, mat_r=mat_r) + \
           ""{tens_lin}{tens_r}->{tens_lout}{tens_r}"".format(tens_lin=tens_lin,
                                                            tens_lout=tens_lout,
                                                            tens_r=tens_r)","

def func1(arg1, arg2):
    (var1, var2, var3, var4) = _einsum_matmul_index_helper(arg1, arg2)
    var6 = var7[:arg2]
    return ('{mat_l}{mat_r}, '.format(mat_l=var1, mat_r=var2) + '{tens_lin}{tens_r}->{tens_lout}{tens_r}'.format(tens_lin=var3, tens_lout=var4, tens_r=var6))
",Return the index string for Numpy . eignsum matrix - matrix multiplication .,Return the einsum of a set of gate indices .,A function that returns a string representation of a matrix multiplication .,1,Prediction2,
"def clearLocalServices(self):
        ''

        for service in list(self._localServices.values()):
            self._sendBye(service)

        self._localServices.clear()","

def func1(arg1):
    ''
    for var1 in list(self._localServices.values()):
        self._sendBye(var1)
    self._localServices.clear()
",send Bye messages for the services and remove them,send Bye messages for the services and remove them,This function is used to send a command to all the local services .,1,Prediction1,
"def response_body(self):
        '''
        
        '''
        var_respbody = VARIANT()
        _WinHttpRequest._ResponseBody(self, byref(var_respbody))
        if var_respbody.is_safearray_of_bytes():
            respbody = var_respbody.str_from_safearray()
            return respbody
        else:
            return ''","

def func1(arg1):
    '\n        \n        '
    var1 = VARIANT()
    _WinHttpRequest._ResponseBody(arg1, byref(var1))
    if var_respbody.is_safearray_of_bytes():
        var4 = var_respbody.str_from_safearray()
        return var4
    else:
        return ''
",Gets response body as a SAFEARRAY and converts the SAFEARRAY to str .,Returns the response body of the request .,\ n \ n,1,Prediction1,
"def my_permission_factory(record, *args, **kwargs):
    
    def can(self):
        rec = Record.get_record(record.id)
        return rec.get('access', '') == 'open'
    return type('MyPermissionChecker', (), {'can': can})()","

def func2(arg1, *arg2, **arg3):

    def func1(arg4):
        var1 = Record.get_record(record.id)
        return (rec.get('access', '') == 'open')
    return var2('MyPermissionChecker', (), {'can': func1})()
",My permission factory .,Create a permission checker for a record .,Check if record is open .,1,Prediction1,
"def read_data_by_config(config: dict):
    
    dataset_config = config.get('dataset', None)

    if dataset_config:
        config.pop('dataset')
        ds_type = dataset_config['type']
        if ds_type == 'classification':
            reader = {'class_name': 'basic_classification_reader'}
            iterator = {'class_name': 'basic_classification_iterator'}
            config['dataset_reader'] = {**dataset_config, **reader}
            config['dataset_iterator'] = {**dataset_config, **iterator}
        else:
            raise Exception(""Unsupported dataset type: {}"".format(ds_type))

    try:
        reader_config = dict(config['dataset_reader'])
    except KeyError:
        raise ConfigError(""No dataset reader is provided in the JSON config."")

    reader = get_model(reader_config.pop('class_name'))()
    data_path = reader_config.pop('data_path', '')
    if isinstance(data_path, list):
        data_path = [expand_path(x) for x in data_path]
    else:
        data_path = expand_path(data_path)

    return reader.read(data_path, **reader_config)","

def func1(arg1: dict):
    var1 = config.get('dataset', None)
    if var1:
        config.pop('dataset')
        var2 = var1['type']
        if (var2 == 'classification'):
            var3 = {'class_name': 'basic_classification_reader'}
            var4 = {'class_name': 'basic_classification_iterator'}
            arg1['dataset_reader'] = {**var1, **var3}
            arg1['dataset_iterator'] = {**var1, **var4}
        else:
            raise Exception('Unsupported dataset type: {}'.format(var2))
    try:
        var6 = dict(arg1['dataset_reader'])
    except KeyError:
        raise ConfigError('No dataset reader is provided in the JSON config.')
    var3 = var10(reader_config.pop('class_name'))()
    var11 = reader_config.pop('data_path', '')
    if isinstance(var11, var13):
        var11 = [expand_path(var15) for var15 in var11]
    else:
        var11 = expand_path(var11)
    return reader.read(var11, **var6)
",Read data by dataset_reader from specified config .,Read data from a JSON config .,Read a dataset from a JSON config .,1,Tie,
"def handle_unmet_dependency(self, idents, parent):
        
        engine = idents[0]
        msg_id = parent['msg_id']

        job = self.pending[engine].pop(msg_id)
        job.blacklist.add(engine)

        if job.blacklist == job.targets:
            self.depending[msg_id] = job
            self.fail_unreachable(msg_id)
        elif not self.maybe_run(job):
            # resubmit failed
            if msg_id not in self.all_failed:
                # put it back in our dependency tree
                self.save_unmet(job)

        if self.hwm:
            try:
                idx = self.targets.index(engine)
            except ValueError:
                pass # skip load-update for dead engines
            else:
                if self.loads[idx] == self.hwm-1:
                    self.update_graph(None)","

def func1(arg1, arg2, arg3):
    var1 = arg2[0]
    var2 = arg3['msg_id']
    var3 = self.pending[engine].pop(var2)
    job.blacklist.add(var1)
    if (job.blacklist == job.targets):
        self.depending[var2] = var3
        self.fail_unreachable(var2)
    elif (not self.maybe_run(var3)):
        if (var2 not in self.all_failed):
            self.save_unmet(var3)
    if self.hwm:
        try:
            var4 = self.targets.index(var1)
        except ValueError:
            pass
        else:
            if (self.loads[var4] == (self.hwm - 1)):
                self.update_graph(None)
",handle an unmet dependency,Called when a message is received from the engine .,Handle unmet dependencies .,0,Prediction2,
"def page(strng, start=0, screen_lines=0, pager_cmd=None,
         html=None, auto_html=False):
    

    # Some routines may auto-compute start offsets incorrectly and pass a
    # negative value.  Offset to 0 for robustness.
    start = max(0, start)
    shell = InteractiveShell.instance()

    if auto_html:
        try:
            # These defaults ensure user configuration variables for docutils
            # are not loaded, only our config is used here.
            defaults = {'file_insertion_enabled': 0,
                        'raw_enabled': 0,
                        '_disable_config': 1}
            html = publish_string(strng, writer_name='html',
                                  settings_overrides=defaults)
        except:
            pass
        
    payload = dict(
        source='IPython.zmq.page.page',
        text=strng,
        html=html,
        start_line_number=start
        )
    shell.payload_manager.write_payload(payload)","

def func1(arg1, arg2=0, arg3=0, arg4=None, arg5=None, arg6=False):
    arg2 = max(0, arg2)
    var2 = InteractiveShell.instance()
    if arg6:
        try:
            var3 = {'file_insertion_enabled': 0, 'raw_enabled': 0, '_disable_config': 1}
            arg5 = publish_string(arg1, writer_name='html', settings_overrides=var3)
        except:
            pass
    var5 = dict(source='IPython.zmq.page.page', text=arg1, html=arg5, start_line_number=arg2)
    shell.payload_manager.write_payload(var5)
",Print a string piping through a pager .,This function is called when a command is executed .,Display a page of text .,0,Prediction2,
"def hash(self):
        """"
        if self.sender is None:
            raise MissingSignatureError()

        class HashSerializable(rlp.Serializable):
            fields = [(field, sedes) for field, sedes in self.fields
                      if field not in ('v', 'r', 's')] + [('_sender', binary)]
            _sedes = None
        return sha3(rlp.encode(self, HashSerializable))","

def func1(arg1):
    ''
    if (self.sender is None):
        raise MissingSignatureError()

    class HashSerializable(rlp.Serializable):
        fields = ([(field, sedes) for (field, sedes) in self.fields if (field not in ('v', 'r', 's'))] + [('_sender', binary)])
        _sedes = None
    return sha3(rlp.encode(arg1, var3))
",signatures are non deterministic,signatures are non deterministic,Return a SHA - 3 hash of arg1 .,1,Prediction1,
"def write_to_file(self, filename, do_compress=None):
        
        is_ext_laz = filename.split(""."")[-1] == ""laz""
        if is_ext_laz and do_compress is None:
            do_compress = True
        with open(filename, mode=""wb"") as out:
            self.write_to(out, do_compress=do_compress)","

def func1(arg1, arg2, arg3=None):
    var1 = (filename.split('.')[(- 1)] == 'laz')
    if (var1 and (arg3 is None)):
        arg3 = True
    with open(arg2, mode='wb') as var3:
        self.write_to(var3, do_compress=arg3)
",Writes the las data into a file,Write the data to a file .,Write to a file .,1,Tie,
"def load_variant(self, variant_obj):
        
        # LOG.debug(""Loading variant %s"", variant_obj['_id'])
        try:
            result = self.variant_collection.insert_one(variant_obj)
        except DuplicateKeyError as err:
            raise IntegrityError(""Variant %s already exists in database"", variant_obj['_id'])
        return result","

def func1(arg1, arg2):
    try:
        var1 = self.variant_collection.insert_one(arg2)
    except DuplicateKeyError as err:
        raise IntegrityError('Variant %s already exists in database', arg2['_id'])
    return var1
",Load a variant object,Insert a variant into the database .,Load a single variant into the database .,0,Tie,
"def valid_identifiers(self):
        

        funcs = list(utils.find_all(self.contexts[-1])) + list(self.builtins)
        return funcs","

def func1(arg1):
    var1 = (list(utils.find_all(self.contexts[(- 1)])) + list(self.builtins))
    return var1
",Get a list of all valid identifiers for the current context .,Return a list of all valid identifiers .,Returns a list of variable names for the first argument .,1,Tie,
"def update_storage_account(self, service_name, description=None,
                               label=None, geo_replication_enabled=None,
                               extended_properties=None,
                               account_type='Standard_GRS'):
        '''
        
        '''
        _validate_not_none('service_name', service_name)
        if geo_replication_enabled == False:
            account_type = 'Standard_LRS'
        return self._perform_put(
            self._get_storage_service_path(service_name),
            _XmlSerializer.update_storage_service_input_to_xml(
                description,
                label,
                account_type,
                extended_properties))","

def func1(arg1, arg2, arg3=None, arg4=None, arg5=None, arg6=None, arg7='Standard_GRS'):
    '\n        \n        '
    _validate_not_none('service_name', arg2)
    if (arg5 == False):
        arg7 = 'Standard_LRS'
    return self._perform_put(self._get_storage_service_path(arg2), _XmlSerializer.update_storage_service_input_to_xml(arg3, arg4, arg7, arg6))
",Updates the label the description and enables or disables the geo - replication status for a storage account in Windows Azure .,\ n \ n,Updates a storage account .,0,Prediction2,
"def subscribeToDeviceEvents(self, typeId=""+"", deviceId=""+"", eventId=""+"", msgFormat=""+"", qos=0):
        
        if self._config.isQuickstart() and deviceId == ""+"":
            self.logger.warning(
                ""QuickStart applications do not support wildcard subscription to events from all devices""
            )
            return 0

        topic = ""iot-2/type/%s/id/%s/evt/%s/fmt/%s"" % (typeId, deviceId, eventId, msgFormat)
        return self._subscribe(topic, qos)","

def func1(arg1, arg2='+', arg3='+', arg4='+', arg5='+', arg6=0):
    if (self._config.isQuickstart() and (arg3 == '+')):
        self.logger.warning('QuickStart applications do not support wildcard subscription to events from all devices')
        return 0
    var1 = ('iot-2/type/%s/id/%s/evt/%s/fmt/%s' % (arg2, arg3, arg4, arg5))
    return self._subscribe(var1, arg6)
",Subscribe to device event messages,Subscribe to events from a device .,Subscribe to events from all devices .,1,Prediction1,
"def plot_tree(T, res=None, title=None, cmap_id=""Pastel2""):
    
    import matplotlib.pyplot as plt
    def round_time(t, res=0.1):
        v = int(t / float(res)) * res
        return v

    # Get color map
    cmap = plt.get_cmap(cmap_id)

    # Get segments by level
    level_bounds = []
    for level in T.levels:
        if level == ""root"":
            continue
        segments = T.get_segments_in_level(level)
        level_bounds.append(segments)

    # Plot axvspans for each segment
    B = float(len(level_bounds))
    #plt.figure(figsize=figsize)
    for i, segments in enumerate(level_bounds):
        labels = utils.segment_labels_to_floats(segments)
        for segment, label in zip(segments, labels):
            #print i, label, cmap(label)
            if res is None:
                start = segment.start
                end = segment.end
                xlabel = ""Time (seconds)""
            else:
                start = int(round_time(segment.start, res=res) / res)
                end = int(round_time(segment.end, res=res) / res)
                xlabel = ""Time (frames)""
            plt.axvspan(start, end,
                        ymax=(len(level_bounds) - i) / B,
                        ymin=(len(level_bounds) - i - 1) / B,
                        facecolor=cmap(label))

    # Plot labels
    L = float(len(T.levels) - 1)
    plt.yticks(np.linspace(0, (L - 1) / L, num=L) + 1 / L / 2.,
               T.levels[1:][::-1])
    plt.xlabel(xlabel)
    if title is not None:
        plt.title(title)
    plt.gca().set_xlim([0, end])","

def func2(arg1, arg2=None, arg3=None, arg4='Pastel2'):
    import matplotlib.pyplot as plt

    def func1(arg5, arg2=0.1):
        var1 = (int((arg5 / float(arg2))) * arg2)
        return var1
    var4 = plt.get_cmap(arg4)
    var5 = []
    for var6 in T.levels:
        if (var6 == 'root'):
            continue
        var7 = T.get_segments_in_level(var6)
        level_bounds.append(var7)
    var8 = float(len(var5))
    for (var10, var7) in enumerate(var5):
        var12 = utils.segment_labels_to_floats(var7)
        for (var13, var14) in zip(var7, var12):
            if (arg2 is None):
                var16 = segment.start
                var17 = segment.end
                var18 = 'Time (seconds)'
            else:
                var16 = int((round_time(segment.start, res=arg2) / arg2))
                var17 = int((round_time(segment.end, res=arg2) / arg2))
                var18 = 'Time (frames)'
            plt.axvspan(var16, var17, ymax=((len(var5) - var10) / var8), ymin=(((len(var5) - var10) - 1) / var8), facecolor=cmap(var14))
    var19 = float((len(T.levels) - 1))
    plt.yticks((np.linspace(0, ((var19 - 1) / var19), num=var19) + ((1 / var19) / 2.0)), T.levels[1:][::(- 1)])
    plt.xlabel(var18)
    if (arg3 is not None):
        plt.title(arg3)
    plt.gca().set_xlim([0, var17])
",Plots a given tree containing hierarchical segmentation .,Plot a tree .,Returns a function that can be used to plot a 2D plot .,1,Tie,
"def expand_abbreviations(self, text):
        
        if not self.abbreviations:
            raise LexiconError(""No abbreviations in lexicon."")

        def chunks(data, SIZE=25):
            """"""
            Regex only supports 100 groups for munging callbacks. So we have to
            chunk the abbreviation dicitonary.
            """"""
            it = iter(data)
            for i in range(0, len(data), SIZE):
                yield {k: data[k] for k in islice(it, SIZE)}

        def cb(g):
            """"""Regex callback""""""
            return self.abbreviations.get(g.group(0)) or g.group(0)

        # Special cases.

        # TODO: We should handle these with a special set of
        # replacements that are made before the others.
        text = re.sub(r'w/', r'wi', text)

        # Main loop.
        for subdict in chunks(self.abbreviations):
            regex = r'(\b' + r'\b)|(\b'.join(subdict.keys()) + r'\b)'
            text = re.sub(regex, cb, text)

        return text","

def func3(arg1, arg2):
    if (not self.abbreviations):
        raise LexiconError('No abbreviations in lexicon.')

    def func1(arg3, arg4=25):
        '\n            Regex only supports 100 groups for munging callbacks. So we have to\n            chunk the abbreviation dicitonary.\n            '
        var2 = iter(arg3)
        for var4 in range(0, len(arg3), arg4):
            (yield {var7: arg3[var7] for var7 in islice(var2, arg4)})

    def func2(arg5):
        'Regex callback'
        return (self.abbreviations.get(g.group(0)) or g.group(0))
    arg2 = re.sub('w/', 'wi', arg2)
    for var9 in chunks(self.abbreviations):
        var10 = (('(\\b' + '\\b)|(\\b'.join(subdict.keys())) + '\\b)')
        arg2 = re.sub(var10, func2, arg2)
    return arg2
",Parse a piece of text and replace any abbreviations with their full word equivalents . Uses the lexicon . abbreviations dictionary to find abbreviations .,Regex callback for munging .,Expand abbreviations in text .,0,Prediction2,
"def _minimal_export_traces(self, outdir=None, analytes=None,
                               samples=None, subset='All_Analyses'):
        
        if analytes is None:
            analytes = self.analytes
        elif isinstance(analytes, str):
            analytes = [analytes]

        if samples is not None:
            subset = self.make_subset(samples)

        samples = self._get_samples(subset)

        focus_stage = 'rawdata'
        # ud = 'counts'

        if not os.path.isdir(outdir):
            os.mkdir(outdir)

        for s in samples:
            d = self.data[s].data[focus_stage]
            out = Bunch()

            for a in analytes:
                out[a] = d[a]

            out = pd.DataFrame(out, index=self.data[s].Time)
            out.index.name = 'Time'

            d = dateutil.parser.parse(self.data[s].meta['date'])
            header = ['# Minimal Reproduction Dataset Exported from LATOOLS on %s' %
                      (time.strftime('%Y:%m:%d %H:%M:%S')),
                      ""# Analysis described in '../analysis.lalog'"",
                      '# Run latools.reproduce to import analysis.',
                      '#',
                      '# Sample: %s' % (s),
                      '# Analysis Time: ' + d.strftime('%Y-%m-%d %H:%M:%S')]

            header = '\n'.join(header) + '\n'

            csv = out.to_csv()

            with open('%s/%s.csv' % (outdir, s), 'w') as f:
                f.write(header)
                f.write(csv)
        return","

def func1(arg1, arg2=None, arg3=None, arg4=None, arg5='All_Analyses'):
    if (arg3 is None):
        arg3 = self.analytes
    elif isinstance(arg3, var2):
        arg3 = [arg3]
    if (arg4 is not None):
        arg5 = self.make_subset(arg4)
    arg4 = self._get_samples(arg5)
    var3 = 'rawdata'
    if (not os.path.isdir(arg2)):
        os.mkdir(arg2)
    for var4 in arg4:
        var5 = self.data[s].data[var3]
        var6 = Bunch()
        for var8 in arg3:
            var6[var8] = var5[var8]
        var6 = pd.DataFrame(var6, index=self.data[s].Time)
        out.index.name = 'Time'
        var5 = dateutil.parser.parse(self.data[s].meta['date'])
        var9 = [('# Minimal Reproduction Dataset Exported from LATOOLS on %s' % time.strftime('%Y:%m:%d %H:%M:%S')), ""# Analysis described in '../analysis.lalog'"", '# Run latools.reproduce to import analysis.', '#', ('# Sample: %s' % var4), ('# Analysis Time: ' + d.strftime('%Y-%m-%d %H:%M:%S'))]
        var9 = ('\n'.join(var9) + '\n')
        var10 = out.to_csv()
        with open(('%s/%s.csv' % (arg2, var4)), 'w') as var12:
            f.write(var9)
            f.write(var10)
    return
",Used for exporting minimal dataset . DON T USE .,Minimal Reproduction Dataset Exported from LATOOLS,Minimal Reproduction Dataset Exported from LATOOLS,0,Tie,
"def gravatar_get_url(obj, size=65, default='identicon'):
    
    return get_gravatar_url(obj, size=size, default=default)","

def func1(arg1, arg2=65, arg3='identicon'):
    return get_gravatar_url(arg1, size=arg2, default=arg3)
",Returns Gravatar image URL for a given string or UserModel .,Get gravatar url .,Get a gravatar URL for an object .,0,Tie,
"def get_comments(self, issue_id):
        
        url = urijoin(self.base_url, self.RESOURCE, self.VERSION_API, self.ISSUE, issue_id, self.COMMENT)
        comments = self.get_items(DEFAULT_DATETIME, url, expand_fields=False)

        return comments","

def func1(arg1, arg2):
    var1 = urijoin(self.base_url, self.RESOURCE, self.VERSION_API, self.ISSUE, arg2, self.COMMENT)
    var3 = self.get_items(var4, var1, expand_fields=False)
    return var3
",Retrieve all the comments of a given issue .,Get a list of comments for a given issue .,Get a list of items from a resource .,1,Prediction1,
"def generate():
    
    old_dir = os.getcwd()
    proj_dir = os.path.join(os.path.dirname(__file__), os.pardir)
    os.chdir(proj_dir)
    version = validate_version()
    if not version:
        os.chdir(old_dir)
        return
    print('Generating changelog for version {}'.format(version))
    options = [
        '--user', 'arve0', '--project', 'leicacam', '-v', '--with-unreleased',
        '--future-release', version]
    generator = ChangelogGenerator(options)
    generator.run()
    os.chdir(old_dir)","

def func1():
    var1 = os.getcwd()
    var2 = os.path.join(os.path.dirname(var3), os.pardir)
    os.chdir(var2)
    var4 = validate_version()
    if (not var4):
        os.chdir(var1)
        return
    print('Generating changelog for version {}'.format(var4))
    var7 = ['--user', 'arve0', '--project', 'leicacam', '-v', '--with-unreleased', '--future-release', var4]
    var8 = ChangelogGenerator(var7)
    generator.run()
    os.chdir(var1)
",Generate changelog .,Run the changelog generator .,Generate a changelog for the given version .,0,Tie,
"def write_polfils(str, str_I, **kwargs):
    ''''''

    lin,circ=fracpols(str, **kwargs)
    obs = Waterfall(str_I, max_load=150)

    obs.data = lin
    obs.write_to_fil(str[:-15]+'.linpol.fil')   #assuming file is named *.cross_pols.fil

    obs.data = circ
    obs.write_to_fil(str[:-15]+'.circpol.fil')","

def func1(arg1, arg2, **arg3):
    ''
    (var1, var2) = fracpols(arg1, **arg3)
    var4 = Waterfall(arg2, max_load=150)
    obs.data = var1
    obs.write_to_fil((arg1[:(- 15)] + '.linpol.fil'))
    obs.data = var2
    obs.write_to_fil((arg1[:(- 15)] + '.circpol.fil'))
",Writes two new filterbank files containing fractional linear and circular polarization data,func1 is a wrapper around fracpols,Write a set of fracpols to a file .,0,Prediction2,
"def get_placeholder(self, value=None, compiler=None, connection=None):
        
        return self.encrypt_sql.format(get_setting(connection, 'PUBLIC_PGP_KEY'))","

def func1(arg1, arg2=None, arg3=None, arg4=None):
    return self.encrypt_sql.format(get_setting(arg4, 'PUBLIC_PGP_KEY'))
",Tell postgres to encrypt this field using PGP .,encrypt_sql function .,Get the public GPG key .,0,Prediction1,
"def signal_kernel(self, signum):
        
        if self.has_kernel:
            self.kernel.send_signal(signum)
        else:
            raise RuntimeError(""Cannot signal kernel. No kernel is running!"")","

def func1(arg1, arg2):
    if self.has_kernel:
        self.kernel.send_signal(arg2)
    else:
        raise RuntimeError('Cannot signal kernel. No kernel is running!')
",Sends a signal to the kernel . Note that since only SIGTERM is supported on Windows this function is only useful on Unix systems .,Send a signal to the kernel,Signal kernel .,1,Tie,
"def f_translate_key(self, key):
        
        if isinstance(key, int):
            if key == 0:
                key = self.v_name
            else:
                key = self.v_name + '_%d' % key
        return key","

def func1(arg1, arg2):
    if isinstance(arg2, var2):
        if (arg2 == 0):
            arg2 = self.v_name
        else:
            arg2 = (self.v_name + ('_%d' % arg2))
    return arg2
",Translates integer indices into the appropriate names,Translate key to v_name .,Returns the name of the arg2 .,1,Prediction1,
"def config(self):
        
        if self._config is not None:
            return self._config
        try:
            return current_app.upload_set_config[self.name]
        except AttributeError:
            raise RuntimeError(""cannot access configuration outside request"")","

def func1(arg1):
    if (self._config is not None):
        return self._config
    try:
        return current_app.upload_set_config[self.name]
    except AttributeError:
        raise RuntimeError('cannot access configuration outside request')
",This gets the current configuration . By default it looks up the current application and gets the configuration from there . But if you don t want to go to the full effort of setting an application or it s otherwise outside of a request context set the _config attribute to an UploadConfiguration instance then set it back to None when you re done .,Return the current configuration for this upload set .,Get configuration of upload set .,1,Tie,
"def _init_params(self, amplitude, length_scale, validate_args):
    
    dtype = util.maybe_get_common_dtype(
        [amplitude, length_scale])
    if amplitude is not None:
      amplitude = tf.convert_to_tensor(
          value=amplitude, name='amplitude', dtype=dtype)
    self._amplitude = _validate_arg_if_not_none(
        amplitude, tf.compat.v1.assert_positive, validate_args)
    if length_scale is not None:
      length_scale = tf.convert_to_tensor(
          value=length_scale, name='length_scale', dtype=dtype)
    self._length_scale = _validate_arg_if_not_none(
        length_scale, tf.compat.v1.assert_positive, validate_args)
    return dtype","

def func1(arg1, arg2, arg3, arg4):
    var1 = util.maybe_get_common_dtype([arg2, arg3])
    if (arg2 is not None):
        arg2 = tf.convert_to_tensor(value=arg2, name='amplitude', dtype=var1)
    self._amplitude = _validate_arg_if_not_none(arg2, tf.compat.v1.assert_positive, arg4)
    if (arg3 is not None):
        arg3 = tf.convert_to_tensor(value=arg3, name='length_scale', dtype=var1)
    self._length_scale = _validate_arg_if_not_none(arg3, tf.compat.v1.assert_positive, arg4)
    return var1
",Shared init logic for amplitude and length_scale params .,Internal function for tf . get_variable_dtype .,Initialize parameters .,0,Prediction2,
"def applyLoggingOpts(log_levels, log_files):
    
    for l, lvl in log_levels:
        l.setLevel(lvl)
    for l, hdl in log_files:
        for h in l.handlers:
            l.removeHandler(h)
        l.addHandler(hdl)","

def func1(arg1, arg2):
    for (var1, var2) in arg1:
        l.setLevel(var2)
    for (var1, var3) in arg2:
        for var4 in l.handlers:
            l.removeHandler(var4)
        l.addHandler(var3)
",Apply logging options produced by LogLevelAction and LogFileAction .,Set levels and handlers .,Apply logging options .,0,Tie,
"def assoc(m, *kvs):
    
    if m is None:
        return lmap.Map.empty().assoc(*kvs)
    if isinstance(m, IAssociative):
        return m.assoc(*kvs)
    raise TypeError(
        f""Object of type {type(m)} does not implement Associative interface""
    )","

def func1(arg1, *arg2):
    if (arg1 is None):
        return lmap.Map.empty().assoc(*arg2)
    if isinstance(arg1, var2):
        return m.assoc(*arg2)
    raise TypeError(f'Object of type {type(arg1)} does not implement Associative interface')
",Associate keys to values in associative data structure m . If m is None returns a new Map with key - values kvs .,Return an Associative object .,Returns a new Map containing the values of the given keys .,0,Tie,
"def clear_task_instances(tis,
                         session,
                         activate_dag_runs=True,
                         dag=None,
                         ):
    
    job_ids = []
    for ti in tis:
        if ti.state == State.RUNNING:
            if ti.job_id:
                ti.state = State.SHUTDOWN
                job_ids.append(ti.job_id)
        else:
            task_id = ti.task_id
            if dag and dag.has_task(task_id):
                task = dag.get_task(task_id)
                task_retries = task.retries
                ti.max_tries = ti.try_number + task_retries - 1
            else:
                # Ignore errors when updating max_tries if dag is None or
                # task not found in dag since database records could be
                # outdated. We make max_tries the maximum value of its
                # original max_tries or the current task try number.
                ti.max_tries = max(ti.max_tries, ti.try_number - 1)
            ti.state = State.NONE
            session.merge(ti)

    if job_ids:
        from airflow.jobs import BaseJob as BJ
        for job in session.query(BJ).filter(BJ.id.in_(job_ids)).all():
            job.state = State.SHUTDOWN

    if activate_dag_runs and tis:
        from airflow.models.dagrun import DagRun  # Avoid circular import
        drs = session.query(DagRun).filter(
            DagRun.dag_id.in_({ti.dag_id for ti in tis}),
            DagRun.execution_date.in_({ti.execution_date for ti in tis}),
        ).all()
        for dr in drs:
            dr.state = State.RUNNING
            dr.start_date = timezone.utcnow()","

def func1(arg1, arg2, arg3=True, arg4=None):
    var1 = []
    for var2 in arg1:
        if (ti.state == State.RUNNING):
            if ti.job_id:
                ti.state = State.SHUTDOWN
                job_ids.append(ti.job_id)
        else:
            var3 = ti.task_id
            if (arg4 and dag.has_task(var3)):
                var4 = dag.get_task(var3)
                var5 = task.retries
                ti.max_tries = ((ti.try_number + var5) - 1)
            else:
                ti.max_tries = max(ti.max_tries, (ti.try_number - 1))
            ti.state = State.NONE
            session.merge(var2)
    if var1:
        from airflow.jobs import BaseJob as BJ
        for var7 in session.query(BJ).filter(BJ.id.in_(job_ids)).all():
            job.state = State.SHUTDOWN
    if (arg3 and arg1):
        from airflow.models.dagrun import DagRun
        var8 = session.query(DagRun).filter(DagRun.dag_id.in_({ti.dag_id for ti in tis}), DagRun.execution_date.in_({ti.execution_date for ti in tis})).all()
        for var9 in var8:
            dr.state = State.RUNNING
            dr.start_date = timezone.utcnow()
",Clears a set of task instances but makes sure the running ones get killed .,Clear all the task instances in tis .,Updates the state of one or more tasks .,1,Tie,
"def find_synonyms(self, word, count=20):
        
        j = h2o.api(""GET /3/Word2VecSynonyms"", data={'model': self.model_id, 'word': word, 'count': count})
        return OrderedDict(sorted(zip(j['synonyms'], j['scores']), key=lambda t: t[1], reverse=True))","

def func1(arg1, arg2, arg3=20):
    var1 = h2o.api('GET /3/Word2VecSynonyms', data={'model': self.model_id, 'word': arg2, 'count': arg3})
    return OrderedDict(sorted(zip(var1['synonyms'], var1['scores']), key=(lambda arg4: arg4[1]), reverse=True))
",Find synonyms using a word2vec model .,Find synonyms for a word .,Get Word2Vec Synonyms,1,Tie,
"def season(self):
        
        date = self.date()
        return date.year - 1 if date.month <= 3 else date.year","

def func1(arg1):
    var1 = self.date()
    return ((date.year - 1) if (date.month <= 3) else date.year)
",Returns the year ID of the season in which this game took place . Useful for week 17 January games .,Return the year of the first day of the year,Returns the season of the year .,0,Tie,both bad
"def _exception_pprint(obj, p, cycle):
    
    if obj.__class__.__module__ in ('exceptions', 'builtins'):
        name = obj.__class__.__name__
    else:
        name = '%s.%s' % (
            obj.__class__.__module__,
            obj.__class__.__name__
        )
    step = len(name) + 1
    p.begin_group(step, name + '(')
    for idx, arg in enumerate(getattr(obj, 'args', ())):
        if idx:
            p.text(',')
            p.breakable()
        p.pretty(arg)
    p.end_group(step, ')')","

def func1(arg1, arg2, arg3):
    if (obj.__class__.__module__ in ('exceptions', 'builtins')):
        var1 = obj.__class__.__name__
    else:
        var1 = ('%s.%s' % (obj.__class__.__module__, obj.__class__.__name__))
    var2 = (len(var1) + 1)
    p.begin_group(var2, (var1 + '('))
    for (var4, var5) in enumerate(getattr(arg1, 'args', ())):
        if var4:
            p.text(',')
            p.breakable()
        p.pretty(var5)
    p.end_group(var2, ')')
",Base pprint for all exceptions .,Pretty print an exception .,Prints a function call .,1,Prediction1,
"def setup_platform(hass, config, add_entities, discovery_info=None):
    
    host = config.get(CONF_HOST)
    token = config.get(CONF_ACCESS_TOKEN)
    name = config.get(CONF_NAME)
    volume_step = config.get(CONF_VOLUME_STEP)
    device_type = config.get(CONF_DEVICE_CLASS)
    device = VizioDevice(host, token, name, volume_step, device_type)
    if device.validate_setup() is False:
        _LOGGER.error(""Failed to set up Vizio platform, ""
                      ""please check if host and API key are correct"")
        return
    elif (token is None or token == """") and device_type == ""tv"":
        _LOGGER.error(""Failed to set up Vizio platform, ""
                      ""if device_class is 'tv' then an auth_token needs ""
                      ""to be provided, otherwise if device_class is ""
                      ""'soundbar' then add the right device_class to config"")
        return

    if config.get(CONF_SUPPRESS_WARNING):
        from requests.packages import urllib3
        _LOGGER.warning(""InsecureRequestWarning is disabled ""
                        ""because of Vizio platform configuration"")
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    add_entities([device], True)","

def func1(arg1, arg2, arg3, arg4=None):
    var1 = config.get(var2)
    var3 = config.get(var4)
    var5 = config.get(var6)
    var7 = config.get(var8)
    var9 = config.get(var10)
    var11 = VizioDevice(var1, var3, var5, var7, var9)
    if (device.validate_setup() is False):
        _LOGGER.error('Failed to set up Vizio platform, please check if host and API key are correct')
        return
    elif (((var3 is None) or (var3 == '')) and (var9 == 'tv')):
        _LOGGER.error(""Failed to set up Vizio platform, if device_class is 'tv' then an auth_token needs to be provided, otherwise if device_class is 'soundbar' then add the right device_class to config"")
        return
    if config.get(var13):
        from requests.packages import urllib3
        _LOGGER.warning('InsecureRequestWarning is disabled because of Vizio platform configuration')
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    add_entities([var11], True)
",Set up the Vizio media player platform .,Setup Vizio platform .,Set up Vizio device .,1,Tie,
"def validate_whatsapp(attr, value):
    
    if attr in (""from_"", ""to""):
        if value is not None and ""whatsapp:"" in value:
            value = value.split(""whatsapp:+"")[-1]
        check_valid(
            ""WhatsApp"",
            attr,
            value,
            validus.isint,
            ""phone number starting with the '+' symbol"",
        )
    elif attr in (""attachments""):
        check_valid(""WhatsApp"", attr, value, validus.isurl, ""url"")","

def func1(arg1, arg2):
    if (arg1 in ('from_', 'to')):
        if ((arg2 is not None) and ('whatsapp:' in arg2)):
            arg2 = value.split('whatsapp:+')[(- 1)]
        check_valid('WhatsApp', arg1, arg2, validus.isint, ""phone number starting with the '+' symbol"")
    elif (arg1 in 'attachments'):
        check_valid('WhatsApp', arg1, arg2, validus.isurl, 'url')
",WhatsApp input validator function .,Check that a WhatsApp attribute is valid .,Check if the argument is a valid phone number .,1,Prediction1,
"def collection_to_df(collection):
    ''' 
     

    '''

    return pd.concat([record.series for record in collection], axis=1).T","

def func1(arg1):
    ' \n     \n\n    '
    return pd.concat([record.series for record in collection], axis=1).T
",Converts a collection back into a pandas DataFrame,\ n \ n \ n,Convert a collection of records into a pandas DataFrame .,0,Prediction2,
"def create_underline(self, tag):
        
        style = tag.get('style')
        if style and 'text-decoration:underline' in style:
            tag.wrap(self.soup.new_tag('u'))","

def func1(arg1, arg2):
    var1 = tag.get('style')
    if (var1 and ('text-decoration:underline' in var1)):
        tag.wrap(self.soup.new_tag('u'))
",See if span tag has underline style and wrap with u tag .,Wrap underline if text - decoration is present .,Create underline if text - decoration - underline is present in style .,0,Tie,
"def verify_signature(self, value, sig):
        
        key = self.derive_key()
        try:
            sig = base64_decode(sig)
        except Exception:
            return False
        return self.algorithm.verify_signature(key, value, sig)","

def func1(arg1, arg2, arg3):
    var1 = self.derive_key()
    try:
        arg3 = base64_decode(arg3)
    except Exception:
        return False
    return self.algorithm.verify_signature(var1, arg2, arg3)
",Verifies the signature for the given value .,Return True if the signature is valid False otherwise .,Verify that a value is signed by a signature .,0,Tie,
"def combineAB(self):
        
        v4definition_meter = V4Meter()
        v4definition_meter.makeAB()
        defv4 = v4definition_meter.getReadBuffer()

        v3definition_meter = V3Meter()
        v3definition_meter.makeReturnFormat()
        defv3 = v3definition_meter.getReadBuffer()

        for fld in defv3:
            if fld not in self.m_all_fields:
                compare_fld = fld.upper()
                if not ""RESERVED"" in compare_fld and not ""CRC"" in compare_fld:
                    self.m_all_fields[fld] = defv3[fld]

        for fld in defv4:
            if fld not in self.m_all_fields:
                compare_fld = fld.upper()
                if not ""RESERVED"" in compare_fld and not ""CRC"" in compare_fld:
                    self.m_all_fields[fld] = defv4[fld]
        pass","

def func1(arg1):
    var1 = V4Meter()
    v4definition_meter.makeAB()
    var3 = v4definition_meter.getReadBuffer()
    var4 = V3Meter()
    v3definition_meter.makeReturnFormat()
    var6 = v3definition_meter.getReadBuffer()
    for var7 in var6:
        if (var7 not in self.m_all_fields):
            var8 = fld.upper()
            if ((not ('RESERVED' in var8)) and (not ('CRC' in var8))):
                self.m_all_fields[var7] = var6[var7]
    for var7 in var3:
        if (var7 not in self.m_all_fields):
            var8 = fld.upper()
            if ((not ('RESERVED' in var8)) and (not ('CRC' in var8))):
                self.m_all_fields[var7] = var3[var7]
    pass
",Use the serial block definitions in V3 and V4 to create one field list .,V4Meter v4definition_meter . makeAB and V3Meter . makeReturnFormat,Combine all non - reserved and non - CRC fields .,0,Tie,equally bad
"def get_content(url, headers={}, decoded=True):
    

    logging.debug('get_content: %s' % url)

    req = request.Request(url, headers=headers)
    if cookies:
        cookies.add_cookie_header(req)
        req.headers.update(req.unredirected_hdrs)

    response = urlopen_with_retry(req)
    data = response.read()

    # Handle HTTP compression for gzip and deflate (zlib)
    content_encoding = response.getheader('Content-Encoding')
    if content_encoding == 'gzip':
        data = ungzip(data)
    elif content_encoding == 'deflate':
        data = undeflate(data)

    # Decode the response body
    if decoded:
        charset = match1(
            response.getheader('Content-Type', ''), r'charset=([\w-]+)'
        )
        if charset is not None:
            data = data.decode(charset, 'ignore')
        else:
            data = data.decode('utf-8', 'ignore')

    return data","

def func1(arg1, arg2={}, arg3=True):
    logging.debug(('get_content: %s' % arg1))
    var1 = request.Request(arg1, headers=arg2)
    if var2:
        cookies.add_cookie_header(var1)
        req.headers.update(req.unredirected_hdrs)
    var3 = urlopen_with_retry(var1)
    var5 = response.read()
    var6 = response.getheader('Content-Encoding')
    if (var6 == 'gzip'):
        var5 = ungzip(var5)
    elif (var6 == 'deflate'):
        var5 = undeflate(var5)
    if arg3:
        var9 = match1(response.getheader('Content-Type', ''), 'charset=([\\w-]+)')
        if (var9 is not None):
            var5 = data.decode(var9, 'ignore')
        else:
            var5 = data.decode('utf-8', 'ignore')
    return var5
",Gets the content of a URL via sending a HTTP GET request .,wrapper for get_content,Get the content of a URL .,0,Prediction2,
"def grab_filt(self, filt, analyte=None):
        
        if isinstance(filt, str):
            if filt in self.components:
                if analyte is None:
                    return self.components[filt]
                else:
                    if self.switches[analyte][filt]:
                        return self.components[filt]
            else:
                try:
                    ind = self.make_fromkey(filt)
                except KeyError:
                    print((""\n\n***Filter key invalid. Please consult ""
                           ""manual and try again.""))
        elif isinstance(filt, dict):
            try:
                ind = self.make_fromkey(filt[analyte])
            except ValueError:
                print((""\n\n***Filter key invalid. Please consult manual ""
                       ""and try again.\nOR\nAnalyte missing from filter ""
                       ""key dict.""))
        elif filt:
            ind = self.make(analyte)
        else:
            ind = ~np.zeros(self.size, dtype=bool)
        return ind","

def func1(arg1, arg2, arg3=None):
    if isinstance(arg2, var2):
        if (arg2 in self.components):
            if (arg3 is None):
                return self.components[arg2]
            elif self.switches[arg3][arg2]:
                return self.components[arg2]
        else:
            try:
                var3 = self.make_fromkey(arg2)
            except KeyError:
                print('\n\n***Filter key invalid. Please consult manual and try again.')
    elif isinstance(arg2, var6):
        try:
            var3 = self.make_fromkey(arg2[arg3])
        except ValueError:
            print('\n\n***Filter key invalid. Please consult manual and try again.\nOR\nAnalyte missing from filter key dict.')
    elif arg2:
        var3 = self.make(arg3)
    else:
        var3 = (~ np.zeros(self.size, dtype=var8))
    return var3
",Flexible access to specific filter using any key format .,Grab a set of components that match a given filter .,Make a variable from arg1 arg2 and arg3 .,1,Prediction1,
"def to_dict(self):
        
        return {
            'content': self.content,
            'resources': [r._asdict() for r in self.resources],  # pylint: disable=W0212
            'js_init_fn': self.js_init_fn,
            'js_init_version': self.js_init_version,
            'json_init_args': self.json_init_args
        }","

def func1(arg1):
    return {'content': self.content, 'resources': [r._asdict() for var1 in self.resources], 'js_init_fn': self.js_init_fn, 'js_init_version': self.js_init_version, 'json_init_args': self.json_init_args}
",Returns the fragment in a dictionary representation .,Return a dict representation of this object .,Return the object as a dictionary .,0,Tie,
"def add_scheme(self,new_scheme):
        
        if not isinstance(new_scheme,ColorScheme):
            raise ValueError,'ColorSchemeTable only accepts ColorScheme instances'
        self[new_scheme.name] = new_scheme","def add_scheme(self,new_scheme):
        
        if not isinstance(new_scheme,ColorScheme):
            raise ValueError,'ColorSchemeTable only accepts ColorScheme instances'
        self[new_scheme.name] = new_scheme",Add a new color scheme to the table .,Adds a new color scheme to the table .,Adds a new scheme to the table .,0,Prediction1,
"def add(self, func, priority=0):
        
        self.chain.append((priority, func))
        self.chain.sort(key=lambda x: x[0])","

def func1(arg1, arg2, arg3=0):
    self.chain.append((arg3, arg2))
    self.chain.sort(key=(lambda arg4: arg4[0]))
",Add a func to the cmd chain with given priority,Add a function to the chain .,Add a function to the chain .,0,Tie,
"def demo(host, port):
    
    # logging.basicConfig(level=logging.DEBUG)

    loop = asyncio.get_event_loop()
    stl = AsyncSatel(host,
                     port,
                     loop,
                     [1, 2, 3, 4, 5, 6, 7, 8, 12, 13, 14, 15, 16, 17, 18, 19,
                      20, 21, 22, 23, 25, 26, 27, 28, 29, 30],
                     [8, 9, 10]
                     )

    loop.run_until_complete(stl.connect())
    loop.create_task(stl.arm(""3333"", 1))
    loop.create_task(stl.disarm(""3333""))
    loop.create_task(stl.keep_alive())
    loop.create_task(stl.monitor_status())

    loop.run_forever()
    loop.close()","

def func1(arg1, arg2):
    var1 = asyncio.get_event_loop()
    var2 = AsyncSatel(arg1, arg2, var1, [1, 2, 3, 4, 5, 6, 7, 8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30], [8, 9, 10])
    loop.run_until_complete(stl.connect())
    loop.create_task(stl.arm('3333', 1))
    loop.create_task(stl.disarm('3333'))
    loop.create_task(stl.keep_alive())
    loop.create_task(stl.monitor_status())
    loop.run_forever()
    loop.close()
",Basic demo of the monitoring capabilities .,Example demo .,Same as func2 .,1,Prediction1,
"def check_success(self):
        
        small = xrange(3)
        for i in xrange(self.iterations):
            key = PKey()
            key.generate_key(TYPE_DSA, 256)
            for i in small:
                cert = X509()
                cert.set_pubkey(key)
                for i in small:
                    cert.get_pubkey()","

def func1(arg1):
    var1 = xrange(3)
    for var3 in xrange(self.iterations):
        var4 = PKey()
        key.generate_key(var6, 256)
        for var3 in var1:
            var7 = X509()
            cert.set_pubkey(var4)
            for var3 in var1:
                cert.get_pubkey()
",Call the method repeatedly such that it will return a PKey object .,This function generates a key and a public key for each iteration .,Check the success of the certificate generation .,0,Prediction1,
"def to_python(self, omobj):
        
        # general overrides
        if omobj.__class__ in self._omclass_to_py:
            return self._omclass_to_py[omobj.__class__](omobj)
        # oms
        elif isinstance(omobj, om.OMSymbol):
            return self._lookup_to_python(omobj.cdbase, omobj.cd, omobj.name)
        # oma
        elif isinstance(omobj, om.OMApplication):
            elem = self.to_python(omobj.elem)
            arguments = [self.to_python(x) for x in omobj.arguments]
            return elem(*arguments)
        raise ValueError('Cannot convert object of class %s to Python.' % omobj.__class__.__name__)","

def func1(arg1, arg2):
    if (omobj.__class__ in self._omclass_to_py):
        return self._omclass_to_py[omobj.__class__](arg2)
    elif isinstance(arg2, om.OMSymbol):
        return self._lookup_to_python(omobj.cdbase, omobj.cd, omobj.name)
    elif isinstance(arg2, om.OMApplication):
        var2 = self.to_python(omobj.elem)
        var3 = [self.to_python(var4) for var4 in omobj.arguments]
        return elem(*var3)
    raise ValueError(('Cannot convert object of class %s to Python.' % omobj.__class__.__name__))
",Convert OpenMath object to Python,Convert an object to a Python object .,Convert an OMSymbol or OMSApplication to Python object .,0,Tie,
"def ping(self, timeout=12):
        
        self.conn(""POST"", ""{0}/users/ME/endpoints/{1}/active"".format(self.conn.msgsHost, self.id),
                  auth=SkypeConnection.Auth.RegToken, json={""timeout"": timeout})","

def func1(arg1, arg2=12):
    self.conn('POST', '{0}/users/ME/endpoints/{1}/active'.format(self.conn.msgsHost, self.id), auth=SkypeConnection.Auth.RegToken, json={'timeout': arg2})
",Send a keep - alive request for the endpoint .,Ping this endpoint .,func1 - Send active message to Skype .,1,Prediction1,
"def _send(self, data, content_type):
        

        headers = {
            ""Content-Type"": content_type,
            ""Authorization"": ""key=%s"" % (self.api_key),
            ""Content-Length"": str(len(data))
        }

        request = Request(self.api_url, data, headers)
        return urlopen(request).read().decode(self.encoding)","

def func1(arg1, arg2, arg3):
    var1 = {'Content-Type': arg3, 'Authorization': ('key=%s' % self.api_key), 'Content-Length': str(len(arg2))}
    var4 = Request(self.api_url, arg2, var1)
    return urlopen(request).read().decode(self.encoding)
",Sends a GCM message with the given content type,Send a request to the API .,Method to make a request to the API .,1,Tie,
"def _all_arcs(self):
        
        arcs = set()
        for bp in self.child_parsers():
            arcs.update(bp._arcs())

        return arcs","

def func1(arg1):
    var1 = set()
    for var3 in self.child_parsers():
        arcs.update(bp._arcs())
    return var1
",Get the set of all arcs in this code object and its children .,Return a set of all arcs in this parser .,A function that returns a set of all variables that can be parsed .,1,Tie,
"def build_fake_data():
  
  num_examples = 10
  x_train = np.random.rand(num_examples, *IMAGE_SHAPE).astype(np.float32)
  y_train = np.random.permutation(np.arange(num_examples)).astype(np.int32)
  x_test = np.random.rand(num_examples, *IMAGE_SHAPE).astype(np.float32)
  y_test = np.random.permutation(np.arange(num_examples)).astype(np.int32)
  return (x_train, y_train), (x_test, y_test)","

def func1():
    var1 = 10
    var2 = np.random.rand(num_examples, *IMAGE_SHAPE).astype(np.float32)
    var3 = np.random.permutation(np.arange(num_examples)).astype(np.int32)
    var4 = np.random.rand(num_examples, *IMAGE_SHAPE).astype(np.float32)
    var5 = np.random.permutation(np.arange(num_examples)).astype(np.int32)
    return ((var2, var3), (var4, var5))
",Build fake CIFAR10 - style data for unit testing .,Build train and test data .,Function to generate random variables .,1,Tie,
"def create_command(text, commands):
    

    class CustomCommand(BaseCommand):
        description = text

        def run(self):
            for cmd in commands:
                subprocess.check_call(cmd)

    return CustomCommand","

def func1(arg1, arg2):

    class CustomCommand(BaseCommand):
        description = text

        def run(self):
            for cmd in commands:
                subprocess.check_call(cmd)
    return var1
",Creates a custom setup . py command .,Create a custom command .,Create a custom command class that can be used to run commands .,1,Prediction2,
"def diff(section):
    
    try:
        settings = config.Settings(section=section)
        storage = STORAGES['s3'](settings=settings)
        conf = s3conf.S3Conf(storage=storage, settings=settings)
        local_root = os.path.join(config.LOCAL_CONFIG_FOLDER, section)
        click.echo(''.join(conf.diff(local_root)))
    except exceptions.EnvfilePathNotDefinedError:
        raise exceptions.EnvfilePathNotDefinedUsageError()","

def func1(arg1):
    try:
        var1 = config.Settings(section=arg1)
        var2 = var3['s3'](settings=var1)
        var4 = s3conf.S3Conf(storage=var2, settings=var1)
        var5 = os.path.join(config.LOCAL_CONFIG_FOLDER, arg1)
        click.echo(''.join(conf.diff(var5)))
    except exceptions.EnvfilePathNotDefinedError:
        raise exceptions.EnvfilePathNotDefinedUsageError()
",For each section defined in the local config file look up for a folder inside the local config folder named after the section . Uploads the environemnt file named as in the S3CONF variable for this section to the remote S3CONF path .,Check if arg1 is a valid config file .,Show the difference between the current configuration and the given section .,0,Prediction2,equally bad
"def display_messages(self, layout):
        
        print(json.dumps(self.messages, indent=4), file=self.out)","

def func1(arg1, arg2):
    print(json.dumps(self.messages, indent=4), file=self.out)
",Launch layouts display,Display the messages in JSON format .,Print the messages in JSON format .,1,Tie,
"def get_attrs(self, node):
        
        attrs = []
        properties = [
            (n, m)
            for n, m in node.items()
            if isinstance(m, astroid.FunctionDef) and decorated_with_property(m)
        ]
        for node_name, associated_nodes in (
            list(node.instance_attrs_type.items())
            + list(node.locals_type.items())
            + properties
        ):
            if not self.show_attr(node_name):
                continue
            names = self.class_names(associated_nodes)
            if names:
                node_name = ""%s : %s"" % (node_name, "", "".join(names))
            attrs.append(node_name)
        return sorted(attrs)","

def func1(arg1, arg2):
    var1 = []
    var2 = [(var3, var4) for (var3, var4) in node.items() if (isinstance(var4, astroid.FunctionDef) and decorated_with_property(var4))]
    for (var7, var8) in ((list(node.instance_attrs_type.items()) + list(node.locals_type.items())) + var2):
        if (not self.show_attr(var7)):
            continue
        var10 = self.class_names(var8)
        if var10:
            var7 = ('%s : %s' % (var7, ', '.join(var10)))
        attrs.append(var7)
    return sorted(var1)
",return visible attributes possibly with class name,Return a list of function names that are decorated with a property .,Get the names of the attributes of the given node .,0,Prediction2,
"def create(self, tarball_url, env=None, app_name=None):
        
        data = self._api.create_build(
            tarball_url=tarball_url,
            env=env,
            app_name=app_name,
        )

        return (data['id'], data['app']['name'])","

def func1(arg1, arg2, arg3=None, arg4=None):
    var1 = self._api.create_build(tarball_url=arg2, env=arg3, app_name=arg4)
    return (var1['id'], var1['app']['name'])
",Creates a Heroku app - setup build .,Function to create a build .,Create a build .,0,Tie,
"def imfrombytes(content, flag='color'):
    
    img_np = np.frombuffer(content, np.uint8)
    flag = imread_flags[flag] if is_str(flag) else flag
    img = cv2.imdecode(img_np, flag)
    return img","

def func1(arg1, arg2='color'):
    var1 = np.frombuffer(arg1, np.uint8)
    arg2 = (var3[arg2] if is_str(arg2) else arg2)
    var4 = cv2.imdecode(var1, arg2)
    return var4
",Read an image from bytes .,Read an image from bytes .,Wrapper for cv2 . imdecode,1,Prediction1,
"def _athlete_endpoint(self, athlete):
        
        return '{host}{athlete}'.format(
            host=self.host,
            athlete=quote_plus(athlete)
        )","

def func1(arg1, arg2):
    return '{host}{athlete}'.format(host=self.host, athlete=quote_plus(arg2))
",Construct athlete endpoint from host and athlete name,Return the endpoint for the given athlete .,Return a string with the host and the athlete .,1,Tie,
"def _read_coll(
    ctx: ReaderContext,
    f: Callable[[Collection[Any]], Union[llist.List, lset.Set, vector.Vector]],
    end_token: str,
    coll_name: str,
):
    
    coll: List = []
    reader = ctx.reader
    while True:
        token = reader.peek()
        if token == """":
            raise SyntaxError(f""Unexpected EOF in {coll_name}"")
        if whitespace_chars.match(token):
            reader.advance()
            continue
        if token == end_token:
            reader.next_token()
            return f(coll)
        elem = _read_next(ctx)
        if elem is COMMENT:
            continue
        coll.append(elem)","

def func1(arg1: ReaderContext, arg2: Callable[([Collection[Any]], Union[(llist.List, lset.Set, vector.Vector)])], arg3: str, arg4: str):
    var1: var2 = []
    var3 = ctx.reader
    while True:
        var4 = reader.peek()
        if (var4 == ''):
            raise SyntaxError(f'Unexpected EOF in {arg4}')
        if whitespace_chars.match(var4):
            reader.advance()
            continue
        if (var4 == arg3):
            reader.next_token()
            return f(var1)
        var6 = _read_next(arg1)
        if (var6 is var8):
            continue
        coll.append(var6)
",Read a collection from the input stream and create the collection using f .,Read a collection .,Read a collection of values .,1,Tie,
"def download(self, path=None, **kwargs):
        
        download_url = self.download_url(**kwargs)
        try:
            # For vault objects, use the object's filename
            # as the fallback if none is specified.
            filename = self.filename
        except AttributeError:
            # If the object has no filename attribute,
            # extract one from the download URL.
            filename = download_url.split('%3B%20filename%3D')[1]
            # Remove additional URL params from the name and ""unquote"" it.
            filename = unquote(filename.split('&')[0])

        if path:
            path = os.path.expanduser(path)
            # If the path is a dir, use the extracted filename
            if os.path.isdir(path):
                path = os.path.join(path, filename)
        else:
            # Create a temporary directory for the file
            path = os.path.join(tempfile.gettempdir(), filename)

        try:
            response = requests.request(method='get', url=download_url)
        except Exception as e:
            _handle_request_error(e)

        if not (200 <= response.status_code < 400):
            _handle_api_error(response)

        with open(path, 'wb') as fileobj:
            fileobj.write(response._content)

        return path","

def func1(arg1, arg2=None, **arg3):
    var1 = self.download_url(**arg3)
    try:
        var2 = self.filename
    except AttributeError:
        var2 = download_url.split('%3B%20filename%3D')[1]
        var2 = unquote(filename.split('&')[0])
    if arg2:
        arg2 = os.path.expanduser(arg2)
        if os.path.isdir(arg2):
            arg2 = os.path.join(arg2, var2)
    else:
        arg2 = os.path.join(tempfile.gettempdir(), var2)
    try:
        var5 = requests.request(method='get', url=var1)
    except Exception as e:
        _handle_request_error(var8)
    if (not (200 <= response.status_code < 400)):
        _handle_api_error(var5)
    with open(arg2, 'wb') as var11:
        fileobj.write(response._content)
    return arg2
",Download the file to the specified directory or file path . Downloads to a temporary directory if no path is specified .,download a file and save it to arg2,Download the file .,0,Prediction2,
"def translate_name(name):
    
    parts = name.split(""_"")
    i = 0
    while parts[i] == """":
        parts[i] = ""_""
        i += 1
    parts[i] = parts[i].lower()
    for j in range(i + 1, len(parts)):
        parts[j] = parts[j].capitalize()
    i = len(parts) - 1
    while parts[i] == """":
        parts[i] = ""_""
        i -= 1
    return """".join(parts)","

def func1(arg1):
    var1 = name.split('_')
    var2 = 0
    while (var1[var2] == ''):
        var1[var2] = '_'
        var2 += 1
    var1[var2] = parts[i].lower()
    for var3 in range((var2 + 1), len(var1)):
        var1[var3] = parts[j].capitalize()
    var2 = (len(var1) - 1)
    while (var1[var2] == ''):
        var1[var2] = '_'
        var2 -= 1
    return ''.join(var1)
",Convert names with underscores into camelcase .,func1 is a function that returns a string that can be used as a function name .,Translate a snake_case name to camel_case .,0,Prediction2,
"def from_kvs(keyvals):
        
        obj = H2OCluster()
        obj._retrieved_at = time.time()
        for k, v in keyvals:
            if k in {""__meta"", ""_exclude_fields"", ""__schema""}: continue
            if k in _cloud_v3_valid_keys:
                obj._props[k] = v
            else:
                raise AttributeError(""Attribute %s cannot be set on H2OCluster (= %r)"" % (k, v))
        return obj","

def func1(arg1):
    var1 = H2OCluster()
    obj._retrieved_at = time.time()
    for (var3, var4) in arg1:
        if (var3 in {'__meta', '_exclude_fields', '__schema'}):
            continue
        if (var3 in var5):
            obj._props[var3] = var4
        else:
            raise AttributeError(('Attribute %s cannot be set on H2OCluster (= %r)' % (var3, var4)))
    return var1
",Create H2OCluster object from a list of key - value pairs .,Create a H2OCluster object from a list of key - value pairs .,H2OCluster constructor .,1,Prediction1,
"def eligible_node(self, id, eligible=None, ineligible=None):
        
        payload = {}

        if eligible is not None and ineligible is not None:
            raise nomad.api.exceptions.InvalidParameters
        if eligible is None and ineligible is None:
            raise nomad.api.exceptions.InvalidParameters

        if eligible is not None and eligible:
            payload = {""Eligibility"": ""eligible"", ""NodeID"": id}
        elif eligible is not None and not eligible:
            payload = {""Eligibility"": ""ineligible"", ""NodeID"": id}
        elif ineligible is not None:
            payload = {""Eligibility"": ""ineligible"", ""NodeID"": id}
        elif ineligible is not None and not ineligible:
            payload = {""Eligibility"": ""eligible"", ""NodeID"": id}

        return self.request(id, ""eligibility"", json=payload, method=""post"").json()","

def func1(arg1, arg2, arg3=None, arg4=None):
    var1 = {}
    if ((arg3 is not None) and (arg4 is not None)):
        raise nomad.api.exceptions.InvalidParameters
    if ((arg3 is None) and (arg4 is None)):
        raise nomad.api.exceptions.InvalidParameters
    if ((arg3 is not None) and arg3):
        var1 = {'Eligibility': 'eligible', 'NodeID': arg2}
    elif ((arg3 is not None) and (not arg3)):
        var1 = {'Eligibility': 'ineligible', 'NodeID': arg2}
    elif (arg4 is not None):
        var1 = {'Eligibility': 'ineligible', 'NodeID': arg2}
    elif ((arg4 is not None) and (not arg4)):
        var1 = {'Eligibility': 'eligible', 'NodeID': arg2}
    return self.request(id, 'eligibility', json=payload, method='post').json()
",Toggle the eligibility of the node .,Check if a node is eligible .,This function is used to send an eligibility request .,1,Tie,
"def log_connection_info(self):
        
        basename = os.path.basename(self.connection_file)
        if basename == self.connection_file or \
            os.path.dirname(self.connection_file) == self.profile_dir.security_dir:
            # use shortname
            tail = basename
            if self.profile != 'default':
                tail += "" --profile %s"" % self.profile
        else:
            tail = self.connection_file
        self.log.critical(""--existing %s"", tail)


        self.ports = dict(shell=self.shell_port, iopub=self.iopub_port,
                                stdin=self.stdin_port, hb=self.hb_port)","

def func1(arg1):
    var1 = os.path.basename(self.connection_file)
    if ((var1 == self.connection_file) or (os.path.dirname(self.connection_file) == self.profile_dir.security_dir)):
        var2 = var1
        if (self.profile != 'default'):
            var2 += (' --profile %s' % self.profile)
    else:
        var2 = self.connection_file
    self.log.critical('--existing %s', var2)
    self.ports = dict(shell=self.shell_port, iopub=self.iopub_port, stdin=self.stdin_port, hb=self.hb_port)
",display connection info and store ports,Set self . ports based on arg1 .,Log information about the connection .,0,Prediction2,
"def f_remove(self, recursive=True, predicate=None):
        
        parent = self.f_get_parent()
        parent.f_remove_child(self.v_name, recursive=recursive, predicate=predicate)","

def func1(arg1, arg2=True, arg3=None):
    var1 = self.f_get_parent()
    parent.f_remove_child(self.v_name, recursive=arg2, predicate=arg3)
",Recursively removes the group and all it s children .,Remove this node from its parent .,Remove this variable from its parent .,1,Tie,
"def parse_description():
    
    from os.path import dirname, join, exists
    readme_fpath = join(dirname(__file__), 'README.md')
    # print('readme_fpath = %r' % (readme_fpath,))
    # This breaks on pip install, so check that it exists.
    if exists(readme_fpath):
        # try:
        #     # convert markdown to rst for pypi
        #     import pypandoc
        #     return pypandoc.convert(readme_fpath, 'rst')
        # except Exception as ex:
            # strip out markdown to make a clean readme for pypi
            textlines = []
            with open(readme_fpath, 'r') as f:
                capture = False
                for line in f.readlines():
                    if '# Purpose' in line:
                        capture = True
                    elif line.startswith('##'):
                        break
                    elif capture:
                        textlines += [line]
            text = ''.join(textlines).strip()
            text = text.replace('\n\n', '_NLHACK_')
            text = text.replace('\n', ' ')
            text = text.replace('_NLHACK_', '\n\n')
            return text
    return ''","

def func1():
    from os.path import dirname, join, exists
    var1 = join(dirname(var4), 'README.md')
    if exists(var1):
        var6 = []
        with open(var1, 'r') as var8:
            var9 = False
            for var10 in f.readlines():
                if ('# Purpose' in var10):
                    var9 = True
                elif line.startswith('##'):
                    break
                elif var9:
                    var6 += [var10]
        var11 = ''.join(textlines).strip()
        var11 = text.replace('\n\n', '_NLHACK_')
        var11 = text.replace('\n', ' ')
        var11 = text.replace('_NLHACK_', '\n\n')
        return var11
    return ''
",Parse the description in the README file,Read README . md and return it .,Parse the README . md file .,0,Tie,
"def list_rules(self, topic_name, subscription_name):
        '''
        
        '''
        _validate_not_none('topic_name', topic_name)
        _validate_not_none('subscription_name', subscription_name)
        request = HTTPRequest()
        request.method = 'GET'
        request.host = self._get_host()
        request.path = '/' + \
            _str(topic_name) + '/subscriptions/' + \
            _str(subscription_name) + '/rules/'
        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access
        request.headers = self._update_service_bus_header(request)
        response = self._perform_request(request)

        return _ETreeXmlToObject.convert_response_to_feeds(
            response, _convert_etree_element_to_rule)","

def func1(arg1, arg2, arg3):
    '\n        \n        '
    _validate_not_none('topic_name', arg2)
    _validate_not_none('subscription_name', arg3)
    var2 = HTTPRequest()
    request.method = 'GET'
    request.host = self._get_host()
    request.path = (((('/' + _str(arg2)) + '/subscriptions/') + _str(arg3)) + '/rules/')
    (request.path, request.query) = self._httpclient._update_request_uri_query(var2)
    request.headers = self._update_service_bus_header(var2)
    var5 = self._perform_request(var2)
    return _ETreeXmlToObject.convert_response_to_feeds(var5, var6)
",Retrieves the rules that exist under the specified subscription .,Gets a list of rules for a subscription .,\ n \ n,1,Prediction1,
"def _dataframe_from_csv(reader, delimiter, with_header, skipspace):
    
    sep = delimiter
    header = 0
    if not with_header:
        header = None

    return pd.read_csv(
        reader,
        header=header,
        sep=sep,
        skipinitialspace=skipspace,
        encoding='utf-8-sig'
    )","

def func1(arg1, arg2, arg3, arg4):
    var1 = arg2
    var2 = 0
    if (not arg3):
        var2 = None
    return pd.read_csv(arg1, header=var2, sep=var1, skipinitialspace=arg4, encoding='utf-8-sig')
",Returns csv data as a pandas Dataframe object,Read data from arg1 .,Create a pandas dataframe from a CSV file .,0,Prediction2,
"def __complete_alias(
        self, prefix: str, name_in_ns: Optional[str] = None
    ) -> Iterable[str]:
        
        candidates = filter(
            Namespace.__completion_matcher(prefix), [(s, n) for s, n in self.aliases]
        )
        if name_in_ns is not None:
            for _, candidate_ns in candidates:
                for match in candidate_ns.__complete_interns(
                    name_in_ns, include_private_vars=False
                ):
                    yield f""{prefix}/{match}""
        else:
            for alias, _ in candidates:
                yield f""{alias}/""","

def func1(arg1, arg2: str, arg3: Optional[str]=None) -> var9[var10]:
    var1 = filter(Namespace.__completion_matcher(arg2), [(var3, var4) for (var3, var4) in self.aliases])
    if (arg3 is not None):
        for (var5, var6) in var1:
            for var7 in candidate_ns.__complete_interns(arg3, include_private_vars=False):
                (yield f'{arg2}/{var7}')
    else:
        for (var8, var5) in var1:
            (yield f'{var8}/')
",Return an iterable of possible completions matching the given prefix from the list of aliased namespaces . If name_in_ns is given further attempt to refine the list to matching names in that namespace .,Yields all aliases that match prefix .,A function that returns a list of variables that match arg1 and arg2 .,1,Prediction1,"check, discuss"
"def validate(self):
        
        if self.value is not None:
            if type(self.value) not in six.integer_types:
                raise TypeError('expected (one of): {0}, observed: {1}'.format(
                    six.integer_types, type(self.value)))
            else:
                if self.value > Interval.MAX:
                    raise ValueError(
                        'interval value greater than accepted max')
                elif self.value < Interval.MIN:
                    raise ValueError('interval value less than accepted min')","

def func1(arg1):
    if (self.value is not None):
        if (type(self.value) not in six.integer_types):
            raise TypeError('expected (one of): {0}, observed: {1}'.format(six.integer_types, type(self.value)))
        elif (self.value > Interval.MAX):
            raise ValueError('interval value greater than accepted max')
        elif (self.value < Interval.MIN):
            raise ValueError('interval value less than accepted min')
",Verify that the value of the Interval is valid .,Check the validity of the interval .,Validate that the interval is valid .,0,Tie,
"def dispose(json_str):
    
    result_str = list(json_str)
    escaped = False
    normal = True
    sl_comment = False
    ml_comment = False
    quoted = False

    a_step_from_comment = False
    a_step_from_comment_away = False

    former_index = None

    for index, char in enumerate(json_str):

        if escaped:  # We have just met a '\'
            escaped = False
            continue

        if a_step_from_comment:  # We have just met a '/'
            if char != '/' and char != '*':
                a_step_from_comment = False
                normal = True
                continue

        if a_step_from_comment_away:  # We have just met a '*'
            if char != '/':
                a_step_from_comment_away = False

        if char == '""':
            if normal and not escaped:
                # We are now in a string
                quoted = True
                normal = False
            elif quoted and not escaped:
                # We are now out of a string
                quoted = False
                normal = True

        elif char == '\\':
            # '\' should not take effect in comment
            if normal or quoted:
                escaped = True

        elif char == '/':
            if a_step_from_comment:
                # Now we are in single line comment
                a_step_from_comment = False
                sl_comment = True
                normal = False
                former_index = index - 1
            elif a_step_from_comment_away:
                # Now we are out of comment
                a_step_from_comment_away = False
                normal = True
                ml_comment = False
                for i in range(former_index, index + 1):
                    result_str[i] = """"

            elif normal:
                # Now we are just one step away from comment
                a_step_from_comment = True
                normal = False

        elif char == '*':
            if a_step_from_comment:
                # We are now in multi-line comment
                a_step_from_comment = False
                ml_comment = True
                normal = False
                former_index = index - 1
            elif ml_comment:
                a_step_from_comment_away = True
        elif char == '\n':
            if sl_comment:
                sl_comment = False
                normal = True
                for i in range(former_index, index + 1):
                    result_str[i] = """"
        elif char == ']' or char == '}':
            if normal:
                _remove_last_comma(result_str, index)

    # Show respect to original input if we are in python2
    return ("""" if isinstance(json_str, str) else u"""").join(result_str)","

def func1(arg1):
    var1 = list(arg1)
    var3 = False
    var4 = True
    var5 = False
    var6 = False
    var7 = False
    var8 = False
    var9 = False
    var10 = None
    for (var11, var12) in enumerate(arg1):
        if var3:
            var3 = False
            continue
        if var8:
            if ((var12 != '/') and (var12 != '*')):
                var8 = False
                var4 = True
                continue
        if var9:
            if (var12 != '/'):
                var9 = False
        if (var12 == '""'):
            if (var4 and (not var3)):
                var7 = True
                var4 = False
            elif (var7 and (not var3)):
                var7 = False
                var4 = True
        elif (var12 == '\\'):
            if (var4 or var7):
                var3 = True
        elif (var12 == '/'):
            if var8:
                var8 = False
                var5 = True
                var4 = False
                var10 = (var11 - 1)
            elif var9:
                var9 = False
                var4 = True
                var6 = False
                for var14 in range(var10, (var11 + 1)):
                    var1[var14] = ''
            elif var4:
                var8 = True
                var4 = False
        elif (var12 == '*'):
            if var8:
                var8 = False
                var6 = True
                var4 = False
                var10 = (var11 - 1)
            elif var6:
                var9 = True
        elif (var12 == '\n'):
            if var5:
                var5 = False
                var4 = True
                for var14 in range(var10, (var11 + 1)):
                    var1[var14] = ''
        elif ((var12 == ']') or (var12 == '}')):
            if var4:
                _remove_last_comma(var1, var11)
    return ('' if isinstance(json_str, str) else u'').join(var1)
",Clear all comments in json_str .,Function 1 .,Dispose of a JSON string .,0,Prediction2,
"def get_private_dir(self, create=False):
        
        if self.is_local():
            name = os.path.abspath(self.path).replace(os.path.sep, ""_"")[:250]  # should not be too long for most os'es
            name = name.replace("":"", ""_"")  # for windows drive names
        else:
            server = self.server
            name = ""%s_%s_%s_%s"" % (server.hostname, server.port, server.base_path.replace(""/"", ""_""), self.name)
        dir = os.path.join(vaex.utils.get_private_dir(), ""dfs"", name)
        if create and not os.path.exists(dir):
            os.makedirs(dir)
        return dir","

def func1(arg1, arg2=False):
    if self.is_local():
        var1 = os.path.abspath(self.path).replace(os.path.sep, '_')[:250]
        var1 = name.replace(':', '_')
    else:
        var2 = self.server
        var1 = ('%s_%s_%s_%s' % (server.hostname, server.port, server.base_path.replace('/', '_'), self.name))
    var3 = os.path.join(vaex.utils.get_private_dir(), 'dfs', var1)
    if (arg2 and (not os.path.exists(var3))):
        os.makedirs(var3)
    return var3
",Each DataFrame has a directory where files are stored for metadata etc .,Returns the path to a local or remote file .,Get the private directory for this node .,0,Prediction2,
"def _collection_literal_to_py_ast(
    ctx: GeneratorContext, form: Iterable[LispForm]
) -> Iterable[GeneratedPyAST]:
    
    yield from map(partial(_const_val_to_py_ast, ctx), form)","

def func1(arg1: GeneratorContext, arg2: Iterable[LispForm]) -> var4[var5]:
    (yield from map(partial(var3, arg1), arg2))
",Turn a quoted collection literal of Lisp forms into Python AST nodes .,Apply a function to an iterable of LispForms .,Convert a list of LispForms into Python ASTs .,0,Prediction2,
"def has_waveform_packet(self):
        
        dimensions = set(self.dimension_names)
        return all(name in dimensions for name in dims.WAVEFORM_FIELDS_NAMES)","

def func1(arg1):
    var1 = set(self.dimension_names)
    return all(((var4 in var1) for var4 in dims.WAVEFORM_FIELDS_NAMES))
",Returns True if the point format has waveform packet dimensions,Return True if this dimension has a waveform packet .,Return True if all WAVEFORM_FIELDS_NAMES are in arg1 .,1,Prediction1,
"def has_changed(self):
        
        request = urllib_request.Request(self.url)
        request.get_method = lambda: 'HEAD'

        response = urllib_request.urlopen(request)
        information = response.info()

        if 'Last-Modified' in information:
            last_modified = information['Last-Modified']

            # Return False if the image has not been modified
            if last_modified == self.image_last_modified:
                return False

        self.image_last_modified = last_modified

        # Return True if the image has been modified
        # or if the image has no last-modified header
        return True","

def func1(arg1):
    var1 = urllib_request.Request(self.url)
    request.get_method = (lambda : 'HEAD')
    var2 = urllib_request.urlopen(var1)
    var3 = response.info()
    if ('Last-Modified' in var3):
        var4 = var3['Last-Modified']
        if (var4 == self.image_last_modified):
            return False
    self.image_last_modified = var4
    return True
",Method to check if an image has changed since it was last downloaded . By making a head request this check can be done quicker that downloading and processing the whole file .,returns True if image was modified False otherwise,Check if the image has changed .,0,Tie,
"def plot_mdr_grid(mdr_instance):
    
    var1_levels = list(set([variables[0] for variables in mdr_instance.feature_map]))
    var2_levels = list(set([variables[1] for variables in mdr_instance.feature_map]))
    max_count = np.array(list(mdr_instance.class_count_matrix.values())).flatten().max()

    """"""
    TODO:
        - Add common axis labels
        - Make sure this scales for smaller and larger record sizes
        - Extend to 3-way+ models, e.g., http://4.bp.blogspot.com/-vgKCjEkWFUc/UPwPuHo6XvI/AAAAAAAAAE0/fORHqDcoikE/s1600/model.jpg
    """"""

    fig, splots = plt.subplots(ncols=len(var1_levels), nrows=len(var2_levels), sharey=True, sharex=True)
    fig.set_figwidth(6)
    fig.set_figheight(6)

    for (var1, var2) in itertools.product(var1_levels, var2_levels):
        class_counts = mdr_instance.class_count_matrix[(var1, var2)]
        splot = splots[var2_levels.index(var2)][var1_levels.index(var1)]
        splot.set_yticks([])
        splot.set_xticks([])
        splot.set_ylim(0, max_count * 1.5)
        splot.set_xlim(-0.5, 1.5)

        if var2_levels.index(var2) == 0:
            splot.set_title('X1 = {}'.format(var1), fontsize=12)
        if var1_levels.index(var1) == 0:
            splot.set_ylabel('X2 = {}'.format(var2), fontsize=12)

        bars = splot.bar(left=range(class_counts.shape[0]),
                         height=class_counts, width=0.5,
                         color='black', align='center')

        bgcolor = 'lightgrey' if mdr_instance.feature_map[(var1, var2)] == 0 else 'darkgrey'
        splot.set_axis_bgcolor(bgcolor)
        for index, bar in enumerate(bars):
            splot.text(index, class_counts[index] + (max_count * 0.1), class_counts[index], ha='center')

    fig.tight_layout()
    return fig","

def func1(arg1):
    var1 = list(set([var4[0] for var4 in mdr_instance.feature_map]))
    var5 = list(set([var4[1] for var4 in mdr_instance.feature_map]))
    var6 = np.array(list(mdr_instance.class_count_matrix.values())).flatten().max()
    '\n    TODO:\n        - Add common axis labels\n        - Make sure this scales for smaller and larger record sizes\n        - Extend to 3-way+ models, e.g., http://4.bp.blogspot.com/-vgKCjEkWFUc/UPwPuHo6XvI/AAAAAAAAAE0/fORHqDcoikE/s1600/model.jpg\n    '
    (var7, var8) = plt.subplots(ncols=len(var1), nrows=len(var5), sharey=True, sharex=True)
    fig.set_figwidth(6)
    fig.set_figheight(6)
    for (var10, var11) in itertools.product(var1, var5):
        var12 = mdr_instance.class_count_matrix[(var10, var11)]
        var13 = var8[var2_levels.index(var11)][var1_levels.index(var10)]
        splot.set_yticks([])
        splot.set_xticks([])
        splot.set_ylim(0, (var6 * 1.5))
        splot.set_xlim((- 0.5), 1.5)
        if (var2_levels.index(var11) == 0):
            splot.set_title('X1 = {}'.format(var10), fontsize=12)
        if (var1_levels.index(var10) == 0):
            splot.set_ylabel('X2 = {}'.format(var11), fontsize=12)
        var14 = splot.bar(left=range(class_counts.shape[0]), height=var12, width=0.5, color='black', align='center')
        var16 = ('lightgrey' if (mdr_instance.feature_map[(var10, var11)] == 0) else 'darkgrey')
        splot.set_axis_bgcolor(var16)
        for (var17, var18) in enumerate(var14):
            splot.text(var17, (var12[var17] + (var6 * 0.1)), var12[var17], ha='center')
    fig.tight_layout()
    return var7
",Visualizes the MDR grid of a given fitted MDR instance . Only works for 2 - way MDR models . This function is currently incomplete .,This function is used to plot a 3 - way model .,Plots the MDR grid .,0,Prediction2,
"def search(self, origin, backend_name, category, archived_after):
        
        archives = self._search_archives(origin, backend_name,
                                         category, archived_after)
        archives = [(fp, date) for fp, date in archives]
        archives = [fp for fp, _ in sorted(archives, key=lambda x: x[1])]

        return archives","

def func1(arg1, arg2, arg3, arg4, arg5):
    var1 = self._search_archives(arg2, arg3, arg4, arg5)
    var1 = [(var2, var3) for (var2, var3) in var1]
    var1 = [var2 for (var2, var4) in sorted(var1, key=(lambda arg6: arg6[1]))]
    return var1
",Search archives .,Search for archived archives .,A function that returns a list of 2 - tuples where the first element is the name of the variable .,1,Prediction1,
"def get_organisation(self, id, name=None):
        '''
        
        '''
        return self.create_organisation(dict(id=id, name=name))","

def func1(arg1, arg2, arg3=None):
    '\n        \n        '
    return self.create_organisation(dict(id=arg2, name=arg3))
",Get an organisation,Get an organisation .,\ n \ n,1,Prediction1,
"def _interop_prop_to_py_ast(
    ctx: GeneratorContext, node: HostField, is_assigning: bool = False
) -> GeneratedPyAST:
    
    assert node.op == NodeOp.HOST_FIELD

    target_ast = gen_py_ast(ctx, node.target)

    return GeneratedPyAST(
        node=ast.Attribute(
            value=target_ast.node,
            attr=munge(node.field),
            ctx=ast.Store() if is_assigning else ast.Load(),
        ),
        dependencies=target_ast.dependencies,
    )","

def func1(arg1: GeneratorContext, arg2: HostField, arg3: bool=False) -> var3:
    assert (node.op == NodeOp.HOST_FIELD)
    var1 = gen_py_ast(arg1, node.target)
    return GeneratedPyAST(node=ast.Attribute(value=target_ast.node, attr=munge(node.field), ctx=(ast.Store() if arg3 else ast.Load())), dependencies=target_ast.dependencies)
",Generate a Python AST node for Python interop property access .,Convert a HostField node to an ast .,Generate a HostField node .,1,Prediction1,
"def add_cv_description(self, cv_id, lang_ref, description=None):
        
        if lang_ref not in self.languages:
            raise ValueError('Language not present: {}'.format(lang_ref))
        self.controlled_vocabularies[cv_id][0].append((lang_ref, description))","

def func1(arg1, arg2, arg3, arg4=None):
    if (arg3 not in self.languages):
        raise ValueError('Language not present: {}'.format(arg3))
    self.controlled_vocabularies[cv_id][0].append((arg3, arg4))
",Add a description to a controlled vocabulary .,Add a controlled vocabularies description .,Function to add controlled vocabularies .,1,Tie,
"def is_literal_or_name(value):
    
    try:
        ast.literal_eval(value)
        return True
    except (SyntaxError, ValueError):
        pass

    if value.strip() in ['dict()', 'list()', 'set()']:
        return True

    # Support removal of variables on the right side. But make sure
    # there are no dots, which could mean an access of a property.
    return re.match(r'^\w+\s*$', value)","

def func1(arg1):
    try:
        ast.literal_eval(arg1)
        return True
    except (var1, var2):
        pass
    if (value.strip() in ['dict()', 'list()', 'set()']):
        return True
    return re.match('^\\w+\\s*$', arg1)
",Return True if value is a literal or a name .,Return True if value is a literal or a name .,Return True if arg1 is a string .,1,Prediction1,
"def get_reconstructed_alignment(self):
        
        from Bio.Align import MultipleSeqAlignment
        from Bio.Seq import Seq
        from Bio.SeqRecord import SeqRecord
        self.logger(""TreeAnc.get_reconstructed_alignment ..."",2)
        if not hasattr(self.tree.root, 'sequence'):
            self.logger(""TreeAnc.reconstructed_alignment... reconstruction not yet done"",3)
            self.reconstruct_anc('probabilistic')

        new_aln = MultipleSeqAlignment([SeqRecord(id=n.name, seq=Seq("""".join(n.sequence)), description="""")
                                        for n in self.tree.find_clades()])

        return new_aln","

def func1(arg1):
    from Bio.Align import MultipleSeqAlignment
    from Bio.Seq import Seq
    from Bio.SeqRecord import SeqRecord
    self.logger('TreeAnc.get_reconstructed_alignment ...', 2)
    if (not hasattr(self.tree.root, 'sequence')):
        self.logger('TreeAnc.reconstructed_alignment... reconstruction not yet done', 3)
        self.reconstruct_anc('probabilistic')
    var2 = MultipleSeqAlignment([SeqRecord(id=n.name, seq=Seq(''.join(n.sequence)), description='') for var6 in self.tree.find_clades()])
    return var2
",Get the multiple sequence alignment including reconstructed sequences for the internal nodes .,get_reconstructed_alignment returns a new MultipleSeqAlignment object,Get the reconstructed alignment .,0,Tie,
"def _prepare_data_payload(data):
        
        if not data: return None
        res = {}
        for key, value in viewitems(data):
            if value is None: continue  # don't send args set to None so backend defaults take precedence
            if isinstance(value, list):
                value = stringify_list(value)
            elif isinstance(value, dict):
                if ""__meta"" in value and value[""__meta""][""schema_name""].endswith(""KeyV3""):
                    value = value[""name""]
                else:
                    value = stringify_dict(value)
            else:
                value = str(value)
            res[key] = value
        return res","

def func1(arg1):
    if (not arg1):
        return None
    var1 = {}
    for (var2, var3) in viewitems(arg1):
        if (var3 is None):
            continue
        if isinstance(var3, var6):
            var3 = stringify_list(var3)
        elif isinstance(var3, var8):
            if (('__meta' in var3) and value['__meta']['schema_name'].endswith('KeyV3')):
                var3 = var3['name']
            else:
                var3 = stringify_dict(var3)
        else:
            var3 = str(var3)
        var1[var2] = var3
    return var1
",Make a copy of the data object preparing it to be sent to the server .,Convert arg1 to a dict .,Prepare data payload .,0,Prediction2,equally bad
"def volume_percentage_used(self, volume):
        
        volume = self._get_volume(volume)
        if volume is not None:
            total = int(volume[""size""][""total""])
            used = int(volume[""size""][""used""])

            if used is not None and used > 0 and \
               total is not None and total > 0:
                return round((float(used) / float(total)) * 100.0, 1)","

def func1(arg1, arg2):
    arg2 = self._get_volume(arg2)
    if (arg2 is not None):
        var1 = int(arg2['size']['total'])
        var3 = int(arg2['size']['used'])
        if ((var3 is not None) and (var3 > 0) and (var1 is not None) and (var1 > 0)):
            return round(((float(var3) / float(var1)) * 100.0), 1)
",Total used size in percentage for volume,Return the percentage of total and used in arg1 to arg2,Get the percentage used for a volume .,0,Prediction2,
"def _copy_from(entries, remote_path, local_path, profile):
    
    commands = []
    paths = set()
    for entry in entries:
        hname = entry.hostname or entry.public_ip
        _local_path = entry.format_string(local_path)
        if _local_path in paths:
            raise ValueError('Duplicate local paths: one or more paths '
                             'had value {} after formatting.'
                             .format(local_path))
        paths.add(_local_path)
        # If the path references a folder, create the folder if it doesn't
        # exist.
        _folder = os.path.split(_local_path)[0]
        if len(_folder) > 0:
            if not os.path.exists(_folder):
                print('Creating directory ' + _folder)
                os.makedirs(_folder)
        cmd = _build_scp_command(hname, profile.username,
                                 profile.identity_file,
                                 is_get=True,
                                 local_path=_local_path,
                                 remote_path=remote_path)
        print('Command:', cmd)
        commands.append({
            'command': cmd,
            'description': entry.display()
        })
    stream_commands(commands)
    print(green('Finished copying'))","

def func1(arg1, arg2, arg3, arg4):
    var1 = []
    var2 = set()
    for var4 in arg1:
        var5 = (entry.hostname or entry.public_ip)
        var6 = entry.format_string(arg3)
        if (var6 in var2):
            raise ValueError('Duplicate local paths: one or more paths had value {} after formatting.'.format(arg3))
        paths.add(var6)
        var8 = os.path.split(var6)[0]
        if (len(var8) > 0):
            if (not os.path.exists(var8)):
                print(('Creating directory ' + var8))
                os.makedirs(var8)
        var11 = _build_scp_command(var5, profile.username, profile.identity_file, is_get=True, local_path=var6, remote_path=arg2)
        print('Command:', var11)
        commands.append({'command': var11, 'description': entry.display()})
    stream_commands(var1)
    print(green('Finished copying'))
",Performs an SCP command where the remote_path is the source and the local_path is a format string formatted individually for each host being copied from so as to create one or more distinct paths on the local system .,Copy files from a list of entries to a local path .,Copy a list of entries to a local file,1,Prediction1,
"def normalize_spl_by_average(self, db):
        
        arr = self.to_numpy_array().copy()
        if len(arr) == 0:
            raise ValueError(""Cannot normalize the SPL of an empty AudioSegment"")

        def rms(x):
            return np.sqrt(np.mean(np.square(x)))

        # Figure out what RMS we would like
        desired_rms = P_REF_PCM * ((10 ** (db/20.0)) - 1E-9)

        # Use successive approximation to solve
        ## Keep trying different multiplication factors until we get close enough or run out of time
        max_ntries = 50
        res_rms = 0.0
        ntries = 0
        factor = 0.1
        left = 0.0
        right = desired_rms
        while (ntries < max_ntries) and not util.isclose(res_rms, desired_rms, abs_tol=0.1):
            res_rms = rms(arr * factor)
            if res_rms < desired_rms:
                left = factor
            else:
                right = factor
            factor = 0.5 * (left + right)
            ntries += 1

        dtype_dict = {1: np.int8, 2: np.int16, 4: np.int32}
        dtype = dtype_dict[self.sample_width]
        new_seg = from_numpy_array(np.array(arr * factor, dtype=dtype), self.frame_rate)
        return new_seg","

def func2(arg1, arg2):
    var1 = self.to_numpy_array().copy()
    if (len(var1) == 0):
        raise ValueError('Cannot normalize the SPL of an empty AudioSegment')

    def func1(arg3):
        return np.sqrt(np.mean(np.square(arg3)))
    var4 = (var5 * ((10 ** (arg2 / 20.0)) - 1e-09))
    var6 = 50
    var7 = 0.0
    var8 = 0
    var9 = 0.1
    var10 = 0.0
    var11 = var4
    while ((var8 < var6) and (not util.isclose(var7, var4, abs_tol=0.1))):
        var7 = rms((var1 * var9))
        if (var7 < var4):
            var10 = var9
        else:
            var11 = var9
        var9 = (0.5 * (var10 + var11))
        var8 += 1
    var12 = {1: np.int8, 2: np.int16, 4: np.int32}
    var13 = var12[self.sample_width]
    var14 = from_numpy_array(np.array((var1 * var9), dtype=var13), self.frame_rate)
    return var14
",Normalize the values in the AudioSegment so that its spl property gives db .,Normalize the audio segment by averaging the average RMS .,Function to normalize the SPL .,1,Prediction1,
"def get_component(self, colour, tolerance=0, default=None):
        
        if not (0 <= tolerance <= np.sqrt(195075)):
            raise LegendError('Tolerance must be between 0 and 441.67')

        for decor in self.__list:
            if colour.lower() == decor.colour:
                return decor.component

        # If we're here, we didn't find one yet.
        r1, g1, b1 = utils.hex_to_rgb(colour)

        # Start with a best match of black.
        best_match = '#000000'
        best_match_dist = np.sqrt(r1**2. + g1**2. + b1**2.)

        # Now compare to each colour in the legend.
        for decor in self.__list:
            r2, g2, b2 = decor.rgb
            distance = np.sqrt((r2-r1)**2. + (g2-g1)**2. + (b2-b1)**2.)
            if distance < best_match_dist:
                best_match = decor.component
                best_match_dist = distance
                best_match_colour = decor.colour

        if best_match_dist <= tolerance:
            return best_match
        else:
            with warnings.catch_warnings():
                warnings.simplefilter(""always"")
                w = ""No match found for {0} "".format(colour.lower())
                w += ""with tolerance of {0}. Best match is "".format(tolerance)
                w += ""{0}, {1}"".format(best_match.summary(), best_match_colour)
                w += "", d={0}"".format(best_match_dist)
                warnings.warn(w)

            return default","

def func1(arg1, arg2, arg3=0, arg4=None):
    if (not (0 <= arg3 <= np.sqrt(195075))):
        raise LegendError('Tolerance must be between 0 and 441.67')
    for var2 in self.__list:
        if (colour.lower() == decor.colour):
            return decor.component
    (var3, var4, var5) = utils.hex_to_rgb(arg2)
    var6 = '#000000'
    var7 = np.sqrt((((var3 ** 2.0) + (var4 ** 2.0)) + (var5 ** 2.0)))
    for var2 in self.__list:
        (var8, var9, var10) = decor.rgb
        var11 = np.sqrt(((((var8 - var3) ** 2.0) + ((var9 - var4) ** 2.0)) + ((var10 - var5) ** 2.0)))
        if (var11 < var7):
            var6 = decor.component
            var7 = var11
            var12 = decor.colour
    if (var7 <= arg3):
        return var6
    else:
        with warnings.catch_warnings():
            warnings.simplefilter('always')
            var13 = 'No match found for {0} '.format(colour.lower())
            var13 += 'with tolerance of {0}. Best match is '.format(arg3)
            var13 += '{0}, {1}'.format(best_match.summary(), var12)
            var13 += ', d={0}'.format(var7)
            warnings.warn(var13)
        return arg4
",Get the component corresponding to a display colour . This is for generating a Striplog object from a colour image of a striplog .,Get the component of a given colour .,This function is used to determine the color of a color .,1,Prediction1,
"def has_no_title(self, title, **kwargs):
        

        try:
            self.assert_no_title(title, **kwargs)
            return True
        except ExpectationNotMet:
            return False","

def func1(arg1, arg2, **arg3):
    try:
        self.assert_no_title(arg2, **arg3)
        return True
    except ExpectationNotMet:
        return False
",Checks if the page doesn t have the given title .,Returns True if the page has no title False otherwise .,Return True if arg1 == arg2 and False otherwise .,1,Prediction1,
"def authenticate_client_id(self, client_id, request, *args, **kwargs):
        
        if client_id is None:
            client_id, _ = self._get_client_creds_from_request(request)

        log.debug('Authenticate client %r.', client_id)
        client = request.client or self._clientgetter(client_id)
        if not client:
            log.debug('Authenticate failed, client not found.')
            return False

        # attach client on request for convenience
        request.client = client
        return True","

def func1(arg1, arg2, arg3, *arg4, **arg5):
    if (arg2 is None):
        (arg2, var1) = self._get_client_creds_from_request(arg3)
    log.debug('Authenticate client %r.', arg2)
    var2 = (request.client or self._clientgetter(arg2))
    if (not var2):
        log.debug('Authenticate failed, client not found.')
        return False
    request.client = var2
    return True
",Authenticate a non - confidential client .,Authenticate a client .,Authenticate a client .,1,Tie,
"def process_result_value(self, value, dialect):
        
        masks = list()
        if value:
            for e in enums.CryptographicUsageMask:
                if e.value & value:
                    masks.append(e)
        return masks","

def func1(arg1, arg2, arg3):
    var1 = list()
    if arg2:
        for var3 in enums.CryptographicUsageMask:
            if (e.value & arg2):
                masks.append(var3)
    return var1
",Returns a new list of enums . CryptographicUsageMask Enums . This converts the integer value into the list of enums .,Return a list of cryptographic usage masks .,Return a list of CryptographicUsageMask objects .,0,Tie,
"def query(self, minhash, size):
        '''
        
        '''
        for i, index in enumerate(self.indexes):
            u = self.uppers[i]
            if u is None:
                continue
            b, r = self._get_optimal_param(u, size)
            for key in index[r]._query_b(minhash, b):
                yield key","

def func1(arg1, arg2, arg3):
    '\n        \n        '
    for (var1, var2) in enumerate(self.indexes):
        var4 = self.uppers[var1]
        if (var4 is None):
            continue
        (var5, var6) = self._get_optimal_param(var4, arg3)
        for var7 in index[r]._query_b(arg2, var5):
            (yield var7)
",Giving the MinHash and size of the query set retrieve keys that references sets with containment with respect to the query set greater than the threshold .,\ n \ n,Returns an iterator over the keys that match the given hash .,0,Prediction2,
"def remove_subscriber(self, ws):
        
        if ws in self.subscribers:
            self.subscribers.remove(ws)

        for name in self.available_events:
            self.remove_event_subscriber(name, ws)","

def func1(arg1, arg2):
    if (arg2 in self.subscribers):
        self.subscribers.remove(arg2)
    for var1 in self.available_events:
        self.remove_event_subscriber(var1, arg2)
",Remove a websocket subscriber .,Remove a subscriber .,Remove a subscriber from the list of subscribers .,1,Tie,
"def _create_msg(self, to, subject, msgHtml, msgPlain, attachments=None):
        '''
        
        '''
        sender = self.sender
        if attachments and isinstance(attachments, str):
            attachments = [attachments]
        else:
            attachments = list(attachments or [])

        msg = MIMEMultipart('alternative')
        msg['Subject'] = subject
        msg['From'] = sender
        msg['To'] = to
        msg.attach(MIMEText(msgPlain, 'plain'))
        msg.attach(MIMEText(msgHtml, 'html'))

        # append attachments if any
        for path in attachments:
            _attachment = self._prep_attachment(path)
            msg.attach(_attachment)

        raw = base64.urlsafe_b64encode(msg.as_bytes()).decode()
        #raw = raw.decode()
        body = {'raw': raw}
        return body","

def func1(arg1, arg2, arg3, arg4, arg5, arg6=None):
    '\n        \n        '
    var1 = self.sender
    if (arg6 and isinstance(arg6, var3)):
        arg6 = [arg6]
    else:
        arg6 = list((arg6 or []))
    var5 = MIMEMultipart('alternative')
    var5['Subject'] = arg3
    var5['From'] = var1
    var5['To'] = arg2
    msg.attach(MIMEText(arg5, 'plain'))
    msg.attach(MIMEText(arg4, 'html'))
    for var8 in arg6:
        var9 = self._prep_attachment(var8)
        msg.attach(var9)
    var10 = base64.urlsafe_b64encode(msg.as_bytes()).decode()
    var11 = {'raw': var10}
    return var11
",attachments should be a list of paths,\ n \ n,Create an email message .,0,Prediction2,
"def constant(name, value):
  
  if not config_parser.MODULE_RE.match(name):
    raise ValueError(""Invalid constant selector '{}'."".format(name))

  if _CONSTANTS.matching_selectors(name):
    err_str = ""Constants matching selector '{}' already exist ({}).""
    raise ValueError(err_str.format(name, _CONSTANTS.matching_selectors(name)))

  _CONSTANTS[name] = value","

def func1(arg1, arg2):
    if (not config_parser.MODULE_RE.match(arg1)):
        raise ValueError(""Invalid constant selector '{}'."".format(arg1))
    if _CONSTANTS.matching_selectors(arg1):
        var2 = ""Constants matching selector '{}' already exist ({}).""
        raise ValueError(err_str.format(arg1, _CONSTANTS.matching_selectors(arg1)))
    var3[arg1] = arg2
",Creates a constant that can be referenced from gin config files .,Set the value of arg1 to arg2 .,Register a new constant .,0,Prediction2,
"def poke(self, context):
        
        if '.' in self.table_name:
            self.database_name, self.table_name = self.table_name.split('.')
        self.log.info(
            'Poking for table %s. %s, expression %s', self.database_name, self.table_name, self.expression
        )

        return self.get_hook().check_for_partition(
            self.database_name, self.table_name, self.expression)","

def func1(arg1, arg2):
    if ('.' in self.table_name):
        (self.database_name, self.table_name) = self.table_name.split('.')
    self.log.info('Poking for table %s. %s, expression %s', self.database_name, self.table_name, self.expression)
    return self.get_hook().check_for_partition(self.database_name, self.table_name, self.expression)
",Checks for existence of the partition in the AWS Glue Catalog table,Poking for a partition .,Pokes for a partition in the database .,0,Prediction2,
"def get_out_file(self, ending='rst'):
        
        return os.path.splitext(self.outfile)[0] + os.path.extsep + ending","

def func1(arg1, arg2='rst'):
    return ((os.path.splitext(self.outfile)[0] + os.path.extsep) + arg2)
",get the output file with the specified ending,Return the name of the output file .,Return the path to the first RST file .,1,Tie,
"def get_url_rev(self):
        
        if '://' not in self.url:
            assert 'file:' not in self.url
            self.url = self.url.replace('git+', 'git+ssh://')
            url, rev = super(Git, self).get_url_rev()
            url = url.replace('ssh://', '')
        else:
            url, rev = super(Git, self).get_url_rev()

        return url, rev","

def func1(arg1):
    if ('://' not in self.url):
        assert ('file:' not in self.url)
        self.url = self.url.replace('git+', 'git+ssh://')
        (var1, var2) = super(Git, self).get_url_rev()
        var1 = url.replace('ssh://', '')
    else:
        (var1, var2) = super(Git, self).get_url_rev()
    return (var1, var2)
",Prefixes stub URLs like user,Remove git + ssh + from the url .,replace git + with git + ssh://,1,Tie,
"def measure_topology(fbasename=None, log=None, ml_version=ml_version):
    
    ml_script1_file = 'TEMP3D_measure_topology.mlx'
    ml_script1 = mlx.FilterScript(file_in=fbasename, ml_version=ml_version)
    compute.measure_topology(ml_script1)
    ml_script1.save_to_file(ml_script1_file)
    ml_script1.run_script(log=log, script_file=ml_script1_file)
    topology = ml_script1.topology
    return topology","

def func1(arg1=None, arg2=None, arg3=arg3):
    var1 = 'TEMP3D_measure_topology.mlx'
    var2 = mlx.FilterScript(file_in=arg1, ml_version=arg3)
    compute.measure_topology(var2)
    ml_script1.save_to_file(var1)
    ml_script1.run_script(log=arg2, script_file=var1)
    var3 = ml_script1.topology
    return var3
",Measures mesh topology,Function 1 to compute the measure topology .,Measure the topology of a 3D file .,0,Tie,
"def plot_waterfall(self, f_start=None, f_stop=None, if_id=0, logged=True, cb=True, MJD_time=False, **kwargs):
        

        plot_f, plot_data = self.grab_data(f_start, f_stop, if_id)

        #Using accending frequency for all plots.
        if self.header[b'foff'] < 0:
            plot_data = plot_data[..., ::-1] # Reverse data
            plot_f = plot_f[::-1]

        if logged:
            plot_data = db(plot_data)

        # Make sure waterfall plot is under 4k*4k
        dec_fac_x, dec_fac_y = 1, 1
        if plot_data.shape[0] > MAX_IMSHOW_POINTS[0]:
            dec_fac_x = int(plot_data.shape[0] / MAX_IMSHOW_POINTS[0])

        if plot_data.shape[1] > MAX_IMSHOW_POINTS[1]:
            dec_fac_y =  int(plot_data.shape[1] /  MAX_IMSHOW_POINTS[1])

        plot_data = rebin(plot_data, dec_fac_x, dec_fac_y)

        try:
            plt.title(self.header[b'source_name'])
        except KeyError:
            plt.title(self.filename)

        extent = self._calc_extent(plot_f=plot_f,plot_t=self.timestamps,MJD_time=MJD_time)

        plt.imshow(plot_data,
            aspect='auto',
            origin='lower',
            rasterized=True,
            interpolation='nearest',
            extent=extent,
            cmap='viridis',
            **kwargs
        )
        if cb:
            plt.colorbar()
        plt.xlabel(""Frequency [MHz]"")
        if MJD_time:
            plt.ylabel(""Time [MJD]"")
        else:
            plt.ylabel(""Time [s]"")","

def func1(arg1, arg2=None, arg3=None, arg4=0, arg5=True, arg6=True, arg7=False, **arg8):
    (var1, var2) = self.grab_data(arg2, arg3, arg4)
    if (self.header[b'foff'] < 0):
        var2 = var2[..., ::(- 1)]
        var1 = var1[::(- 1)]
    if arg5:
        var2 = db(var2)
    (var4, var5) = (1, 1)
    if (plot_data.shape[0] > var6[0]):
        var4 = int((plot_data.shape[0] / var6[0]))
    if (plot_data.shape[1] > var6[1]):
        var5 = int((plot_data.shape[1] / var6[1]))
    var2 = rebin(var2, var4, var5)
    try:
        plt.title(self.header[b'source_name'])
    except KeyError:
        plt.title(self.filename)
    var10 = self._calc_extent(plot_f=var1, plot_t=self.timestamps, MJD_time=arg7)
    plt.imshow(var2, aspect='auto', origin='lower', rasterized=True, interpolation='nearest', extent=var10, cmap='viridis', **arg8)
    if arg6:
        plt.colorbar()
    plt.xlabel('Frequency [MHz]')
    if arg7:
        plt.ylabel('Time [MJD]')
    else:
        plt.ylabel('Time [s]')
",Plot waterfall of data,Plot a waterfall .,Function 1 of the plot .,1,Prediction1,
"def _build(self, model):
    
    if not _is_dict_like(model):
      raise TypeError('`model` must be convertible to `dict` (saw: {}).'.format(
          type(model).__name__))
    [
        self._dist_fn,
        self._dist_fn_wrapped,
        self._dist_fn_args,
        self._dist_fn_name,  # JointDistributionSequential doesn't have this.
    ] = _prob_chain_rule_flatten(model)","

def func1(arg1, arg2):
    if (not _is_dict_like(arg2)):
        raise TypeError('`model` must be convertible to `dict` (saw: {}).'.format(type(model).__name__))
    [self._dist_fn, self._dist_fn_wrapped, self._dist_fn_args, self._dist_fn_name] = _prob_chain_rule_flatten(arg2)
",Creates dist_fn dist_fn_wrapped dist_fn_args dist_fn_name .,Build the distribution function .,Wrapper for _prob_chain_rule_flatten .,1,Prediction2,
"def demo(funcname, interactive=True, echo=True, test=False):
    
    import h2o.demos as h2odemo
    assert_is_type(funcname, str)
    assert_is_type(interactive, bool)
    assert_is_type(echo, bool)
    assert_is_type(test, bool)

    demo_function = getattr(h2odemo, funcname, None)
    if demo_function and type(demo_function) is type(demo):
        demo_function(interactive, echo, test)
    else:
        print(""Demo for %s is not available."" % funcname)","

def var8(arg1, arg2=True, arg3=True, arg4=False):
    import h2o.demos as h2odemo
    assert_is_type(arg1, var2)
    assert_is_type(arg2, var3)
    assert_is_type(arg3, var3)
    assert_is_type(arg4, var3)
    var4 = getattr(var6, arg1, None)
    if (var4 and (type(var4) is type(var8))):
        demo_function(arg2, arg3, arg4)
    else:
        print(('Demo for %s is not available.' % arg1))
",H2O built - in demo facility .,Run a demo function .,Demo function .,1,Tie,
"def filter_by_label(X, y, ref_label, reverse=False):
    '''
    
    '''
    check_reference_label(y, ref_label)

    return list(zip(*filter(lambda t: (not reverse) == (t[1] == ref_label),
                            zip(X, y))))","

def func1(arg1, arg2, arg3, arg4=False):
    '\n    \n    '
    check_reference_label(arg2, arg3)
    return list(zip(*filter((lambda arg5: ((not arg4) == (arg5[1] == arg3))), zip(arg1, arg2))))
",Select items with label from dataset .,\ n \ n,Filter X and y by label .,0,Prediction2,
"def symmetric_difference_update(self, oset: Scope) -> Scope:
        
        skey = set()
        keys = list(self._hsig.keys())
        for k in keys:
            if k in oset:
                skey.add(k)
        for k in oset._hsig.keys():
            if k not in skey:
                self._hsig[k] = oset.get(k)
        for k in skey:
            del self._hsig[k]
        return self","

def func1(arg1, arg2: Scope) -> var6:
    var1 = set()
    var3 = list(self._hsig.keys())
    for var5 in var3:
        if (var5 in arg2):
            skey.add(var5)
    for var5 in oset._hsig.keys():
        if (var5 not in var1):
            self._hsig[var5] = oset.get(var5)
    for var5 in var1:
        del self._hsig[var5]
    return arg1
",Remove common values and Update specific values from another Set,Update self with the symmetric difference of oset .,Change the scope of arg1 to arg2 .,1,Prediction1,
"def str_lower(x):
    
    sl = _to_string_sequence(x).lower()
    return column.ColumnStringArrow(sl.bytes, sl.indices, sl.length, sl.offset, string_sequence=sl)","

def func1(arg1):
    var1 = _to_string_sequence(x).lower()
    return column.ColumnStringArrow(sl.bytes, sl.indices, sl.length, sl.offset, string_sequence=var1)
",Converts string samples to lower case .,Convert arg1 to a column string arrow .,Convert a string sequence to a string arrow .,0,Prediction2,
"def _execute(self, source, hidden):
        
        msg_id = self.kernel_manager.shell_channel.execute(source, hidden)
        self._request_info['execute'][msg_id] = self._ExecutionRequest(msg_id, 'user')
        self._hidden = hidden
        if not hidden:
            self.executing.emit(source)","

def func1(arg1, arg2, arg3):
    var1 = self.kernel_manager.shell_channel.execute(arg2, arg3)
    self._request_info['execute'][var1] = self._ExecutionRequest(var1, 'user')
    self._hidden = arg3
    if (not arg3):
        self.executing.emit(arg2)
",Execute source . If hidden do not show any output .,Execute a shell command .,Called when a command is executed .,1,Prediction1,
"def choi_to_rauli(choi, order=1):
    
    if order == 0:
        order = 'weight'
    elif order == 1:
        order = 'tensor'

    # get number of qubits'
    num_qubits = int(np.log2(np.sqrt(len(choi))))
    pgp = pauli_group(num_qubits, case=order)
    rauli = []
    for i in pgp:
        for j in pgp:
            pauliop = np.kron(j.to_matrix().T, i.to_matrix())
            rauli += [np.trace(np.dot(choi, pauliop))]
    return np.array(rauli).reshape(4 ** num_qubits, 4 ** num_qubits)","

def func1(arg1, arg2=1):
    if (arg2 == 0):
        arg2 = 'weight'
    elif (arg2 == 1):
        arg2 = 'tensor'
    var1 = int(np.log2(np.sqrt(len(arg1))))
    var4 = pauli_group(var1, case=arg2)
    var6 = []
    for var7 in var4:
        for var8 in var4:
            var9 = np.kron(j.to_matrix().T, i.to_matrix())
            var6 += [np.trace(np.dot(arg1, var9))]
    return np.array(rauli).reshape((4 ** var1), (4 ** var1))
",Convert a Choi - matrix to a Pauli - basis superoperator .,Convert a 2D matrix to a 4x4 matrix .,func1 is a wrapper for pauli_group,1,Prediction2,
"def build_input_pipeline(x, y, batch_size):
  
  training_dataset = tf.data.Dataset.from_tensor_slices((x, y))
  training_batches = training_dataset.repeat().batch(batch_size)
  training_iterator = tf.compat.v1.data.make_one_shot_iterator(training_batches)
  batch_features, batch_labels = training_iterator.get_next()
  return batch_features, batch_labels","

def func1(arg1, arg2, arg3):
    var1 = tf.data.Dataset.from_tensor_slices((arg1, arg2))
    var2 = training_dataset.repeat().batch(arg3)
    var3 = tf.compat.v1.data.make_one_shot_iterator(var2)
    (var4, var5) = training_iterator.get_next()
    return (var4, var5)
",Build a Dataset iterator for supervised classification .,Build input pipeline .,One - shot version of func1 .,1,Prediction1,
"def load_config_file(self, filename, path=None):
        
        loader = PyFileConfigLoader(filename, path=path)
        try:
            config = loader.load_config()
        except ConfigFileNotFound:
            # problem finding the file, raise
            raise
        except Exception:
            # try to get the full filename, but it will be empty in the
            # unlikely event that the error raised before filefind finished
            filename = loader.full_filename or filename
            # problem while running the file
            self.log.error(""Exception while loading config file %s"",
                            filename, exc_info=True)
        else:
            self.log.debug(""Loaded config file: %s"", loader.full_filename)
            self.update_config(config)","

def func1(arg1, arg2, arg3=None):
    var1 = PyFileConfigLoader(arg2, path=arg3)
    try:
        var3 = loader.load_config()
    except ConfigFileNotFound:
        raise
    except Exception:
        arg2 = (loader.full_filename or arg2)
        self.log.error('Exception while loading config file %s', arg2, exc_info=True)
    else:
        self.log.debug('Loaded config file: %s', loader.full_filename)
        self.update_config(var3)
",Load a . py based config file by filename and path .,Load a config file .,Load config from a file .,0,Tie,
"def register(name: str = None) -> type:
    
    def decorate(model_cls: type, reg_name: str = None) -> type:
        model_name = reg_name or short_name(model_cls)
        global _REGISTRY
        cls_name = model_cls.__module__ + ':' + model_cls.__name__
        if model_name in _REGISTRY and _REGISTRY[model_name] != cls_name:
            logger.warning('Registry name ""{}"" has been already registered and will be overwritten.'.format(model_name))
        _REGISTRY[model_name] = cls_name
        return model_cls

    return lambda model_cls_name: decorate(model_cls_name, name)","

def func2(arg1: str=None) -> var5:

    def func1(arg2: type, arg3: str=None) -> var5:
        var1 = (arg3 or short_name(arg2))
        global _REGISTRY
        var3 = ((model_cls.__module__ + ':') + model_cls.__name__)
        if ((var1 in var4) and (var4[var1] != var3)):
            logger.warning('Registry name ""{}"" has been already registered and will be overwritten.'.format(var1))
        var4[var1] = var3
        return arg2
    return (lambda arg4: decorate(arg4, arg1))
",Register classes that could be initialized from JSON configuration file . If name is not passed the class name is converted to snake - case .,Register a model class with the registry .,Decorate a function to register a registry name .,1,Tie,
"def _register_engine(self, uid):
        
        # head of the line:
        self.targets.insert(0,uid)
        self.loads.insert(0,0)

        # initialize sets
        self.completed[uid] = set()
        self.failed[uid] = set()
        self.pending[uid] = {}

        # rescan the graph:
        self.update_graph(None)","

def func1(arg1, arg2):
    self.targets.insert(0, arg2)
    self.loads.insert(0, 0)
    self.completed[arg2] = set()
    self.failed[arg2] = set()
    self.pending[arg2] = {}
    self.update_graph(None)
",New engine with ident uid became available .,This function is called when a new target is added to the graph .,Register a new engine .,0,Prediction2,
"def validated_user(cls, token, purpose, minutes_valid):
        
        try:
            username, auth_hash = loads(token.decode('base64'))
        except (ValueError, Error):
            cls.auth_failed(token=token)
        try:
            user = cls.user_model.objects.get(**{
                cls.user_model.USERNAME_FIELD: username,
                'is_active': True,
            })
            user.backend = 'django.contrib.auth.backends.ModelBackend'
        except cls.user_model.DoesNotExist:
            cls.auth_failed(username=username, token=token)
        if auth_hash not in iter_auth_hashes(user, purpose, minutes_valid):
            cls.auth_failed(username=username, token=token)
        return user","

def func1(arg1, arg2, arg3, arg4):
    try:
        (var1, var2) = loads(token.decode('base64'))
    except (var4, var5):
        cls.auth_failed(token=arg2)
    try:
        var6 = cls.user_model.objects.get(**{cls.user_model.USERNAME_FIELD: var1, 'is_active': True})
        user.backend = 'django.contrib.auth.backends.ModelBackend'
    except cls.user_model.DoesNotExist:
        cls.auth_failed(username=var1, token=arg2)
    if (var2 not in iter_auth_hashes(var6, arg3, arg4)):
        cls.auth_failed(username=var1, token=arg2)
    return var6
",Resolve and validate auth token returns user object .,Validate a token and return a User instance .,Authenticates a token .,1,Prediction1,
"def _find_indent(self, line):
        
        indent_spaces = self.indent_spaces
        full_dedent = self._full_dedent
        
        inisp = num_ini_spaces(line)
        if inisp < indent_spaces:
            indent_spaces = inisp
            if indent_spaces <= 0:
                #print 'Full dedent in text',self.source # dbg
                full_dedent = True

        if line.rstrip()[-1] == ':':
            indent_spaces += 4
        elif dedent_re.match(line):
            indent_spaces -= 4
            if indent_spaces <= 0:
                full_dedent = True

        # Safety
        if indent_spaces < 0:
            indent_spaces = 0
            #print 'safety' # dbg
            
        return indent_spaces, full_dedent","

def func1(arg1, arg2):
    var1 = self.indent_spaces
    var2 = self._full_dedent
    var3 = num_ini_spaces(arg2)
    if (var3 < var1):
        var1 = var3
        if (var1 <= 0):
            var2 = True
    if (line.rstrip()[(- 1)] == ':'):
        var1 += 4
    elif dedent_re.match(arg2):
        var1 -= 4
        if (var1 <= 0):
            var2 = True
    if (var1 < 0):
        var1 = 0
    return (var1, var2)
",Compute the new indentation level for a single line .,Return the number of spaces and whether the line should be indented .,Decide whether to use indent_spaces and _full_dedent .,1,Prediction1,
"def eval_file(filename: str, ctx: compiler.CompilerContext, module: types.ModuleType):
    
    last = None
    for form in reader.read_file(filename, resolver=runtime.resolve_alias):
        last = compiler.compile_and_exec_form(form, ctx, module)
    return last","

def func1(arg1: str, arg2: compiler.CompilerContext, arg3: types.ModuleType):
    var1 = None
    for var2 in reader.read_file(arg1, resolver=runtime.resolve_alias):
        var1 = compiler.compile_and_exec_form(var2, arg2, arg3)
    return var1
",Evaluate a file with the given name into a Python module AST node .,Evaluate the given file and return the result .,Compile and execute form for arg1 .,1,Prediction1,
"def open_log_file(self):
        
        """"""
        TO DO.
        """"""
        # webbrowser.open('https://marrabld.github.io/planarradpy/')
        f = open(os.path.expanduser('~/.planarradpy/log/libplanarradpy.log'))
        # self.uiLog.textEdit.setText(str(f.readlines()))
        self.uiLog.textEdit.setPlainText(str(f.read()))
        self.log_window.show()","

def func1(arg1):
    '\n        TO DO.\n        '
    var1 = open(os.path.expanduser('~/.planarradpy/log/libplanarradpy.log'))
    self.uiLog.textEdit.setPlainText(str(f.read()))
    self.log_window.show()
",The following opens the log file of PlanarRad .,Opens the log file .,TO DO \ n,1,Prediction1,
"def _m(self):
        
        assert not hasattr(self, ""_interfaces"") or not self._interfaces, \
            ""Too late to change direction of interface""
        self._direction = DIRECTION.asIntfDirection(DIRECTION.opposite(self._masterDir))

        return self","

def func1(arg1):
    assert ((not hasattr(arg1, '_interfaces')) or (not self._interfaces)), 'Too late to change direction of interface'
    self._direction = DIRECTION.asIntfDirection(DIRECTION.opposite(self._masterDir))
    return arg1
",Note that this interface will be master,Change the direction of an interface .,Change the direction of the interface,0,Tie,
"def find_source(self, filename):
        
        source = None

        base, ext = os.path.splitext(filename)
        TRY_EXTS = {
            '.py':  ['.py', '.pyw'],
            '.pyw': ['.pyw'],
        }
        try_exts = TRY_EXTS.get(ext)
        if not try_exts:
            return filename, None

        for try_ext in try_exts:
            try_filename = base + try_ext
            if os.path.exists(try_filename):
                return try_filename, None
            source = self.coverage.file_locator.get_zip_data(try_filename)
            if source:
                return try_filename, source
        raise NoSource(""No source for code: '%s'"" % filename)","

def func1(arg1, arg2):
    var1 = None
    (var2, var3) = os.path.splitext(arg2)
    var4 = {'.py': ['.py', '.pyw'], '.pyw': ['.pyw']}
    var5 = TRY_EXTS.get(var3)
    if (not var5):
        return (arg2, None)
    for var6 in var5:
        var7 = (var2 + var6)
        if os.path.exists(var7):
            return (var7, None)
        var1 = self.coverage.file_locator.get_zip_data(var7)
        if var1:
            return (var7, var1)
    raise NoSource((""No source for code: '%s'"" % arg2))
",Find the source for filename .,Try to find a source file for the given code .,Find the source for the given filename .,0,Tie,
"def remove(request, video_id):
    

    # prepare redirection url
    try:
        next_url = settings.YOUTUBE_DELETE_REDIRECT_URL
    except AttributeError:
        next_url = reverse(""django_youtube.views.upload"")

    # Remove from db
    try:
        Video.objects.get(video_id=video_id).delete()
    except:
        from django.contrib import messages
        messages.add_message(
            request, messages.ERROR, _('Video could not be deleted.'))

    # Return to upload page or specified page
    return HttpResponseRedirect(next_url)","

def func1(arg1, arg2):
    try:
        var1 = settings.YOUTUBE_DELETE_REDIRECT_URL
    except AttributeError:
        var1 = reverse('django_youtube.views.upload')
    try:
        Video.objects.get(video_id=video_id).delete()
    except:
        from django.contrib import messages
        messages.add_message(arg1, messages.ERROR, _('Video could not be deleted.'))
    return HttpResponseRedirect(var1)
",Removes the video from youtube and from db Requires POST,Delete a video .,Remove a video .,0,Tie,
"def dict_self(self):
        
        return {k: v for k, v in self.__dict__.items() if k in FSM_ATTRS}","

def func1(arg1):
    return {var1: var2 for (var1, var2) in self.__dict__.items() if (var1 in var3)}
",Return the self object attributes not inherited as dict .,Return a copy of the FSM as a dictionary .,Return a dict of variables that are equal to arg1 .,1,Prediction1,
"def album_tracks(self, spotify_id, limit=20, offset=0, market='US'):
        
        route = Route('GET', '/albums/{spotify_id}/tracks', spotify_id=spotify_id)
        payload = {'limit': limit, 'offset': offset}

        if market:
            payload['market'] = market

        return self.request(route, params=payload)","

def func1(arg1, arg2, arg3=20, arg4=0, arg5='US'):
    var1 = Route('GET', '/albums/{spotify_id}/tracks', spotify_id=arg2)
    var3 = {'limit': arg3, 'offset': arg4}
    if arg5:
        var3['market'] = arg5
    return self.request(var1, params=var3)
",Get an albums tracks by an ID .,Get a list of tracks for a given album .,Request tracks from Spotify .,1,Tie,
"def _load(self, load_dict):
        
        for key in list(load_dict.keys()):
            # We delete keys over time:
            if key in load_dict:
                if SparseResult.IDENTIFIER in key:
                    new_key = key.split(SparseResult.IDENTIFIER)[0]

                    is_dia = load_dict.pop(new_key + SparseResult.IDENTIFIER + 'is_dia')

                    name_list = SparseParameter._get_name_list(is_dia)
                    rename_list = ['%s%s%s' % (new_key, SparseResult.IDENTIFIER, name)
                                   for name in name_list]

                    data_list = [load_dict.pop(name) for name in rename_list]
                    matrix = SparseParameter._reconstruct_matrix(data_list)
                    self._data[new_key] = matrix
                else:
                    self._data[key] = load_dict[key]","

def func1(arg1, arg2):
    for var1 in list(load_dict.keys()):
        if (var1 in arg2):
            if (SparseResult.IDENTIFIER in var1):
                var3 = key.split(SparseResult.IDENTIFIER)[0]
                var4 = load_dict.pop(((var3 + SparseResult.IDENTIFIER) + 'is_dia'))
                var5 = SparseParameter._get_name_list(var4)
                var6 = [('%s%s%s' % (var3, SparseResult.IDENTIFIER, var7)) for var7 in var5]
                var8 = [load_dict.pop(var7) for var7 in var6]
                var9 = SparseParameter._reconstruct_matrix(var8)
                self._data[var3] = var9
            else:
                self._data[var1] = arg2[var1]
",Loads data from load_dict,Load data from a dictionary .,This function is used to construct a sparse parameter from a dictionary .,1,Prediction1,
"def _writeResponse(self, response):
        
        encoded = dumps(response, default=_default)
        self.transport.write(encoded)","

def func1(arg1, arg2):
    var1 = dumps(arg2, default=var3)
    self.transport.write(var1)
",Serializes the response to JSON and writes it to the transport .,Write a single value to the transport .,Write a response to the transport .,0,Tie,
"def register_options_provider(self, provider, own_group=True):
        
        assert provider.priority <= 0, ""provider's priority can't be >= 0""
        for i in range(len(self.options_providers)):
            if provider.priority > self.options_providers[i].priority:
                self.options_providers.insert(i, provider)
                break
        else:
            self.options_providers.append(provider)
        non_group_spec_options = [
            option for option in provider.options if ""group"" not in option[1]
        ]
        groups = getattr(provider, ""option_groups"", ())
        if own_group and non_group_spec_options:
            self.add_option_group(
                provider.name.upper(),
                provider.__doc__,
                non_group_spec_options,
                provider,
            )
        else:
            for opt, optdict in non_group_spec_options:
                self.add_optik_option(provider, self.cmdline_parser, opt, optdict)
        for gname, gdoc in groups:
            gname = gname.upper()
            goptions = [
                option
                for option in provider.options
                if option[1].get(""group"", """").upper() == gname
            ]
            self.add_option_group(gname, gdoc, goptions, provider)","

def func1(arg1, arg2, arg3=True):
    assert (provider.priority <= 0), ""provider's priority can't be >= 0""
    for var1 in range(len(self.options_providers)):
        if (provider.priority > self.options_providers[i].priority):
            self.options_providers.insert(var1, arg2)
            break
    else:
        self.options_providers.append(arg2)
    var4 = [var5 for var5 in provider.options if ('group' not in var5[1])]
    var6 = getattr(arg2, 'option_groups', ())
    if (arg3 and var4):
        self.add_option_group(provider.name.upper(), provider.__doc__, var4, arg2)
    else:
        for (var8, var9) in var4:
            self.add_optik_option(arg2, self.cmdline_parser, var8, var9)
    for (var10, var11) in var6:
        var10 = gname.upper()
        var12 = [var5 for var5 in provider.options if (option[1].get('group', '').upper() == var10)]
        self.add_option_group(var10, var11, var12, arg2)
",register an options provider,Register an option provider .,Add an option to the command line parser .,1,Prediction1,
"def print_wordfreq(freqs, n=10):
    
    
    words, counts = freqs.keys(), freqs.values()
    items = zip(counts, words)
    items.sort(reverse=True)
    for (count, word) in items[:n]:
        print(word, count)","

def func1(arg1, arg2=10):
    (var1, var2) = (freqs.keys(), freqs.values())
    var3 = zip(var2, var1)
    items.sort(reverse=True)
    for (var5, var6) in var3[:arg2]:
        print(var6, var5)
",Print the n most common words and counts in the freqs dict .,Print n most frequent words .,prints the frequency of arg1 to arg2,1,Prediction1,
"def mkdirs(self, target):
    ''''''
    path = os.path.dirname(target)
    if path and path != PATH_SEP and not os.path.isdir(path):
      # Multi-threading means there will be intervleaved execution
      # between the check and creation of the directory.
      try:
        os.makedirs(path)
      except OSError as ose:
        if ose.errno != errno.EEXIST:
          raise Failure('Unable to create directory (%s)' % (path,))","

def func1(arg1, arg2):
    ''
    var1 = os.path.dirname(arg2)
    if (var1 and (var1 != var2) and (not os.path.isdir(var1))):
        try:
            os.makedirs(var1)
        except OSError as ose:
            if (ose.errno != errno.EEXIST):
                raise Failure(('Unable to create directory (%s)' % (var1,)))
",Ensure all directories are created for a given target file .,Create a directory if it doesn t exist .,Create a new directory if it doesn t exist .,1,Tie,
"def validateAttrib(self, method, cls = None):
        
        # TODO: is there a need for case-sensitive value comparison?
        any = False
        for group in self.attribs:
            match = True
            for key, value in group:
                attr = get_method_attr(method, cls, key)
                if callable(value):
                    if not value(key, method, cls):
                        match = False
                        break
                elif value is True:
                    # value must exist and be True
                    if not bool(attr):
                        match = False
                        break
                elif value is False:
                    # value must not exist or be False
                    if bool(attr):
                        match = False
                        break
                elif type(attr) in (list, tuple):
                    # value must be found in the list attribute
                    if not str(value).lower() in [str(x).lower()
                                                  for x in attr]:
                        match = False
                        break
                else:
                    # value must match, convert to string and compare
                    if (value != attr
                        and str(value).lower() != str(attr).lower()):
                        match = False
                        break
            any = any or match
        if any:
            # not True because we don't want to FORCE the selection of the
            # item, only say that it is acceptable
            return None
        return False","

def func1(arg1, arg2, arg3=None):
    var1 = False
    for var2 in self.attribs:
        var3 = True
        for (var4, var5) in var2:
            var6 = get_method_attr(arg2, arg3, var4)
            if callable(var5):
                if (not value(var4, arg2, arg3)):
                    var3 = False
                    break
            elif (var5 is True):
                if (not bool(var6)):
                    var3 = False
                    break
            elif (var5 is False):
                if bool(var6):
                    var3 = False
                    break
            elif (type(var6) in (var11, var12)):
                if (not (str(value).lower() in [str(x).lower() for var13 in var6])):
                    var3 = False
                    break
            elif ((var5 != var6) and (str(value).lower() != str(attr).lower())):
                var3 = False
                break
        var1 = (var1 or var3)
    if var1:
        return None
    return False
",Verify whether a method has the required attributes The method is considered a match if it matches all attributes for any attribute group . .,Validate an attribute against a method .,returns True if arg1 == arg2 or arg3 == None,1,Prediction1,
"def reasonable_desired_version(self, desired_version, allow_equal=False,
                                  allow_patch_skip=False):
        
        try:
            desired_version = desired_version.base_version
        except:
            pass
        (new_major, new_minor, new_patch) = \
                map(int, desired_version.split('.'))

        tag_versions = self._versions_from_tags()
        if not tag_versions:
            # no tags yet, and legal version is legal!
            return """"
        max_version = max(self._versions_from_tags()).base_version
        (old_major, old_minor, old_patch) = \
                map(int, str(max_version).split('.'))

        update_str = str(max_version) + "" -> "" + str(desired_version)

        v_desired = vers.Version(desired_version)
        v_max = vers.Version(max_version)

        if allow_equal and v_desired == v_max:
            return """"

        if v_desired < v_max:
            return (""Bad update: New version doesn't increase on last tag: ""
                    + update_str + ""\n"")

        bad_update = skipped_version((old_major, old_minor, old_patch),
                                     (new_major, new_minor, new_patch),
                                     allow_patch_skip)

        msg = """"
        if bad_update:
            msg = (""Bad update: Did you skip a version from ""
                   + update_str + ""?\n"")

        return msg","

def func1(arg1, arg2, arg3=False, arg4=False):
    try:
        arg2 = desired_version.base_version
    except:
        pass
    (var1, var2, var3) = map(var5, desired_version.split('.'))
    var6 = self._versions_from_tags()
    if (not var6):
        return ''
    var7 = max(self._versions_from_tags()).base_version
    (var8, var9, var10) = map(var5, str(max_version).split('.'))
    var11 = ((str(var7) + ' -> ') + str(arg2))
    var13 = vers.Version(arg2)
    var14 = vers.Version(var7)
    if (arg3 and (var13 == var14)):
        return ''
    if (var13 < var14):
        return ((""Bad update: New version doesn't increase on last tag: "" + var11) + '\n')
    var15 = skipped_version((var8, var9, var10), (var1, var2, var3), arg4)
    var17 = ''
    if var15:
        var17 = (('Bad update: Did you skip a version from ' + var11) + '?\n')
    return var17
",Determine whether the desired version is a reasonable next version .,Return an update string for the given arguments .,Verify that the desired version is reasonable .,0,Prediction2,
"def resolve_weak_types(storage, debug=False):
    

    for run in storage['runs']:
        prev_strong = prev_type = run['sor']
        start, length = run['start'], run['length']
        chars = storage['chars'][start:start+length]
        for _ch in chars:
            # W1. Examine each nonspacing mark (NSM) in the level run, and
            # change the type of the NSM to the type of the previous character.
            # If the NSM is at the start of the level run, it will get the type
            # of sor.
            bidi_type = _ch['type']

            if bidi_type == 'NSM':
                _ch['type'] = bidi_type = prev_type

            # W2. Search backward from each instance of a European number until
            # the first strong type (R, L, AL, or sor) is found. If an AL is
            # found, change the type of the European number to Arabic number.
            if bidi_type == 'EN' and prev_strong == 'AL':
                _ch['type'] = 'AN'

            # update prev_strong if needed
            if bidi_type in ('R', 'L', 'AL'):
                prev_strong = bidi_type

            prev_type = _ch['type']

        # W3. Change all ALs to R
        for _ch in chars:
            if _ch['type'] == 'AL':
                _ch['type'] = 'R'

        # W4. A single European separator between two European numbers changes
        # to a European number. A single common separator between two numbers of
        # the same type changes to that type.
        for idx in range(1, len(chars) - 1):
            bidi_type = chars[idx]['type']
            prev_type = chars[idx-1]['type']
            next_type = chars[idx+1]['type']

            if bidi_type == 'ES' and (prev_type == next_type == 'EN'):
                chars[idx]['type'] = 'EN'

            if bidi_type == 'CS' and prev_type == next_type and \
                    prev_type in ('AN', 'EN'):
                chars[idx]['type'] = prev_type

        # W5. A sequence of European terminators adjacent to European numbers
        # changes to all European numbers.
        for idx in range(len(chars)):
            if chars[idx]['type'] == 'EN':
                for et_idx in range(idx-1, -1, -1):
                    if chars[et_idx]['type'] == 'ET':
                        chars[et_idx]['type'] = 'EN'
                    else:
                        break
                for et_idx in range(idx+1, len(chars)):
                    if chars[et_idx]['type'] == 'ET':
                        chars[et_idx]['type'] = 'EN'
                    else:
                        break

        # W6. Otherwise, separators and terminators change to Other Neutral.
        for _ch in chars:
            if _ch['type'] in ('ET', 'ES', 'CS'):
                _ch['type'] = 'ON'

        # W7. Search backward from each instance of a European number until the
        # first strong type (R, L, or sor) is found. If an L is found, then
        # change the type of the European number to L.
        prev_strong = run['sor']
        for _ch in chars:
            if _ch['type'] == 'EN' and prev_strong == 'L':
                _ch['type'] = 'L'

            if _ch['type'] in ('L', 'R'):
                prev_strong = _ch['type']

    if debug:
        debug_storage(storage, runs=True)","

def func1(arg1, arg2=False):
    for var1 in arg1['runs']:
        var2 = var3 = var1['sor']
        (var4, var5) = (var1['start'], var1['length'])
        var6 = arg1['chars'][var4:(var4 + var5)]
        for var7 in var6:
            var8 = var7['type']
            if (var8 == 'NSM'):
                var7['type'] = var8 = var3
            if ((var8 == 'EN') and (var2 == 'AL')):
                var7['type'] = 'AN'
            if (var8 in ('R', 'L', 'AL')):
                var2 = var8
            var3 = var7['type']
        for var7 in var6:
            if (var7['type'] == 'AL'):
                var7['type'] = 'R'
        for var9 in range(1, (len(var6) - 1)):
            var8 = var6[var9]['type']
            var3 = var6[(var9 - 1)]['type']
            var12 = var6[(var9 + 1)]['type']
            if ((var8 == 'ES') and (var3 == var12 == 'EN')):
                var6[var9]['type'] = 'EN'
            if ((var8 == 'CS') and (var3 == var12) and (var3 in ('AN', 'EN'))):
                var6[var9]['type'] = var3
        for var9 in range(len(var6)):
            if (var6[var9]['type'] == 'EN'):
                for var13 in range((var9 - 1), (- 1), (- 1)):
                    if (var6[var13]['type'] == 'ET'):
                        var6[var13]['type'] = 'EN'
                    else:
                        break
                for var13 in range((var9 + 1), len(var6)):
                    if (var6[var13]['type'] == 'ET'):
                        var6[var13]['type'] = 'EN'
                    else:
                        break
        for var7 in var6:
            if (var7['type'] in ('ET', 'ES', 'CS')):
                var7['type'] = 'ON'
        var2 = var1['sor']
        for var7 in var6:
            if ((var7['type'] == 'EN') and (var2 == 'L')):
                var7['type'] = 'L'
            if (var7['type'] in ('L', 'R')):
                var2 = var7['type']
    if arg2:
        debug_storage(arg1, runs=True)
",Reslove weak type rules W1 - W3 .,func1 is a wrapper for func2,Resolve weak types .,0,Prediction2,
"def convert_to_this_nbformat(nb, orig_version=1):
    
    if orig_version == 1:
        newnb = new_notebook()
        ws = new_worksheet()
        for cell in nb.cells:
            if cell.cell_type == u'code':
                newcell = new_code_cell(input=cell.get('code'),prompt_number=cell.get('prompt_number'))
            elif cell.cell_type == u'text':
                newcell = new_text_cell(u'markdown',source=cell.get('text'))
            ws.cells.append(newcell)
        newnb.worksheets.append(ws)
        return newnb
    else:
        raise ValueError('Cannot convert a notebook from v%s to v2' % orig_version)","

def func1(arg1, arg2=1):
    if (arg2 == 1):
        var1 = new_notebook()
        var3 = new_worksheet()
        for var5 in nb.cells:
            if (cell.cell_type == u'code'):
                var6 = new_code_cell(input=cell.get('code'), prompt_number=cell.get('prompt_number'))
            elif (cell.cell_type == u'text'):
                var6 = new_text_cell(u'markdown', source=cell.get('text'))
            ws.cells.append(var6)
        newnb.worksheets.append(var3)
        return var1
    else:
        raise ValueError(('Cannot convert a notebook from v%s to v2' % arg2))
",Convert a notebook to the v2 format .,Convert a notebook from v1 to v2,Convert a notebook from v1 to v2,0,Tie,
"def get_mems_of_org(self):
        
        print 'Getting members\' emails.'
        for member in self.org_retrieved.iter_members():
            login = member.to_json()['login']
            user_email = self.logged_in_gh.user(login).to_json()['email']
            if user_email is not None:
                self.emails[login] = user_email
            else:#user has no public email
                self.emails[login] = 'none'
            #used for sorting regardless of case
            self.logins_lower[login.lower()] = login","def get_mems_of_org(self):
        
        print 'Getting members\' emails.'
        for member in self.org_retrieved.iter_members():
            login = member.to_json()['login']
            user_email = self.logged_in_gh.user(login).to_json()['email']
            if user_email is not None:
                self.emails[login] = user_email
            else:#user has no public email
                self.emails[login] = 'none'
            #used for sorting regardless of case
            self.logins_lower[login.lower()] = login",Retrieves the emails of the members of the organization . Note this Only gets public emails . Private emails would need authentication for each user .,Retrieves the emails of the members of the organization .,Get the mems of the org .,0,Prediction1,
"def refresh_token(self,
                      grant_type,
                      client_id,
                      client_secret,
                      refresh_token,
                      **params):
        

        # Ensure proper grant_type
        if grant_type != 'refresh_token':
            return self._make_json_error_response('unsupported_grant_type')

        # Check conditions
        is_valid_client_id = self.validate_client_id(client_id)
        is_valid_client_secret = self.validate_client_secret(client_id,
                                                             client_secret)
        scope = params.get('scope', '')
        is_valid_scope = self.validate_scope(client_id, scope)
        data = self.from_refresh_token(client_id, refresh_token, scope)
        is_valid_refresh_token = data is not None

        # Return proper error responses on invalid conditions
        if not (is_valid_client_id and is_valid_client_secret):
            return self._make_json_error_response('invalid_client')

        if not is_valid_scope:
            return self._make_json_error_response('invalid_scope')

        if not is_valid_refresh_token:
            return self._make_json_error_response('invalid_grant')

        # Discard original refresh token
        self.discard_refresh_token(client_id, refresh_token)

        # Generate access tokens once all conditions have been met
        access_token = self.generate_access_token()
        token_type = self.token_type
        expires_in = self.token_expires_in
        refresh_token = self.generate_refresh_token()

        # Save information to be used to validate later requests
        self.persist_token_information(client_id=client_id,
                                       scope=scope,
                                       access_token=access_token,
                                       token_type=token_type,
                                       expires_in=expires_in,
                                       refresh_token=refresh_token,
                                       data=data)

        # Return json response
        return self._make_json_response({
            'access_token': access_token,
            'token_type': token_type,
            'expires_in': expires_in,
            'refresh_token': refresh_token
        })","

def arg5(arg1, arg2, arg3, arg4, arg5, **arg6):
    if (arg2 != 'refresh_token'):
        return self._make_json_error_response('unsupported_grant_type')
    var1 = self.validate_client_id(arg3)
    var2 = self.validate_client_secret(arg3, arg4)
    var3 = params.get('scope', '')
    var4 = self.validate_scope(arg3, var3)
    var5 = self.from_refresh_token(arg3, arg5, var3)
    var6 = (var5 is not None)
    if (not (var1 and var2)):
        return self._make_json_error_response('invalid_client')
    if (not var4):
        return self._make_json_error_response('invalid_scope')
    if (not var6):
        return self._make_json_error_response('invalid_grant')
    self.discard_refresh_token(arg3, arg5)
    var7 = self.generate_access_token()
    var8 = self.token_type
    var9 = self.token_expires_in
    arg5 = self.generate_refresh_token()
    self.persist_token_information(client_id=arg3, scope=var3, access_token=var7, token_type=var8, expires_in=var9, refresh_token=arg5, data=var5)
    return self._make_json_response({'access_token': var7, 'token_type': var8, 'expires_in': var9, 'refresh_token': arg5})
",Generate access token HTTP response from a refresh token .,Refresh a token using a refresh token .,Generate a new access token .,1,Prediction1,
"def get_ticker_price(self, ticker,
                         startDate=None, endDate=None,
                         fmt='json', frequency='daily'):
        
        url = self._get_url(ticker, frequency)
        params = {
            'format': fmt if fmt != ""object"" else 'json',  # conversion local
            'resampleFreq': frequency
        }

        if startDate:
            params['startDate'] = startDate
        if endDate:
            params['endDate'] = endDate

        # TODO: evaluate whether to stream CSV to cache on disk, or
        # load as array in memory, or just pass plain text
        response = self._request('GET', url, params=params)
        if fmt == ""json"":
            return response.json()
        elif fmt == ""object"":
            data = response.json()
            return [dict_to_object(item, ""TickerPrice"") for item in data]
        else:
            return response.content.decode(""utf-8"")","

def func1(arg1, arg2, arg3=None, arg4=None, arg5='json', arg6='daily'):
    var1 = self._get_url(arg2, arg6)
    var2 = {'format': (arg5 if (arg5 != 'object') else 'json'), 'resampleFreq': arg6}
    if arg3:
        var2['startDate'] = arg3
    if arg4:
        var2['endDate'] = arg4
    var3 = self._request('GET', var1, params=var2)
    if (arg5 == 'json'):
        return response.json()
    elif (arg5 == 'object'):
        var4 = response.json()
        return [dict_to_object(var6, 'TickerPrice') for var6 in var4]
    else:
        return response.content.decode('utf-8')
",By default return latest EOD Composite Price for a stock ticker . On average each feed contains 3 data sources .,Get the price of a ticker .,func1 returns a list of TickerPrice objects,1,Tie,
"def get_user_config_dir(app_name, app_author, roaming=True, force_xdg=True):
    
    if WIN:
        key = 'APPDATA' if roaming else 'LOCALAPPDATA'
        folder = os.path.expanduser(os.environ.get(key, '~'))
        return os.path.join(folder, app_author, app_name)
    if MAC and not force_xdg:
        return os.path.join(os.path.expanduser(
            '~/Library/Application Support'), app_name)
    return os.path.join(
        os.path.expanduser(os.environ.get('XDG_CONFIG_HOME', '~/.config')),
        _pathify(app_name))","

def func1(arg1, arg2, arg3=True, arg4=True):
    if var1:
        var2 = ('APPDATA' if arg3 else 'LOCALAPPDATA')
        var3 = os.path.expanduser(os.environ.get(var2, '~'))
        return os.path.join(var3, arg2, arg1)
    if (var4 and (not arg4)):
        return os.path.join(os.path.expanduser('~/Library/Application Support'), arg1)
    return os.path.join(os.path.expanduser(os.environ.get('XDG_CONFIG_HOME', '~/.config')), _pathify(arg1))
",Returns the config folder for the application . The default behavior is to return whatever is most appropriate for the operating system .,Get user config directory .,Return a path to a file .,1,Prediction1,
"def get_schedule():
    '''
    '''
    params = {'agentid': config()['agent']['name'].encode('utf8')}
    lookahead = config()['agent']['cal_lookahead'] * 24 * 60 * 60
    if lookahead:
        params['cutoff'] = str((timestamp() + lookahead) * 1000)
    uri = '%s/calendars?%s' % (config()['service-scheduler'][0],
                               urlencode(params))
    try:
        vcal = http_request(uri)
    except pycurl.error as e:
        logger.error('Could not get schedule: %s' % e)
        return

    try:
        cal = parse_ical(vcal.decode('utf-8'))
    except Exception:
        logger.error('Could not parse ical')
        logger.error(traceback.format_exc())
        return
    db = get_session()
    db.query(UpcomingEvent).delete()
    for event in cal:
        # Ignore events that have already ended
        if event['dtend'] <= timestamp():
            continue
        e = UpcomingEvent()
        e.start = event['dtstart']
        e.end = event['dtend']
        e.uid = event.get('uid')
        e.title = event.get('summary')
        e.set_data(event)
        db.add(e)
    db.commit()","

def func1():
    '\n    '
    var1 = {'agentid': config()['agent']['name'].encode('utf8')}
    var2 = (((config()['agent']['cal_lookahead'] * 24) * 60) * 60)
    if var2:
        var1['cutoff'] = str(((timestamp() + var2) * 1000))
    var6 = ('%s/calendars?%s' % (config()['service-scheduler'][0], urlencode(var1)))
    try:
        var8 = http_request(var6)
    except pycurl.error as e:
        logger.error(('Could not get schedule: %s' % var10))
        return
    try:
        var11 = parse_ical(vcal.decode('utf-8'))
    except Exception:
        logger.error('Could not parse ical')
        logger.error(traceback.format_exc())
        return
    var14 = get_session()
    db.query(UpcomingEvent).delete()
    for var16 in var11:
        if (var16['dtend'] <= timestamp()):
            continue
        var10 = UpcomingEvent()
        e.start = var16['dtstart']
        e.end = var16['dtend']
        e.uid = event.get('uid')
        e.title = event.get('summary')
        e.set_data(var16)
        db.add(var10)
    db.commit()
",Try to load schedule from the Matterhorn core . Returns a valid schedule or None on failure .,Get a schedule of upcoming events .,\ n,1,Prediction1,
"def autolink(el, link_regexes=_link_regexes,
             avoid_elements=_avoid_elements,
             avoid_hosts=_avoid_hosts,
             avoid_classes=_avoid_classes):
    
    if el.tag in avoid_elements:
        return
    class_name = el.get('class')
    if class_name:
        class_name = class_name.split()
        for match_class in avoid_classes:
            if match_class in class_name:
                return
    for child in list(el):
        autolink(child, link_regexes=link_regexes,
                 avoid_elements=avoid_elements,
                 avoid_hosts=avoid_hosts,
                 avoid_classes=avoid_classes)
        if child.tail:
            text, tail_children = _link_text(
                child.tail, link_regexes, avoid_hosts, factory=el.makeelement)
            if tail_children:
                child.tail = text
                index = el.index(child)
                el[index+1:index+1] = tail_children
    if el.text:
        text, pre_children = _link_text(
            el.text, link_regexes, avoid_hosts, factory=el.makeelement)
        if pre_children:
            el.text = text
            el[:0] = pre_children","

def var9(arg1, arg2=var1, arg3=var2, arg4=var3, arg5=var4):
    if (el.tag in arg3):
        return
    var5 = el.get('class')
    if var5:
        var5 = class_name.split()
        for var6 in arg5:
            if (var6 in var5):
                return
    for var7 in list(arg1):
        autolink(var7, link_regexes=arg2, avoid_elements=arg3, avoid_hosts=arg4, avoid_classes=arg5)
        if child.tail:
            (var10, var11) = _link_text(child.tail, arg2, arg4, factory=el.makeelement)
            if var11:
                child.tail = var10
                var13 = el.index(var7)
                arg1[(var13 + 1):(var13 + 1)] = var11
    if el.text:
        (var10, var14) = _link_text(el.text, arg2, arg4, factory=el.makeelement)
        if var14:
            el.text = var10
            arg1[:0] = var14
",Turn any URLs into links .,Autolinks variables in arg1 .,Autolink an element .,0,Prediction2,
"def list(self, path):
        '''
        '''

        self.__validate_storage_path(path)
        entity = self.api_client.get_entity_by_query(path=path)
        if entity['entity_type'] not in self.__BROWSABLE_TYPES:
            raise StorageArgumentException('The entity type ""{0}"" cannot be'
                                           'listed'.format(entity['entity_type']))
        entity_uuid = entity['uuid']
        file_names = []

        # get files
        more_pages = True
        page_number = 1
        while more_pages:
            response = self.api_client.list_folder_content(
                entity_uuid, page=page_number, ordering='name')
            more_pages = response['next'] is not None
            page_number += 1
            for child in response['results']:
                pattern = '/{name}' if child['entity_type'] == 'folder' else '{name}'
                file_names.append(pattern.format(name=child['name']))

        return file_names","

def func1(arg1, arg2):
    '\n        '
    self.__validate_storage_path(arg2)
    var1 = self.api_client.get_entity_by_query(path=arg2)
    if (var1['entity_type'] not in self.__BROWSABLE_TYPES):
        raise StorageArgumentException('The entity type ""{0}"" cannot belisted'.format(var1['entity_type']))
    var3 = var1['uuid']
    var4 = []
    var5 = True
    var6 = 1
    while var5:
        var7 = self.api_client.list_folder_content(var3, page=var6, ordering='name')
        var5 = (var7['next'] is not None)
        var6 += 1
        for var8 in var7['results']:
            var9 = ('/{name}' if (var8['entity_type'] == 'folder') else '{name}')
            file_names.append(pattern.format(name=var8['name']))
    return var4
",List the entities found directly under the given path .,\ n,Returns a list of file names for the given path .,0,Prediction2,
"def get_meteor_ids(model, object_ids):
    
    # Django model._meta is now public API -> pylint: disable=W0212
    meta = model._meta
    result = collections.OrderedDict(
        (str(obj_pk), None)
        for obj_pk
        in object_ids
    )
    if isinstance(meta.pk, AleaIdField):
        # primary_key is an AleaIdField, use it.
        return collections.OrderedDict(
            (obj_pk, obj_pk) for obj_pk in object_ids
        )
    alea_unique_fields = [
        field
        for field in meta.local_fields
        if isinstance(field, AleaIdField) and field.unique and not field.null
    ]
    if len(alea_unique_fields) == 1:
        aid = alea_unique_fields[0].name
        query = model.objects.filter(
            pk__in=object_ids,
        ).values_list('pk', aid)
    else:
        content_type = ContentType.objects.get_for_model(model)
        query = ObjectMapping.objects.filter(
            content_type=content_type,
            object_id__in=list(result)
        ).values_list('object_id', 'meteor_id')
    for obj_pk, meteor_id in query:
        result[str(obj_pk)] = meteor_id
    for obj_pk, meteor_id in result.items():
        if meteor_id is None:
            result[obj_pk] = get_meteor_id(model, obj_pk)
    return result","

def func1(arg1, arg2):
    var1 = model._meta
    var2 = collections.OrderedDict(((str(var4), None) for var4 in arg2))
    if isinstance(meta.pk, var6):
        return collections.OrderedDict(((var4, var4) for var4 in arg2))
    var7 = [var8 for var8 in meta.local_fields if (isinstance(var8, var6) and field.unique and (not field.null))]
    if (len(var7) == 1):
        var10 = alea_unique_fields[0].name
        var11 = model.objects.filter(pk__in=object_ids).values_list('pk', var10)
    else:
        var12 = ContentType.objects.get_for_model(arg1)
        var11 = ObjectMapping.objects.filter(content_type=content_type, object_id__in=list(result)).values_list('object_id', 'meteor_id')
    for (var4, var13) in var11:
        var2[str(var4)] = var13
    for (var4, var13) in result.items():
        if (var13 is None):
            var2[var4] = get_meteor_id(arg1, var4)
    return var2
",Return Alea ID mapping for all given ids of specified model .,Get a list of meteor_ids for a given model .,alea_unique_fields = object_ids = object_ids,1,Prediction1,equally bad
"def bash_complete(self, path, cmd, *cmds):
        
        path = pathlib.Path(path)
        subcmds = list(self.subcmds.keys())
        with path.open('w') as bcf:
            # main function
            print('_{}() {{'.format(cmd), file=bcf)
            print('COMPREPLY=()', file=bcf)
            print(r'local cur=${COMP_WORDS[COMP_CWORD]}', end='\n\n', file=bcf)
            optstr = ' '.join(self._bash_comp_command(None))
            print(r'local options=""{}""'.format(optstr), end='\n\n', file=bcf)
            if subcmds:
                print('local commands=""{}""'.format(' '.join(subcmds)),
                      file=bcf)
                print('declare -A suboptions', file=bcf)
            for sub in subcmds:
                optstr = ' '.join(self._bash_comp_command(sub))
                print('suboptions[{}]=""{}""'.format(sub, optstr), file=bcf)
            condstr = 'if'
            for sub in subcmds:
                print(condstr, r'[[ ""${COMP_LINE}"" == *""', sub, '""* ]] ; then',
                      file=bcf)
                print(r'COMPREPLY=( `compgen -W ""${suboptions[', sub,
                      r']}"" -- ${cur}` )', sep='', file=bcf)
                condstr = 'elif'
            print(condstr, r'[[ ${cur} == -* ]] ; then', file=bcf)
            print(r'COMPREPLY=( `compgen -W ""${options}"" -- ${cur}`)',
                  file=bcf)
            if subcmds:
                print(r'else', file=bcf)
                print(r'COMPREPLY=( `compgen -W ""${commands}"" -- ${cur}`)',
                      file=bcf)
            print('fi', file=bcf)
            print('}', end='\n\n', file=bcf)
            print('complete -F _{0} {0}'.format(cmd), *cmds, file=bcf)","

def func1(arg1, arg2, arg3, *arg4):
    arg2 = pathlib.Path(arg2)
    var1 = list(self.subcmds.keys())
    with path.open('w') as var3:
        print('_{}() {{'.format(arg3), file=var3)
        print('COMPREPLY=()', file=var3)
        print('local cur=${COMP_WORDS[COMP_CWORD]}', end='\n\n', file=var3)
        var5 = ' '.join(self._bash_comp_command(None))
        print('local options=""{}""'.format(var5), end='\n\n', file=var3)
        if var1:
            print('local commands=""{}""'.format(' '.join(var1)), file=var3)
            print('declare -A suboptions', file=var3)
        for var6 in var1:
            var5 = ' '.join(self._bash_comp_command(var6))
            print('suboptions[{}]=""{}""'.format(var6, var5), file=var3)
        var7 = 'if'
        for var6 in var1:
            print(var7, '[[ ""${COMP_LINE}"" == *""', var6, '""* ]] ; then', file=var3)
            print('COMPREPLY=( `compgen -W ""${suboptions[', var6, ']}"" -- ${cur}` )', sep='', file=var3)
            var7 = 'elif'
        print(var7, '[[ ${cur} == -* ]] ; then', file=var3)
        print('COMPREPLY=( `compgen -W ""${options}"" -- ${cur}`)', file=var3)
        if var1:
            print('else', file=var3)
            print('COMPREPLY=( `compgen -W ""${commands}"" -- ${cur}`)', file=var3)
        print('fi', file=var3)
        print('}', end='\n\n', file=var3)
        print('complete -F _{0} {0}'.format(arg3), *arg4, file=var3)
",Write bash complete script .,This function is the main entry point for shell commands .,Run a bash command .,0,Tie,
"def get_url_from_image_key(image_instance, image_key):
    
    img_key_split = image_key.split('__')
    if 'x' in img_key_split[-1]:
        size_key = img_key_split.pop(-1)
    else:
        size_key = None
    img_url = reduce(getattr, img_key_split, image_instance)
    if size_key:
        img_url = img_url[size_key].url
    return img_url","

def func1(arg1, arg2):
    var1 = image_key.split('__')
    if ('x' in var1[(- 1)]):
        var2 = img_key_split.pop((- 1))
    else:
        var2 = None
    var3 = reduce(var5, var1, arg1)
    if var2:
        var3 = img_url[size_key].url
    return var3
",Build a URL from image_key .,reduces image_key and size_key to image_url,Get image url from image key .,0,Tie,
"def signal_handler(signum, stackframe):
    
    global g_runner
    global g_handling_signal

    if g_handling_signal:
        # Don't do this recursively.
        return
    g_handling_signal = True

    print("""")
    print(""----------------------------------------------------------------------"")
    print("""")
    print(""SIGNAL CAUGHT ("" + str(signum) + "").  TEARING DOWN CLOUDS."")
    print("""")
    print(""----------------------------------------------------------------------"")
    g_runner.terminate()","

def func1(arg1, arg2):
    global g_runner
    global g_handling_signal
    if var1:
        return
    var1 = True
    print('')
    print('----------------------------------------------------------------------')
    print('')
    print((('SIGNAL CAUGHT (' + str(arg1)) + ').  TEARING DOWN CLOUDS.'))
    print('')
    print('----------------------------------------------------------------------')
    g_runner.terminate()
",Helper function to handle caught signals .,This function is called when a signal is received .,This function is called when a signal is received .,1,Tie,
"def fromRaw(cls, skype=None, raw={}):
        
        return cls(skype, raw, **cls.rawToFields(raw))","

def func1(arg1, arg2=None, arg3={}):
    return cls(arg2, arg3, **cls.rawToFields(arg3))
",Create a new instance based on the raw properties of an API response .,Create a new instance from arg1 and arg2 .,Create a new instance from raw data .,0,Prediction2,
"def progress(iterator, prefix):
    
    if terminal_width(prefix) > 25:
        prefix = ("".."" + get_cut_prefix(prefix, 23))
    speed_updated = start = time()
    speed_written = written = 0
    speed_history = deque(maxlen=5)

    for data in iterator:
        yield data

        now = time()
        elapsed = now - start
        written += len(data)

        speed_elapsed = now - speed_updated
        if speed_elapsed >= 0.5:
            speed_history.appendleft((
                written - speed_written,
                speed_updated,
            ))
            speed_updated = now
            speed_written = written

            speed_history_written = sum(h[0] for h in speed_history)
            speed_history_elapsed = now - speed_history[-1][1]
            speed = speed_history_written / speed_history_elapsed

            status = create_status_line(
                prefix=prefix,
                written=format_filesize(written),
                elapsed=format_time(elapsed),
                speed=format_filesize(speed)
            )
            print_inplace(status)
    sys.stderr.write(""\n"")
    sys.stderr.flush()","

def func1(arg1, arg2):
    if (terminal_width(arg2) > 25):
        arg2 = ('..' + get_cut_prefix(arg2, 23))
    var3 = var4 = time()
    var6 = var7 = 0
    var8 = deque(maxlen=5)
    for var10 in arg1:
        (yield var10)
        var11 = time()
        var12 = (var11 - var4)
        var7 += len(var10)
        var14 = (var11 - var3)
        if (var14 >= 0.5):
            speed_history.appendleft(((var7 - var6), var3))
            var3 = var11
            var6 = var7
            var15 = sum((var17[0] for var17 in var8))
            var18 = (var11 - var8[(- 1)][1])
            var19 = (var15 / var18)
            var20 = create_status_line(prefix=arg2, written=format_filesize(var7), elapsed=format_time(var12), speed=format_filesize(var19))
            print_inplace(var20)
    sys.stderr.write('\n')
    sys.stderr.flush()
",Progress an iterator and updates a pretty status line to the terminal .,Print progress information to stderr .,A generator function that prints out a status line .,1,Tie,
"def compile_sympy_accesses(self, sources=True, destinations=True):
        
        sympy_accesses = defaultdict(list)
        # Compile sympy accesses
        for var_name in self.variables:
            if sources:
                for r in self.sources.get(var_name, []):
                    if r is None:
                        continue
                    sympy_accesses[var_name].append(self.access_to_sympy(var_name, r))
            if destinations:
                for w in self.destinations.get(var_name, []):
                    if w is None:
                        continue
                    sympy_accesses[var_name].append(self.access_to_sympy(var_name, w))

        return sympy_accesses","

def func1(arg1, arg2=True, arg3=True):
    var1 = defaultdict(var3)
    for var4 in self.variables:
        if arg2:
            for var5 in self.sources.get(var4, []):
                if (var5 is None):
                    continue
                sympy_accesses[var_name].append(self.access_to_sympy(var4, var5))
        if arg3:
            for var6 in self.destinations.get(var4, []):
                if (var6 is None):
                    continue
                sympy_accesses[var_name].append(self.access_to_sympy(var4, var6))
    return var1
",Return a dictionary of lists of sympy accesses for each variable .,Return a dict of variable names to accesses .,Compile sympy accesses .,0,Prediction1,
"def add_point(self, point, value, check=True):
        
        if self.tier_type != 'TextTier':
            raise Exception('Tiertype must be TextTier.')
        if check and any(i for i in self.intervals if i[0] == point):
                raise Exception('No overlap is allowed')
        self.intervals.append((point, value))","

def func1(arg1, arg2, arg3, arg4=True):
    if (self.tier_type != 'TextTier'):
        raise Exception('Tiertype must be TextTier.')
    if (arg4 and any((var3 for var3 in self.intervals if (var3[0] == arg2)))):
        raise Exception('No overlap is allowed')
    self.intervals.append((arg2, arg3))
",Add a point to the TextTier,Add a point to the text tier .,Add an interval to the list of intervals .,1,Tie,
"def merge(self, status: 'Status[Input, Output]') -> 'Status[Input, Output]':
        
        if status is None or status.farthest is None:
            # No new message; simply return unchanged
            pass
        elif self.farthest is None:
            # No current message to compare to; use the message from status
            self.farthest = status.farthest
            self.expected = status.expected
        elif status.farthest.position < self.farthest.position:
            # New message is not farther; keep current message
            pass
        elif status.farthest.position > self.farthest.position:
            # New message is farther than current message; replace with new message
            self.farthest = status.farthest
            self.expected = status.expected
        else:
            # New message and current message are equally far; merge messages
            self.expected = status.expected + self.expected

        return self","

def func1(arg1, arg2: 'Status[Input, Output]') -> 'Status[Input, Output]':
    if ((arg2 is None) or (status.farthest is None)):
        pass
    elif (self.farthest is None):
        self.farthest = status.farthest
        self.expected = status.expected
    elif (status.farthest.position < self.farthest.position):
        pass
    elif (status.farthest.position > self.farthest.position):
        self.farthest = status.farthest
        self.expected = status.expected
    else:
        self.expected = (status.expected + self.expected)
    return arg1
",Merge the failure message from another status into this one .,Merge two statuses .,Change the farthest position to match the expected position .,1,Prediction1,
"def get_file_id(db, user_id, api_path):
    
    return _get_file(
        db,
        user_id,
        api_path,
        [files.c.id],
        unused_decrypt_func,
    )['id']","

def func1(arg1, arg2, arg3):
    return _get_file(arg1, arg2, arg3, [files.c.id], var2)['id']
",Get the value in the id column for the file with the given user_id and path .,wrapper for _get_file,Get file id .,0,Prediction2,
"def create_interrupt_event():
        
        # Create a security attributes struct that permits inheritance of the
        # handle by new processes.
        # FIXME: We can clean up this mess by requiring pywin32 for IPython.
        class SECURITY_ATTRIBUTES(ctypes.Structure):
            _fields_ = [ (""nLength"", ctypes.c_int),
                         (""lpSecurityDescriptor"", ctypes.c_void_p),
                         (""bInheritHandle"", ctypes.c_int) ]
        sa = SECURITY_ATTRIBUTES()
        sa_p = ctypes.pointer(sa)
        sa.nLength = ctypes.sizeof(SECURITY_ATTRIBUTES)
        sa.lpSecurityDescriptor = 0
        sa.bInheritHandle = 1

        return ctypes.windll.kernel32.CreateEventA(
            sa_p,  # lpEventAttributes
            False, # bManualReset
            False, # bInitialState
            '')","

def func1():

    class SECURITY_ATTRIBUTES(ctypes.Structure):
        _fields_ = [('nLength', ctypes.c_int), ('lpSecurityDescriptor', ctypes.c_void_p), ('bInheritHandle', ctypes.c_int)]
    var1 = SECURITY_ATTRIBUTES()
    var3 = ctypes.pointer(var1)
    sa.nLength = ctypes.sizeof(var2)
    sa.lpSecurityDescriptor = 0
    sa.bInheritHandle = 1
    return ctypes.windll.kernel32.CreateEventA(var3, False, False, '')
",Create an interrupt event handle .,Create an interrupt event .,Create a SECURITY_ATTRIBUTES structure and return it .,1,Prediction1,
"def resolve_expression(self, *args, **kwargs):
        

        result = dict()
        for key, value in self.value.items():
            if hasattr(value, 'resolve_expression'):
                result[key] = value.resolve_expression(
                    *args, **kwargs)
            else:
                result[key] = value

        return HStoreValue(result)","

def func1(arg1, *arg2, **arg3):
    var1 = dict()
    for (var3, var4) in self.value.items():
        if hasattr(var4, 'resolve_expression'):
            var1[var3] = value.resolve_expression(*arg2, **arg3)
        else:
            var1[var3] = var4
    return HStoreValue(var1)
",Resolves expressions inside the dictionary .,Return a new HStoreValue that is a copy of the current HStoreValue .,Return a new HStoreValue with resolved values .,0,Prediction2,
"def validate_user(self, username, password, client, request,
                      *args, **kwargs):
        
        log.debug('Validating username %r and its password', username)
        if self._usergetter is not None:
            user = self._usergetter(
                username, password, client, request, *args, **kwargs
            )
            if user:
                request.user = user
                return True
            return False
        log.debug('Password credential authorization is disabled.')
        return False","

def func1(arg1, arg2, arg3, arg4, arg5, *arg6, **arg7):
    log.debug('Validating username %r and its password', arg2)
    if (self._usergetter is not None):
        var1 = self._usergetter(arg2, arg3, arg4, arg5, *arg6, **arg7)
        if var1:
            request.user = var1
            return True
        return False
    log.debug('Password credential authorization is disabled.')
    return False
",Ensure the username and password is valid .,Validate a username and password .,Authenticates a username and its password .,1,Tie,
"def get_wildcard_key(self, wildcard_key, bucket_name=None, delimiter=''):
        
        if not bucket_name:
            (bucket_name, wildcard_key) = self.parse_s3_url(wildcard_key)

        prefix = re.split(r'[*]', wildcard_key, 1)[0]
        klist = self.list_keys(bucket_name, prefix=prefix, delimiter=delimiter)
        if klist:
            key_matches = [k for k in klist if fnmatch.fnmatch(k, wildcard_key)]
            if key_matches:
                return self.get_key(key_matches[0], bucket_name)","

def func1(arg1, arg2, arg3=None, arg4=''):
    if (not arg3):
        (arg3, arg2) = self.parse_s3_url(arg2)
    var1 = re.split('[*]', arg2, 1)[0]
    var2 = self.list_keys(arg3, prefix=var1, delimiter=arg4)
    if var2:
        var3 = [var4 for var4 in var2 if fnmatch.fnmatch(var4, arg2)]
        if var3:
            return self.get_key(var3[0], arg3)
",Returns a boto3 . s3 . Object object matching the wildcard expression,Get a key matching a wildcard key .,Get a key from arg1 to arg2,1,Prediction1,
"def multiple_choice_field_data(field, **kwargs):
    
    if field.choices:
        from django_any.functions import valid_choices 
        l = list(valid_choices(field.choices))
        random.shuffle(l)
        choices = []
        count = xunit.any_int(min_value=1, max_value=len(field.choices))
        for i in xrange(0, count):
            choices.append(l[i])
        return ' '.join(choices)
    return 'None'","

def func1(arg1, **arg2):
    if field.choices:
        from django_any.functions import valid_choices
        var1 = list(valid_choices(field.choices))
        random.shuffle(var1)
        var4 = []
        var5 = xunit.any_int(min_value=1, max_value=len(field.choices))
        for var7 in xrange(0, var5):
            choices.append(var1[var7])
        return ' '.join(var4)
    return 'None'
",Return random value for MultipleChoiceField,Returns a random string for a multiple choice field .,Return a random string from field . choices .,1,Tie,
"def convert_outlook_msg(msg_bytes):
    
    if not is_outlook_msg(msg_bytes):
        raise ValueError(""The supplied bytes are not an Outlook MSG file"")
    orig_dir = os.getcwd()
    tmp_dir = tempfile.mkdtemp()
    os.chdir(tmp_dir)
    with open(""sample.msg"", ""wb"") as msg_file:
        msg_file.write(msg_bytes)
    try:
        subprocess.check_call([""msgconvert"", ""sample.msg""],
                              stdout=null_file, stderr=null_file)
        eml_path = ""sample.eml""
        with open(eml_path, ""rb"") as eml_file:
            rfc822 = eml_file.read()
    except FileNotFoundError:
        raise EmailParserError(
            ""Failed to convert Outlook MSG: msgconvert utility not found"")
    finally:
        os.chdir(orig_dir)
        shutil.rmtree(tmp_dir)

    return rfc822","

def func1(arg1):
    if (not is_outlook_msg(arg1)):
        raise ValueError('The supplied bytes are not an Outlook MSG file')
    var3 = os.getcwd()
    var4 = tempfile.mkdtemp()
    os.chdir(var4)
    with open('sample.msg', 'wb') as var6:
        msg_file.write(arg1)
    try:
        subprocess.check_call(['msgconvert', 'sample.msg'], stdout=var7, stderr=var7)
        var8 = 'sample.eml'
        with open(var8, 'rb') as var9:
            var10 = eml_file.read()
    except FileNotFoundError:
        raise EmailParserError('Failed to convert Outlook MSG: msgconvert utility not found')
    finally:
        os.chdir(var3)
        shutil.rmtree(var4)
    return var10
",Uses the msgconvert Perl utility to convert an Outlook MS file to standard RFC 822 format,Convert an Outlook MSG to RFC822 .,Convert the supplied bytes to an Outlook MSG file .,1,Prediction1,
"def create_output(plugin):
    

    if (args.output or args.stdout) and (args.record or args.record_and_pipe):
        console.exit(""Cannot use record options with other file output options."")

    if args.output:
        if args.output == ""-"":
            out = FileOutput(fd=stdout)
        else:
            out = check_file_output(args.output, args.force)
    elif args.stdout:
        out = FileOutput(fd=stdout)
    elif args.record_and_pipe:
        record = check_file_output(args.record_and_pipe, args.force)
        out = FileOutput(fd=stdout, record=record)
    else:
        http = namedpipe = record = None

        if not args.player:
            console.exit(""The default player (VLC) does not seem to be ""
                         ""installed. You must specify the path to a player ""
                         ""executable with --player."")

        if args.player_fifo:
            pipename = ""streamlinkpipe-{0}"".format(os.getpid())
            log.info(""Creating pipe {0}"", pipename)

            try:
                namedpipe = NamedPipe(pipename)
            except IOError as err:
                console.exit(""Failed to create pipe: {0}"", err)
        elif args.player_http:
            http = create_http_server()

        title = create_title(plugin)

        if args.record:
            record = check_file_output(args.record, args.force)

        log.info(""Starting player: {0}"", args.player)

        out = PlayerOutput(args.player, args=args.player_args,
                           quiet=not args.verbose_player,
                           kill=not args.player_no_close,
                           namedpipe=namedpipe, http=http,
                           record=record, title=title)

    return out","

def func1(arg1):
    if ((args.output or args.stdout) and (args.record or args.record_and_pipe)):
        console.exit('Cannot use record options with other file output options.')
    if args.output:
        if (args.output == '-'):
            var1 = FileOutput(fd=var3)
        else:
            var1 = check_file_output(args.output, args.force)
    elif args.stdout:
        var1 = FileOutput(fd=var3)
    elif args.record_and_pipe:
        var5 = check_file_output(args.record_and_pipe, args.force)
        var1 = FileOutput(fd=var3, record=var5)
    else:
        var6 = var7 = var5 = None
        if (not args.player):
            console.exit('The default player (VLC) does not seem to be installed. You must specify the path to a player executable with --player.')
        if args.player_fifo:
            var8 = 'streamlinkpipe-{0}'.format(os.getpid())
            log.info('Creating pipe {0}', var8)
            try:
                var7 = NamedPipe(var8)
            except IOError as err:
                console.exit('Failed to create pipe: {0}', var11)
        elif args.player_http:
            var6 = create_http_server()
        var13 = create_title(arg1)
        if args.record:
            var5 = check_file_output(args.record, args.force)
        log.info('Starting player: {0}', args.player)
        var1 = PlayerOutput(args.player, args=args.player_args, quiet=(not args.verbose_player), kill=(not args.player_no_close), namedpipe=var7, http=var6, record=var5, title=var13)
    return var1
",Decides where to write the stream .,Create a FileOutput instance .,Handle command line options .,1,Prediction1,
"def entry(self):
        
        api = Api()
        api.authenticate()
        return api.fetch_video(self.video_id)","

def func1(arg1):
    var1 = Api()
    api.authenticate()
    return api.fetch_video(self.video_id)
",Connects to Youtube Api and retrieves the video entry object,Fetch a video by id,Get the entry of the video .,0,Prediction1,
"def visit_importfrom(self, node):
        
        basename = node.modname
        imported_module = self._get_imported_module(node, basename)

        self._check_import_as_rename(node)
        self._check_misplaced_future(node)
        self._check_deprecated_module(node, basename)
        self._check_preferred_module(node, basename)
        self._check_wildcard_imports(node, imported_module)
        self._check_same_line_imports(node)
        self._check_reimport(node, basename=basename, level=node.level)

        if isinstance(node.parent, astroid.Module):
            # Allow imports nested
            self._check_position(node)
        if isinstance(node.scope(), astroid.Module):
            self._record_import(node, imported_module)
        if imported_module is None:
            return
        modnode = node.root()
        self._check_relative_import(modnode, node, imported_module, basename)

        for name, _ in node.names:
            if name != ""*"":
                self._add_imported_module(node, ""%s.%s"" % (imported_module.name, name))
            else:
                self._add_imported_module(node, imported_module.name)","

def func1(arg1, arg2):
    var1 = node.modname
    var2 = self._get_imported_module(arg2, var1)
    self._check_import_as_rename(arg2)
    self._check_misplaced_future(arg2)
    self._check_deprecated_module(arg2, var1)
    self._check_preferred_module(arg2, var1)
    self._check_wildcard_imports(arg2, var2)
    self._check_same_line_imports(arg2)
    self._check_reimport(arg2, basename=var1, level=node.level)
    if isinstance(node.parent, astroid.Module):
        self._check_position(arg2)
    if isinstance(node.scope(), astroid.Module):
        self._record_import(arg2, var2)
    if (var2 is None):
        return
    var4 = node.root()
    self._check_relative_import(var4, arg2, var2, var1)
    for (var5, var6) in node.names:
        if (var5 != '*'):
            self._add_imported_module(arg2, ('%s.%s' % (imported_module.name, var5)))
        else:
            self._add_imported_module(arg2, imported_module.name)
",triggered when a from statement is seen,Check if arg2 is an imported module .,Check an import from node .,0,Prediction2,
"def save_token(token, domain='analytics.luminoso.com', token_file=None):
        
        token_file = token_file or get_token_filename()
        if os.path.exists(token_file):
            saved_tokens = json.load(open(token_file))
        else:
            saved_tokens = {}
        saved_tokens[domain] = token
        directory, filename = os.path.split(token_file)
        if directory and not os.path.exists(directory):
            os.makedirs(directory)
        with open(token_file, 'w') as f:
            json.dump(saved_tokens, f)","

def func1(arg1, arg2='analytics.luminoso.com', arg3=None):
    arg3 = (arg3 or get_token_filename())
    if os.path.exists(arg3):
        var2 = json.load(open(arg3))
    else:
        var2 = {}
    var2[arg2] = arg1
    (var4, var5) = os.path.split(arg3)
    if (var4 and (not os.path.exists(var4))):
        os.makedirs(var4)
    with open(arg3, 'w') as var6:
        json.dump(var2, var6)
",Take a long - lived API token and store it to a local file . Long - lived tokens can be retrieved through the UI . Optional arguments are the domain for which the token is valid and the file in which to store the token .,Save a token to the token file .,This function is used to set the value of an item in a Luminoso account,1,Prediction1,
"def return_job(self, job):
        

        if self._closed:
            return

        js = self._job_sources[job]
        if len(self._ready_callbacks) > 0:
            callback = self._ready_callbacks.popleft()
            callback(job)
        else:
            del self._job_sources[job]
            js.return_job(job)","

def func1(arg1, arg2):
    if self._closed:
        return
    var1 = self._job_sources[arg2]
    if (len(self._ready_callbacks) > 0):
        var3 = self._ready_callbacks.popleft()
        callback(arg2)
    else:
        del self._job_sources[arg2]
        js.return_job(arg2)
",Returns a job to its source job set to be run again later .,Called when a job is ready to be processed .,Called when a job is returned .,0,Tie,
"def crz(self, theta, ctl, tgt):
    
    return self.append(CrzGate(theta), [ctl, tgt], [])","

def func1(arg1, arg2, arg3, arg4):
    return self.append(CrzGate(arg2), [arg3, arg4], [])
",Apply crz from ctl to tgt with angle theta .,Add a crz gate .,Same as CrzGate . append,1,Prediction1,
"def lost_dimensions(point_fmt_in, point_fmt_out):
    

    unpacked_dims_in = PointFormat(point_fmt_in).dtype
    unpacked_dims_out = PointFormat(point_fmt_out).dtype

    out_dims = unpacked_dims_out.fields
    completely_lost = []
    for dim_name in unpacked_dims_in.names:
        if dim_name not in out_dims:
            completely_lost.append(dim_name)
    return completely_lost","

def func1(arg1, arg2):
    var1 = PointFormat(point_fmt_in).dtype
    var2 = PointFormat(point_fmt_out).dtype
    var3 = unpacked_dims_out.fields
    var4 = []
    for var5 in unpacked_dims_in.names:
        if (var5 not in var3):
            completely_lost.append(var5)
    return var4
",Returns a list of the names of the dimensions that will be lost when converting from point_fmt_in to point_fmt_out,Returns a list of variable names that are missing from the unpacked_dims_in .,Return list of dimension names that are missing from point_fmt_in and point_fmt_out,0,Prediction2,
"def write_assembly(self, output_file, filtered=True):
        

        logger.debug(""Writing the filtered assembly into: {}"".format(
            output_file))
        with open(output_file, ""w"") as fh:

            for contig_id, contig in self.contigs.items():
                if contig_id not in self.filtered_ids and filtered:
                    fh.write("">{}_{}\\n{}\\n"".format(self.sample,
                                                     contig[""header""],
                                                     contig[""sequence""]))","

def func1(arg1, arg2, arg3=True):
    logger.debug('Writing the filtered assembly into: {}'.format(arg2))
    with open(arg2, 'w') as var2:
        for (var3, var4) in self.contigs.items():
            if ((var3 not in self.filtered_ids) and arg3):
                fh.write('>{}_{}\\n{}\\n'.format(self.sample, var4['header'], var4['sequence']))
",Writes the assembly to a new file .,Write the filtered assembly into arg2 .,Write the filtered assembly to a file .,0,Prediction2,
"def accept(self, reply_socket, channel):
        
        info = self.info or b''
        self.send_raw(reply_socket, ACCEPT, info, *channel)","

def func1(arg1, arg2, arg3):
    var1 = (self.info or b'')
    self.send_raw(arg2, var2, var1, *arg3)
",Sends ACCEPT reply .,Send a message to the server .,Accept a message .,0,Tie,
"def trainable_gamma(shape, min_concentration=1e-3, min_scale=1e-5, name=None):
  
  with tf.compat.v1.variable_scope(None, default_name=""trainable_gamma""):
    unconstrained_concentration = tf.compat.v1.get_variable(
        ""unconstrained_concentration"",
        shape,
        initializer=tf.compat.v1.initializers.random_normal(
            mean=0.5, stddev=0.1))
    unconstrained_scale = tf.compat.v1.get_variable(
        ""unconstrained_scale"",
        shape,
        initializer=tf.compat.v1.initializers.random_normal(stddev=0.1))
    concentration = tf.maximum(tf.nn.softplus(unconstrained_concentration),
                               min_concentration)
    rate = tf.maximum(1. / tf.nn.softplus(unconstrained_scale), 1. / min_scale)
    rv = ed.Gamma(concentration=concentration, rate=rate, name=name)
    return rv","

def func1(arg1, arg2=0.001, arg3=1e-05, arg4=None):
    with tf.compat.v1.variable_scope(None, default_name='trainable_gamma'):
        var1 = tf.compat.v1.get_variable('unconstrained_concentration', arg1, initializer=tf.compat.v1.initializers.random_normal(mean=0.5, stddev=0.1))
        var2 = tf.compat.v1.get_variable('unconstrained_scale', arg1, initializer=tf.compat.v1.initializers.random_normal(stddev=0.1))
        var3 = tf.maximum(tf.nn.softplus(var1), arg2)
        var4 = tf.maximum((1.0 / tf.nn.softplus(var2)), (1.0 / arg3))
        var5 = ed.Gamma(concentration=var3, rate=var4, name=arg4)
        return var5
",Learnable Gamma via concentration and scale parameterization .,Function 1 .,Create a tensor that can be used for training .,0,Prediction2,equally bad
"def create_storage_account(self, service_name, description, label,
                               affinity_group=None, location=None,
                               geo_replication_enabled=None,
                               extended_properties=None,
                               account_type='Standard_GRS'):
        '''
        
        '''
        _validate_not_none('service_name', service_name)
        _validate_not_none('description', description)
        _validate_not_none('label', label)
        if affinity_group is None and location is None:
            raise ValueError(
                'location or affinity_group must be specified')
        if affinity_group is not None and location is not None:
            raise ValueError(
                'Only one of location or affinity_group needs to be specified')
        if geo_replication_enabled == False:
            account_type = 'Standard_LRS'
        return self._perform_post(
            self._get_storage_service_path(),
            _XmlSerializer.create_storage_service_input_to_xml(
                service_name,
                description,
                label,
                affinity_group,
                location,
                account_type,
                extended_properties),
            as_async=True)","

def func1(arg1, arg2, arg3, arg4, arg5=None, arg6=None, arg7=None, arg8=None, arg9='Standard_GRS'):
    '\n        \n        '
    _validate_not_none('service_name', arg2)
    _validate_not_none('description', arg3)
    _validate_not_none('label', arg4)
    if ((arg5 is None) and (arg6 is None)):
        raise ValueError('location or affinity_group must be specified')
    if ((arg5 is not None) and (arg6 is not None)):
        raise ValueError('Only one of location or affinity_group needs to be specified')
    if (arg7 == False):
        arg9 = 'Standard_LRS'
    return self._perform_post(self._get_storage_service_path(), _XmlSerializer.create_storage_service_input_to_xml(arg2, arg3, arg4, arg5, arg6, arg9, arg8), as_async=True)
",Creates a new storage account in Windows Azure .,\ n \ n,Creates a storage account .,0,Prediction2,
"def get_environment_for(file_path):
    
    work_dir = os.path.dirname(os.path.abspath(file_path))

    if not os.path.exists(work_dir):
        raise IOError('Could not find folder for dirname of file {}.'.format(file_path))

    try:
        jinja_env = Environment(loader=FileSystemLoader(work_dir))
    except:
        raise
    else:
        return jinja_env","

def func1(arg1):
    var1 = os.path.dirname(os.path.abspath(arg1))
    if (not os.path.exists(var1)):
        raise IOError('Could not find folder for dirname of file {}.'.format(arg1))
    try:
        var3 = Environment(loader=FileSystemLoader(var1))
    except:
        raise
    else:
        return var3
",Return a Jinja2 environment for where file_path is .,Returns an environment object for the given file path .,Returns an Environment object for the given file path .,0,Tie,
"def _qasmif(self, string):
        
        if self.control is None:
            return string
        return ""if(%s==%d) "" % (self.control[0].name, self.control[1]) + string","

def func1(arg1, arg2):
    if (self.control is None):
        return arg2
    return (('if(%s==%d) ' % (self.control[0].name, self.control[1])) + arg2)
",Print an if statement if needed .,Returns an expression that evaluates to True if arg1 == arg2 .,Return a QASM if statement .,0,Prediction2,
"def _at_shutdown(self):
        
        # io.rprint(""Kernel at_shutdown"") # dbg
        if self._shutdown_message is not None:
            self.session.send(self.iopub_socket, self._shutdown_message, ident=self._topic('shutdown'))
            self.log.debug(""%s"", self._shutdown_message)
        [ s.flush(zmq.POLLOUT) for s in self.shell_streams ]","

def func1(arg1):
    if (self._shutdown_message is not None):
        self.session.send(self.iopub_socket, self._shutdown_message, ident=self._topic('shutdown'))
        self.log.debug('%s', self._shutdown_message)
    [s.flush(zmq.POLLOUT) for var1 in self.shell_streams]
",Actions taken at shutdown by the kernel called by python s atexit .,Send a shutdown message to all streams .,Send shutdown message to iopub socket .,1,Tie,
"def mechs(self):
        
        if not self._mechs:
            self._mechs = self._inquire(False, False, False, True)[3]
        return self._mechs","

def func1(arg1):
    if (not self._mechs):
        self._mechs = self._inquire(False, False, False, True)[3]
    return self._mechs
",The set of mechanisms supported by the credential .,Get a list of mechs .,Return a list of mechanisms .,1,Tie,
"def clobber_in_except(
    node: astroid.node_classes.NodeNG
) -> Tuple[bool, Tuple[str, str]]:
    
    if isinstance(node, astroid.AssignAttr):
        return True, (node.attrname, ""object %r"" % (node.expr.as_string(),))
    if isinstance(node, astroid.AssignName):
        name = node.name
        if is_builtin(name):
            return (True, (name, ""builtins""))

        stmts = node.lookup(name)[1]
        if stmts and not isinstance(
            stmts[0].assign_type(),
            (astroid.Assign, astroid.AugAssign, astroid.ExceptHandler),
        ):
            return True, (name, ""outer scope (line %s)"" % stmts[0].fromlineno)
    return False, None","

def func1(arg1: astroid.node_classes.NodeNG) -> var5[(var6, var5[(var7, var7)])]:
    if isinstance(arg1, astroid.AssignAttr):
        return (True, (node.attrname, ('object %r' % (node.expr.as_string(),))))
    if isinstance(arg1, astroid.AssignName):
        var2 = node.name
        if is_builtin(var2):
            return (True, (var2, 'builtins'))
        var4 = node.lookup(var2)[1]
        if (var4 and (not isinstance(stmts[0].assign_type(), (astroid.Assign, astroid.AugAssign, astroid.ExceptHandler)))):
            return (True, (var2, ('outer scope (line %s)' % stmts[0].fromlineno)))
    return (False, None)
",Checks if an assignment node in an except handler clobbers an existing variable .,Return True if arg1 is an assignment expression .,Check if the given node is in the except scope .,0,Prediction2,
"def ndtr(x, name=""ndtr""):
  

  with tf.name_scope(name):
    x = tf.convert_to_tensor(value=x, name=""x"")
    if dtype_util.as_numpy_dtype(x.dtype) not in [np.float32, np.float64]:
      raise TypeError(
          ""x.dtype=%s is not handled, see docstring for supported types.""
          % x.dtype)
    return _ndtr(x)","

def func1(arg1, arg2='ndtr'):
    with tf.name_scope(arg2):
        arg1 = tf.convert_to_tensor(value=arg1, name='x')
        if (dtype_util.as_numpy_dtype(x.dtype) not in [np.float32, np.float64]):
            raise TypeError(('x.dtype=%s is not handled, see docstring for supported types.' % x.dtype))
        return _ndtr(arg1)
",Normal distribution function .,Convert a tensor to an ndtr tensor .,Wrapper for _ndtr .,1,Tie,equally bad